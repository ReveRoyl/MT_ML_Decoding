{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ysm6Y6V764k",
    "tags": []
   },
   "source": [
    "## env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L\n",
    "!nvcc -V\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1656346799559,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "3NRIzhpaAPr8",
    "outputId": "3b03ab24-b730-404d-aecd-b7fae439a0fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TF_ENABLE_ONEDNN_OPTS\"]=0\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import sys\n",
    "sys.path.append('Code/code/')\n",
    "from load_data import load_MEG_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda import amp\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "# %env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "from scipy.integrate import simps\n",
    "from mne.time_frequency import psd_array_welch\n",
    "from band_power import (\n",
    "    bandpower_multi_bands,\n",
    "    standard_scaling_sklearn,\n",
    ")\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from utils import(\n",
    "    ChannelPool,\n",
    "    EarlyStopping,\n",
    "    SpatialAttention,\n",
    "    Flatten_MEG,\n",
    "    ChannelAttention,\n",
    "    Concatenate,\n",
    "    L1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1656346662628,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "kVg_nRUnT75F"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if  torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5CpfcYgAEeh",
    "tags": []
   },
   "source": [
    "# Mnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwmxzEwZAPr8",
    "tags": []
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5030,
     "status": "ok",
     "timestamp": 1656346804863,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "E4yCXxNLAPr9",
    "outputId": "20f25db3-ea31-4fd6-f746-e9fe956e23fc"
   },
   "outputs": [],
   "source": [
    "Split = 0.90\n",
    "X_train, y_train = load_MEG_dataset([str(i).zfill(3) for i in range(1,2)], mode = 'concatenate', output_format='numpy',shuffle = False, training=True, train_test_split=Split, batch_size=500)#, pca_n_components =30)\n",
    "X_test, y_test = load_MEG_dataset([str(i).zfill(3) for i in range(1,2)], mode = 'concatenate', output_format='numpy',shuffle = False, training=False, train_test_split=Split, batch_size=500)#, pca_n_components =30)\n",
    "\n",
    "X_train, X_test = (X_train-X_train.mean())/X_train.std(), (X_test-X_test.mean())/X_test.std()\n",
    "\n",
    "X_train = X_train[:, None, ...]\n",
    "X_test = X_test[:, None, ...]\n",
    "\n",
    "# X_train=np.repeat(X_train,3,axis=1)\n",
    "# X_test=np.repeat(X_test,3,axis=1)\n",
    "\n",
    "y_train = (y_train / 2) - 1\n",
    "y_test = (y_test / 2) - 1\n",
    "\n",
    "X_train_tensors = torch.Tensor(X_train)\n",
    "X_test_tensors = torch.Tensor(X_test)\n",
    "y_train_tensors = torch.from_numpy(y_train) \n",
    "y_test_tensors = torch.from_numpy(y_test)\n",
    "\n",
    "# y_train_tensors = F.one_hot(y_train_tensors)\n",
    "# y_test_tensors = F.one_hot(y_test_tensors)\n",
    "# X_train_tensors = F.interpolate(X_train_tensors, size=(272, 272), mode ='bicubic')\n",
    "# X_test_tensors = F.interpolate(X_test_tensors, size=(272, 272), mode ='bicubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1656346806649,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "2U7T9VHSAPr-"
   },
   "outputs": [],
   "source": [
    "X_train_tensors=X_train_tensors.cuda()\n",
    "X_test_tensors=X_test_tensors.cuda()\n",
    "y_train_tensors=y_train_tensors.cuda()\n",
    "y_test_tensors=y_test_tensors.cuda()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## band power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1656346806948,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "Qh0tZZq62f4C",
    "outputId": "175f769b-8003-40ab-8e81-a60c910fae31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_numpy = X_train_tensors.cpu().numpy()\n",
    "X_test_numpy = X_test_tensors.cpu().numpy()\n",
    "\n",
    "X = np.swapaxes(X_train_numpy, 2, -1).squeeze()\n",
    "data = X[X.shape[0]-1, 70, :]\n",
    "psd_mne, freqs_mne = psd_array_welch(data, 100, 1., 70., n_per_seg=None,\n",
    "                          n_overlap=0, n_jobs=1)\n",
    "for low, high in [(0.5, 4), (4, 8), (8, 10), (10, 12), (12, 30),\n",
    "                  (30, 70)]:\n",
    "    print(\"processing bands (low, high) : ({},{})\".format(low, high))\n",
    "    # Find intersecting values in frequency vector\n",
    "    idx_delta = np.logical_and(freqs_mne >= low, freqs_mne <= high)\n",
    "      # Frequency resolution\n",
    "    freq_res = freqs_mne[1] - freqs_mne[0]  # = 1 / 4 = 0.25\n",
    "\n",
    "    # Compute the absolute power by approximating the area under the curve\n",
    "    power = simps(psd_mne[idx_delta], dx=freq_res)\n",
    "    print('Absolute power: {:.4f} uV^2'.format(power))\n",
    "    \n",
    "    total_power = simps(psd_mne, dx=freq_res)\n",
    "    rel_power = power / total_power\n",
    "    \n",
    "    print('Relative power: {:.4f}'.format(rel_power))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_bp = np.squeeze(X_train_numpy, axis=1)\n",
    "# X_train_bp = X_train_bp[: :, :, :]\n",
    "X_train_bp = standard_scaling_sklearn(X_train_bp)\n",
    "X_test_bp = np.squeeze(X_test_numpy, axis=1)\n",
    "# X_train_bp = X_train_bp[: :, :, :]\n",
    "X_test_bp = standard_scaling_sklearn(X_test_bp)\n",
    "bands = [(1, 4), (4, 8), (8, 10), (10, 13), (13, 30), (30, 70)]\n",
    "bp_train = bandpower_multi_bands(X_train_bp, fs=800.0, bands=bands, relative=True)\n",
    "bp_test = bandpower_multi_bands(X_test_bp, fs=800.0, bands=bands, relative=True)\n",
    "bp_train_tensor = torch.Tensor(bp_train).cuda()\n",
    "bp_test_tensor = torch.Tensor(bp_test).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_train_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmxXd-1LGqPy",
    "tags": []
   },
   "source": [
    "## Parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xcp2pMSWGqPy"
   },
   "outputs": [],
   "source": [
    "CRED    = '\\33[31m'\n",
    "CGREEN  = '\\33[32m'\n",
    "CYELLOW = '\\33[33m'\n",
    "CBLUE   = '\\33[34m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJxb95imHxNM",
    "tags": []
   },
   "source": [
    "## ASRCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ASRCNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Model inspired by [Aoe at al., 10.1038/s41598-019-41500-x] integrated with bandpower.\n",
    "    \"\"\"\n",
    "    class Showsize(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ASRCNet.Showsize, self).__init__()\n",
    "        def forward(self, x):\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "\n",
    "    def __init__(self, n_times):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_times (int):\n",
    "                n_times dimension of the input data.\n",
    "        \"\"\"\n",
    "        super(ASRCNet, self).__init__()\n",
    "        # if n_times == 501:  # TODO automatic n_times\n",
    "        #     self.n_times = 12\n",
    "        # elif n_times == 601:\n",
    "        #     self.n_times = 18\n",
    "        # elif n_times == 701:\n",
    "        #     self.n_times = 24\n",
    "        # else:\n",
    "        #     raise ValueError(\n",
    "        #         \"Network can work only with n_times = 501, 601, 701 \"\n",
    "        #         \"(epoch duration of 1., 1.2, 1.4 sec),\"\n",
    "        #         \" got instead {}\".format(n_times)\n",
    "        #     )\n",
    "        self.n_times = n_times\n",
    "        self.spatial = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(272,64), bias=True), #kernel size 204, 64\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, stride=(1, 1), kernel_size=(1, 16), bias=True), # kernel size 1,16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            # nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(8, 8), bias=True),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, stride=(1, 1), kernel_size=(8, 8), bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(5, 3), stride=(5, 3)),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, stride=(1, 1), kernel_size=(1, 4), bias=True),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(1, 4), bias=True), #conv6\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            nn.Conv2d(64, 128, stride=(1, 1), kernel_size=(1, 2), bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, stride=(1, 1), kernel_size=(1, 2), bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            nn.Conv2d(128, 256, stride=(1, 1), kernel_size=(1, 2), bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, stride=(1, 1), kernel_size=(1, 2), bias=True), #conv10\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            ChannelAttention([None, 256, 64, self.n_times]),\n",
    "            SpatialAttention(),\n",
    "        )\n",
    "\n",
    "        self.concatenate = Concatenate()\n",
    "\n",
    "        self.flatten = Flatten_MEG()\n",
    "\n",
    "        self.ff1 = nn.Sequential(\n",
    "            # nn.Linear(256 * 26 * self.n_times + 272 * 6, 1024),\n",
    "            nn.Linear(1636, 1024),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(),\n",
    "            self.Showsize(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            self.Showsize(),\n",
    "        )\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024 , 14),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim =1)\n",
    "\n",
    "    def forward(self, x, pb):\n",
    "        x = self.spatial(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.temporal(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.concatenate(x, pb)\n",
    "        x = self.ff1(self.flatten(x))\n",
    "        x = self.ff2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rpsmnet = ASRCNet(30).cuda()\n",
    "print(rpsmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0PFl-B9Gf5G"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJcV7N9KHWAU"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "num_epochs=50\n",
    "train = Data.TensorDataset(X_train_tensors, y_train_tensors, bp_train_tensor)\n",
    "test = Data.TensorDataset(X_test_tensors, y_test_tensors, bp_test_tensor)\n",
    "train_loader = Data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = Data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rpsmnet.parameters(), lr=learning_rate)#, weight_decay=0.001)\n",
    "# optimizer = torch.optim.RAdam(rpsmnet.parameters(), lr=1)\n",
    "# optimizer = torch.optim.SGD(rpsmnet.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "# optimizer = torch.optim.Adam([{'params': rpsmnet.spatial.parameters()}, \n",
    "#                              {'params': rpsmnet.temporal.parameters()},\n",
    "# #                              {'params': rpsmnet.attention.parameters()},\n",
    "#                              {'params': rpsmnet.ff1.parameters()},\n",
    "#                              {'params': rpsmnet.concatenate.parameters(),'weight_decay': 0.005},\n",
    "#                              {'params': rpsmnet.ff2.parameters(), 'weight_decay': 0.005}], lr=learning_rate)#, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "error",
     "timestamp": 1656336840520,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "lDvOt24MGf5L",
    "outputId": "bf0e73e4-4335-4e8f-c128-dd2635df4a37",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "summary_writer = SummaryWriter(f'./models/test')\n",
    "# valid_loss = []\n",
    "running_loss = 0\n",
    "door_for_test = 1\n",
    "rpsmnet.train()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    t_loss = 0\n",
    "    train_loss = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "#         get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels, bp = data #, bp\n",
    "        # print(inputs.shape)\n",
    "        inputs, labels, bp = inputs.to(device), labels.type(torch.LongTensor).to(device), bp.to(device)# , bp  , bp.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = rpsmnet(inputs,bp)\n",
    "        # print(outputs)\n",
    "        with torch.autocast('cuda'):\n",
    "        # loss = criterion(outputs, torch.tensor(labels).cuda())\n",
    "            loss = criterion(outputs, labels)\n",
    "#         l2_lambda = 0.001\n",
    "#         l2_reg = torch.tensor(0.).cuda()\n",
    "#         for param in rpsmnet.parameters():\n",
    "#             l2_reg += torch.norm(param)\n",
    "#         loss += l2_lambda * l2_reg\n",
    "        \n",
    "        # l1_lambda = 0.001\n",
    "        # l1_norm = sum(torch.linalg.norm(p, 1) for p in rpsmnet.parameters())\n",
    "        # # l2_norm = sum(torch.linalg.norm(p, 2) for p in model.parameters())\n",
    "        # loss = loss + l1_lambda * l1_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "        # print(t_loss)\n",
    "        train_loss.append(np.mean(t_loss))\n",
    "    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        # if i % (math.ceil(900*Split/BATCH_SIZE)-1) == 0 and i !=0:\n",
    "        print(CRED+ f'[{epoch + 1}, {i + 1}] trainning loss: {train_loss[-1]}'+ CRED)\n",
    "    summary_writer.add_scalar('train_loss', train_loss[-1], epoch)\n",
    "    \n",
    "    if door_for_test == 1:\n",
    "        if epoch % 1 == 0:\n",
    "            # rpsmnet.eval()\n",
    "            valid_loss = []\n",
    "            acc = []\n",
    "            va_loss = 0\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                i_list = []\n",
    "                i_list.append(i)\n",
    "                val_x, val_y, bp = data #, bp \n",
    "                val_x, val_y, bp = val_x.to(device), val_y.type(torch.LongTensor).to(device), bp.to(device)#, bp.to(device) #, bp\n",
    "                Testoutput = rpsmnet(val_x, bp)# , bp\n",
    "                # v_loss = criterion(Testoutput, val_y, torch.Tensor(Testoutput.size(0)).cuda().fill_(1.0))\n",
    "                v_loss = criterion(Testoutput, val_y) #loss\n",
    "                va_loss += v_loss.item()\n",
    "                valid_loss.append(np.mean(va_loss))\n",
    "                # if i = i_list[-1]:    # print every first\n",
    "                _, predicted = torch.max(Testoutput,1)\n",
    "                labels = val_y.cpu()\n",
    "                predicted = predicted.cpu()\n",
    "                mean_accuracy = accuracy_score(labels[labels != 99], predicted[predicted != 99])\n",
    "                acc.append(np.mean(mean_accuracy))\n",
    "            print(CGREEN+f'[{epoch + 1}, {i + 1}] valid_loss: {valid_loss[-1]} Acc: {acc[-1]}'+CGREEN)\n",
    "            # rpsmnet.train()\n",
    "        summary_writer.add_scalar('valid_loss', valid_loss[-1], epoch)\n",
    "#     scheduler.step(acc[-1])\n",
    "\n",
    "  # if epoch % 1 == 0:\n",
    "  #   GETcorrectnumber(train_loader,CYELLOW)      \n",
    "  #   GETcorrectnumber(test_loader,CBLUE)\n",
    "  # aoemnet.train()\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "    early_stopping(valid_loss[-1], rpsmnet)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(rpsmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WubBEV-ctmdF"
   },
   "source": [
    "Confusion matrix and mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h6xSCd_riOV"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "confusion_matrix_test_loader = Data.DataLoader(test, batch_size = 90, shuffle = False)\n",
    "with torch.no_grad():\n",
    "    list_mean_accuracy = []\n",
    "    for i, data in enumerate(confusion_matrix_test_loader, 0):\n",
    "        inputs, labels, bp = data\n",
    "        inputs, labels, bp = inputs.to(device), labels.type(torch.LongTensor).to(device), bp.to(device)\n",
    "        # inputs, labels = data\n",
    "        # inputs, labels = inputs.to(device), labels.type(torch.LongTensor).to(device)\n",
    "        outputs = rpsmnet(inputs, bp)\n",
    "        # outputs = rpsmnet(inputs)\n",
    "#         optimizer.step()          \n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        # predicted = torch.max(outputs)\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "        labels = labels.numpy()\n",
    "        predicted = predicted.numpy()\n",
    "        # mean_conf_mat = confusion_matrix(labels, predicted)\n",
    "        # mean_accuracy = accuracy_score(labels[labels != 99], predicted[predicted != 99])\n",
    "        # mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)  # normalise\n",
    "        mean_conf_mat = confusion_matrix(labels, predicted)\n",
    "        mean_accuracy = accuracy_score(labels[labels != 99], predicted[predicted != 99])\n",
    "        mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1) \n",
    "        list_mean_accuracy.append(mean_accuracy)\n",
    "        print(\"Mean accuracy = {0}\".format(mean_accuracy))\n",
    "        ConfusionMatrixDisplay.from_predictions(labels, predicted)\n",
    "        # plt.savefig('/content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/notebooks/templates/save_folder/fig-{}.png'.format(session_id), dpi=600)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52utnK5XGf5M"
   },
   "outputs": [],
   "source": [
    "!rm -rf /logs/ # clear logs\n",
    "# if 'google.colab' in str(get_ipython()): # tensor board\n",
    "%load_ext tensorboard  \n",
    "# %tensorboard --logdir logs\n",
    "%tensorboard --logdir=./models/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJxb95imHxNM",
    "tags": []
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hiddenlayer as hl\n",
    "\n",
    "# transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n",
    "\n",
    "# graph = hl.build_graph(rpsmnet, torch.zeros(64,1,272,800).cuda())\n",
    "# graph.theme = hl.graph.THEMES['blue'].copy()\n",
    "# graph.save('ASRCnet_hiddenlayer', format='png')\n",
    "\n",
    "# from torchviz import make_dot\n",
    "# x = torch.randn(64, 1, 272, 800).requires_grad_(True).cuda() # 定义一个网络的输入值\n",
    "# y = rpsmnet(x)    # 获取网络的预测值\n",
    "# # y = y.cuda()\n",
    "# MyConvNetVis = make_dot(y)#, params=dict(list(rpsmnet.named_parameters()) + [('x', x)]))\n",
    "# MyConvNetVis.format = \"png\"\n",
    "# # 指定文件生成的文件夹\n",
    "# MyConvNetVis.directory = \"data\"\n",
    "# # 生成文件\n",
    "# MyConvNetVis.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
