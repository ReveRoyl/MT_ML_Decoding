{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ysm6Y6V764k",
    "tags": []
   },
   "source": [
    "## env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-1242830d-0dff-bf8c-ac33-61a08f8c447a)\n",
      "GPU 1: NVIDIA A100-SXM4-40GB (UUID: GPU-7d81048e-a999-9a44-bacb-6cfa0327cc98)\n",
      "GPU 2: NVIDIA A100-SXM4-40GB (UUID: GPU-77e505f3-e454-9468-bf4c-a21cf5e7dac6)\n",
      "GPU 3: NVIDIA A100-SXM4-40GB (UUID: GPU-b3a0be24-9fd6-96c5-f6df-a78b70e2096a)\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "Fri Jul 22 17:36:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    73W / 400W |  38906MiB / 40960MiB |     29%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    72W / 400W |  12960MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:B1:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  On   | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    57W / 400W |  12934MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3328935      C   python3                         38903MiB |\n",
      "|    1   N/A  N/A   3283001      C   python                          12955MiB |\n",
      "|    3   N/A  N/A   3283461      C   python                          12929MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L\n",
    "!nvcc -V\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1656346799559,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "3NRIzhpaAPr8",
    "outputId": "3b03ab24-b730-404d-aecd-b7fae439a0fe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-07-22 17:36:27.163745: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-22 17:36:27.169879: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-22 17:36:27.169902: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# TF_ENABLE_ONEDNN_OPTS=0\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import sys\n",
    "sys.path.append('Code/code/')\n",
    "from load_data import load_MEG_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda import amp\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "# %env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "from scipy.integrate import simps\n",
    "from mne.time_frequency import psd_array_welch\n",
    "from band_power import (\n",
    "    bandpower_multi_bands,\n",
    "    standard_scaling_sklearn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1656346662628,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "kVg_nRUnT75F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if  torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5CpfcYgAEeh",
    "tags": []
   },
   "source": [
    "# Mnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwmxzEwZAPr8",
    "tags": []
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5030,
     "status": "ok",
     "timestamp": 1656346804863,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "E4yCXxNLAPr9",
    "outputId": "20f25db3-ea31-4fd6-f746-e9fe956e23fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Split = 0.90\n",
    "X_train, y_train = load_MEG_dataset([str(i).zfill(3) for i in range(1,2)], mode = 'concatenate', output_format='numpy',shuffle = True, training=True, train_test_split=Split, batch_size=500)\n",
    "X_test, y_test = load_MEG_dataset([str(i).zfill(3) for i in range(1,2)], mode = 'concatenate', output_format='numpy',shuffle = True, training=False, train_test_split=Split, batch_size=500)\n",
    "\n",
    "X_train = X_train[:, None, ...]\n",
    "X_test = X_test[:, None, ...]\n",
    "\n",
    "# X_train=np.repeat(X_train,3,axis=1)\n",
    "# X_test=np.repeat(X_test,3,axis=1)\n",
    "\n",
    "y_train = (y_train / 2) - 1\n",
    "y_test = (y_test / 2) - 1\n",
    "\n",
    "X_train_tensors = torch.Tensor(X_train)\n",
    "X_test_tensors = torch.Tensor(X_test)\n",
    "y_train_tensors = torch.from_numpy(y_train) \n",
    "y_test_tensors = torch.from_numpy(y_test)\n",
    "# y_train_tensors = F.one_hot(y_train_tensors)\n",
    "# y_test_tensors = F.one_hot(y_test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([810, 1, 272, 800])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors = F.interpolate(X_train_tensors, size=(272, 800))\n",
    "X_test_tensors = F.interpolate(X_test_tensors, size=(272, 800))\n",
    "print(X_train_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1656346806649,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "2U7T9VHSAPr-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors=X_train_tensors.cuda()\n",
    "X_test_tensors=X_test_tensors.cuda()\n",
    "y_train_tensors=y_train_tensors.cuda()\n",
    "y_test_tensors=y_test_tensors.cuda()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([810, 1, 272, 800])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## band power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1656346806948,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "Qh0tZZq62f4C",
    "outputId": "175f769b-8003-40ab-8e81-a60c910fae31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 1.024 (s)\n",
      "processing bands (low, high) : (1,4)\n",
      "Absolute power: 0.0610 uV^2\n",
      "Relative power: 0.1251\n",
      "processing bands (low, high) : (4,8)\n",
      "Absolute power: 0.0315 uV^2\n",
      "Relative power: 0.0647\n",
      "processing bands (low, high) : (8,10)\n",
      "Absolute power: 0.0220 uV^2\n",
      "Relative power: 0.0452\n",
      "processing bands (low, high) : (10,13)\n",
      "Absolute power: 0.0031 uV^2\n",
      "Relative power: 0.0064\n",
      "processing bands (low, high) : (13,30)\n",
      "Absolute power: 0.0577 uV^2\n",
      "Relative power: 0.1184\n",
      "processing bands (low, high) : (30,70)\n",
      "Absolute power: 0.2356 uV^2\n",
      "Relative power: 0.4837\n"
     ]
    }
   ],
   "source": [
    "X = np.swapaxes(X_train, 2, -1).squeeze()\n",
    "data = X[X.shape[0]-1, 70, :]\n",
    "psd_mne, freqs_mne = psd_array_welch(data, 250, 1., 70., n_per_seg=None,\n",
    "                          n_overlap=0, n_jobs=1)\n",
    "for low, high in [(1, 4), (4, 8), (8, 10), (10, 13), (13, 30),\n",
    "                  (30, 70)]:\n",
    "    print(\"processing bands (low, high) : ({},{})\".format(low, high))\n",
    "    # Find intersecting values in frequency vector\n",
    "    idx_delta = np.logical_and(freqs_mne >= low, freqs_mne <= high)\n",
    "      # Frequency resolution\n",
    "    freq_res = freqs_mne[1] - freqs_mne[0]  # = 1 / 4 = 0.25\n",
    "\n",
    "    # Compute the absolute power by approximating the area under the curve\n",
    "    power = simps(psd_mne[idx_delta], dx=freq_res)\n",
    "    print('Absolute power: {:.4f} uV^2'.format(power))\n",
    "    \n",
    "    total_power = simps(psd_mne, dx=freq_res)\n",
    "    rel_power = power / total_power\n",
    "    \n",
    "    print('Relative power: {:.4f}'.format(rel_power))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n"
     ]
    }
   ],
   "source": [
    "X_train_bp = np.squeeze(X_train, axis=1)\n",
    "# X_train_bp = X_train_bp[: :, :, :]\n",
    "X_train_bp = standard_scaling_sklearn(X_train_bp)\n",
    "X_test_bp = np.squeeze(X_test, axis=1)\n",
    "# X_train_bp = X_train_bp[: :, :, :]\n",
    "X_test_bp = standard_scaling_sklearn(X_test_bp)\n",
    "bands = [(1, 4), (4, 8), (8, 10), (10, 13), (13, 30), (30, 70)]\n",
    "bp_train = bandpower_multi_bands(X_train_bp, fs=100.0, bands=bands, relative=True)\n",
    "bp_test = bandpower_multi_bands(X_test_bp, fs=100.0, bands=bands, relative=True)\n",
    "bp_train_tensor = torch.Tensor(bp_train).cuda()\n",
    "bp_test_tensor = torch.Tensor(bp_test).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmxXd-1LGqPy",
    "tags": []
   },
   "source": [
    "## Parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xcp2pMSWGqPy"
   },
   "outputs": [],
   "source": [
    "CRED    = '\\33[31m'\n",
    "CGREEN  = '\\33[32m'\n",
    "CYELLOW = '\\33[33m'\n",
    "CBLUE   = '\\33[34m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8xFKHSqAR4I",
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uUckNRIGCy4I"
   },
   "outputs": [],
   "source": [
    "class ChannelPool(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat((torch.max(x, 1)[0].unsqueeze(1),\n",
    "                          torch.mean(x, 1).unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int):\n",
    "                How long to wait after last time validation loss improved.\n",
    "                Default: 7\n",
    "            verbose (bool):\n",
    "                If True, prints a message for each validation loss improvement.\n",
    "                Default: False\n",
    "            delta (float):\n",
    "                Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                Default: 0\n",
    "            path (str):\n",
    "                Path for the checkpoint to be saved to.\n",
    "                Default: 'checkpoint.pt'\n",
    "            trace_func (function):\n",
    "                trace print function.\n",
    "                Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.4f} --> {val_loss:.4f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rx7HUXktBbbL"
   },
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.spatialAttention = nn.Sequential(\n",
    "            ChannelPool(),\n",
    "            nn.Conv2d(2, 1, 7, 7, padding=3),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.spatialAttention(x)\n",
    "        # print(attention.shape)\n",
    "        # print(x.shape)\n",
    "        return x * attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iQVFvyxdEoRm"
   },
   "outputs": [],
   "source": [
    "class Flatten_MEG(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SgXMsUwoFPZT"
   },
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "            Implementation of a channel attention module.\n",
    "        \"\"\"\n",
    "    class Showsize(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ChannelAttention.Showsize, self).__init__()\n",
    "        def forward(self, x):\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "\n",
    "    def __init__(self, shape, reduction_factor=16):\n",
    "\n",
    "        super(ChannelAttention, self).__init__()\n",
    "\n",
    "        _, in_channel, h, w = shape\n",
    "        self.mlp = nn.Sequential(\n",
    "            # self.Showsize(),\n",
    "            Flatten_MEG(),\n",
    "            # self.Showsize(),\n",
    "            nn.Linear(in_channel, in_channel // reduction_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_channel // reduction_factor, in_channel),\n",
    "        )\n",
    "        self.avg = nn.AvgPool2d(kernel_size=(h, w), stride=(h, w))\n",
    "        self.max = nn.MaxPool2d(kernel_size=(h, w), stride=(h, w))\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x', x.shape)\n",
    "        avg = self.avg(x)\n",
    "        max = self.max(x)\n",
    "        sum = self.mlp(avg) + self.mlp(max)\n",
    "        # print(sum.shape)\n",
    "        attention = (\n",
    "            torch.sigmoid(sum)\n",
    "            .unsqueeze(2)\n",
    "            .unsqueeze(3)\n",
    "        )\n",
    "        print('att', attention.shape)\n",
    "        return x * attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concatenate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Concatenate, self).__init__()\n",
    "\n",
    "    def forward(self, x, bp):\n",
    "\n",
    "        # min_ = x.min(1, keepdim=True)[0]\n",
    "        # if min_[0] < 0:\n",
    "        #     x = x + min_\n",
    "        # else:\n",
    "        #     x = x - min_\n",
    "        # x = x / x.max()\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        bp = bp.view(bp.shape[0], -1)\n",
    "        x = torch.cat([x, bp], -1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GETcorrectnumber(loader, printcolor):\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        n_class_correct = [0 for i in range(num_classes)]\n",
    "        n_class_samples = [0 for i in range(num_classes)]\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = rpsmnet(inputs)\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            # n_correct += (predicted == labels).sum().item()\n",
    "            for k in range(predicted.shape[0]):\n",
    "                if predicted[k]==labels[k]:\n",
    "                    n_correct +=1\n",
    "            # for i in range(num_classes): # accuracy for each class\n",
    "            #     label = labels[i]\n",
    "            #     pred = predicted[i]\n",
    "            #     if (label == pred):\n",
    "            #         n_class_correct[i] += 1\n",
    "            #     n_class_samples[i] += 1\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(printcolor+f'[{epoch + 1}] t accuracy： {acc}%'+printcolor)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1(torch.nn.Module):\n",
    "    def __init__(self, module, weight_decay):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Backward hook is registered on the specified module\n",
    "        self.hook = self.module.register_full_backward_hook(self._weight_decay_hook)\n",
    "\n",
    "    # Not dependent on backprop incoming values, placeholder\n",
    "    def _weight_decay_hook(self, *_):\n",
    "        for param in self.module.parameters():\n",
    "            # If there is no gradient or it was zeroed out\n",
    "            # Zeroed out using optimizer.zero_grad() usually\n",
    "            # Turn on if needed with grad accumulation/more safer way\n",
    "            # if param.grad is None or torch.all(param.grad == 0.0):\n",
    "\n",
    "            # Apply regularization on it\n",
    "            param.grad = self.regularize(param)\n",
    "\n",
    "    def regularize(self, parameter):\n",
    "        # L1 regularization formula\n",
    "        return self.weight_decay * torch.sign(parameter.data)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Simply forward and args and kwargs to module\n",
    "        return self.module(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJxb95imHxNM",
    "tags": []
   },
   "source": [
    "## RPS_Mnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPS_MNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Model inspired by [Aoe at al., 10.1038/s41598-019-41500-x] integrated with bandpower.\n",
    "    \"\"\"\n",
    "    class Showsize(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(RPS_MNet.Showsize, self).__init__()\n",
    "        def forward(self, x):\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "\n",
    "    def __init__(self, n_times):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_times (int):\n",
    "                n_times dimension of the input data.\n",
    "        \"\"\"\n",
    "        super(RPS_MNet, self).__init__()\n",
    "        # if n_times == 501:  # TODO automatic n_times\n",
    "        #     self.n_times = 12\n",
    "        # elif n_times == 601:\n",
    "        #     self.n_times = 18\n",
    "        # elif n_times == 701:\n",
    "        #     self.n_times = 24\n",
    "        # else:\n",
    "        #     raise ValueError(\n",
    "        #         \"Network can work only with n_times = 501, 601, 701 \"\n",
    "        #         \"(epoch duration of 1., 1.2, 1.4 sec),\"\n",
    "        #         \" got instead {}\".format(n_times)\n",
    "        #     )\n",
    "        self.n_times = n_times\n",
    "        self.spatial = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(272,64), bias=False), #kernel size 204, 64\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            self.Showsize(),\n",
    "            nn.Conv2d(32, 64, stride=(1, 1), kernel_size=(1, 16), bias=False), # kernel size 1,16\n",
    "            nn.ReLU(),\n",
    "            self.Showsize(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            self.Showsize(),\n",
    "        )\n",
    "\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(8, 8), bias=False),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            self.Showsize(),\n",
    "            nn.Conv2d(32, 32, stride=(1, 1), kernel_size=(8, 8), bias=False),\n",
    "            nn.ReLU(),\n",
    "            self.Showsize(),\n",
    "            nn.MaxPool2d(kernel_size=(5, 3), stride=(5, 3)),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            self.Showsize(),\n",
    "            nn.Conv2d(32, 64, stride=(1, 1), kernel_size=(1, 4), bias=False),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            self.Showsize(),\n",
    "            nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(1, 4), bias=False), #conv6\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            self.Showsize(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            self.Showsize(),\n",
    "            nn.Conv2d(64, 128, stride=(1, 1), kernel_size=(1, 2), bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            self.Showsize(),\n",
    "            nn.Conv2d(128, 128, stride=(1, 1), kernel_size=(1, 2), bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            self.Showsize(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            self.Showsize(),\n",
    "            nn.Conv2d(128, 256, stride=(1, 1), kernel_size=(1, 2), bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            self.Showsize(),\n",
    "            nn.Conv2d(256, 256, stride=(1, 1), kernel_size=(1, 2), bias=False), #conv10\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            self.Showsize(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            self.Showsize(),\n",
    "\n",
    "        )\n",
    "\n",
    "        # self.attention = nn.Sequential(\n",
    "        #     ChannelAttention([None, 256, 26, self.n_times]), SpatialAttention()\n",
    "        # )\n",
    "\n",
    "        self.concatenate = Concatenate()\n",
    "\n",
    "        self.flatten = Flatten_MEG()\n",
    "\n",
    "        self.ff1 = nn.Sequential(\n",
    "            # nn.Linear(256 * 26 * self.n_times + 272 * 6, 1024),\n",
    "            nn.Linear(30720, 1024),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(),\n",
    "            self.Showsize(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            self.Showsize(),\n",
    "        )\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2656, 14),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, pb):\n",
    "        x = self.spatial(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.temporal(x)\n",
    "        # x = self.attention(x)\n",
    "        x = self.ff1(self.flatten(x))\n",
    "        x = self.concatenate(x, pb)\n",
    "        x = self.ff2(x)\n",
    "\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RPS_MNet(nn.Module):\n",
    "#     \"\"\"\n",
    "#         Model inspired by [Aoe at al., 10.1038/s41598-019-41500-x] integrated with bandpower.\n",
    "#     \"\"\"\n",
    "#     class Showsize(nn.Module):\n",
    "#         def __init__(self):\n",
    "#             super(RPS_MNet.Showsize, self).__init__()\n",
    "#         def forward(self, x):\n",
    "#             # print(x.shape)\n",
    "#             return x\n",
    "        \n",
    "#     def __init__(self, n_times):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             n_times (int):\n",
    "#                 n_times dimension of the input data.\n",
    "#         \"\"\"\n",
    "#         super(RPS_MNet, self).__init__()\n",
    "#         if n_times == 501:  # TODO automatic n_times\n",
    "#             self.n_times = 26\n",
    "#         elif n_times == 601:\n",
    "#             self.n_times = 18\n",
    "#         elif n_times == 701:\n",
    "#             self.n_times = 24\n",
    "#         else:\n",
    "#             raise ValueError(\n",
    "#                 \"Network can work only with n_times = 501, 601, 701 \"\n",
    "#                 \"(epoch duration of 1., 1.2, 1.4 sec),\"\n",
    "#                 \" got instead {}\".format(n_times)\n",
    "#             )\n",
    "\n",
    "#         self.spatial = nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, stride=(1, 1), kernel_size=[272, 64], bias=False),\n",
    "#             nn.ReLU(),\n",
    "#             # nn.BatchNorm2d(32),\n",
    "#             nn.Conv2d(32, 64, kernel_size=[1, 16], bias=False),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=[1, 2], stride=(1, 2)),\n",
    "#             # self.Showsize(),\n",
    "#             # nn.BatchNorm2d(64),\n",
    "#         )\n",
    "\n",
    "\n",
    "#         self.temporal = nn.Sequential(nn.Conv2d(1, 32, kernel_size=[8, 8], bias=True),\n",
    "#                                       nn.ReLU(),\n",
    "#                                       # self.Showsize(),\n",
    "#                                       # nn.BatchNorm2d(32),\n",
    "#                                       nn.Conv2d(32, 32, kernel_size=[8, 8], bias=True),\n",
    "#                                       nn.ReLU(),\n",
    "#                                       nn.MaxPool2d(kernel_size=[1, 3], stride=(1, 2)),\n",
    "#                                       # nn.BatchNorm2d(32),\n",
    "#                                       nn.Conv2d(32, 64, kernel_size=[6, 6], bias=True),\n",
    "#                                       nn.ReLU(),\n",
    "#                                       # nn.BatchNorm2d(64),\n",
    "#                                       nn.Conv2d(64, 64, kernel_size=[6, 6], bias=True),\n",
    "#                                       nn.ReLU(),\n",
    "#                                       # nn.BatchNorm2d(64),\n",
    "#                                       nn.MaxPool2d(kernel_size=[1, 2], stride=(1, 2)),\n",
    "#                                       nn.Conv2d(64, 128, kernel_size=[5, 5], bias=True),\n",
    "#                                       nn.ReLU(),\n",
    "#                                       nn.Dropout2d(p=0.3),\n",
    "#                                       # nn.BatchNorm2d(128),\n",
    "#                                       nn.Conv2d(128, 128, kernel_size=[5, 5], bias=True),\n",
    "#                                       nn.ReLU(),\n",
    "#                                       nn.Dropout2d(p=0.3),\n",
    "#                                       # nn.BatchNorm2d(128),\n",
    "#                                       nn.MaxPool2d(kernel_size=[1, 2], stride=(1, 2)),\n",
    "#                                       nn.Conv2d(128, 256, kernel_size=[4, 4], bias=True),\n",
    "#                                       nn.ReLU(),\n",
    "#                                       nn.Dropout2d(p=0.3),\n",
    "#                                       # nn.BatchNorm2d(256),\n",
    "#                                       nn.Conv2d(256, 256, kernel_size=[4, 4], bias=True),\n",
    "#                                       nn.ReLU(),\n",
    "#                                       nn.Dropout2d(p=0.3),\n",
    "#                                       # nn.BatchNorm2d(256),\n",
    "#                                       # self.Showsize(),\n",
    "#                                       )\n",
    "\n",
    "#         self.attention = nn.Sequential(\n",
    "#             ChannelAttention((None, 256, 26, self.n_times)),\n",
    "#         )\n",
    "\n",
    "#         self.attention2 = nn.Sequential(\n",
    "#             SpatialAttention(),\n",
    "#         )\n",
    "\n",
    "#         self.concatenate = Concatenate()\n",
    "\n",
    "#         # self.flatten = Flatten_MEG()\n",
    "\n",
    "#         self.ff = nn.Sequential(\n",
    "#             nn.Linear(256 * 26 * self.n_times + 272 * 6, 1024),\n",
    "#             nn.BatchNorm1d(num_features=1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 1024),\n",
    "#             nn.BatchNorm1d(num_features=1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 14),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, pb):\n",
    "#         x = self.spatial(x)\n",
    "#         x = torch.transpose(x, 1, 2)\n",
    "#         x = self.temporal(x)\n",
    "#         x = self.attention(x)\n",
    "#         x = self.attention2(x)\n",
    "#         x = self.concatenate(x, pb)\n",
    "#         x = self.ff(x)\n",
    "\n",
    "#         return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /users/k21116947/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "rpsmnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False).cuda()\n",
    "print(rpsmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPS_MNet(\n",
      "  (spatial): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(272, 64), stride=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Showsize()\n",
      "    (4): Conv2d(32, 64, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "    (5): ReLU()\n",
      "    (6): Showsize()\n",
      "    (7): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Showsize()\n",
      "  )\n",
      "  (temporal): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Showsize()\n",
      "    (3): Conv2d(32, 32, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): Showsize()\n",
      "    (6): MaxPool2d(kernel_size=(5, 3), stride=(5, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Showsize()\n",
      "    (8): Conv2d(32, 64, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): Showsize()\n",
      "    (11): Conv2d(64, 64, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Showsize()\n",
      "    (14): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Showsize()\n",
      "    (16): Conv2d(64, 128, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Dropout2d(p=0.3, inplace=False)\n",
      "    (19): Showsize()\n",
      "    (20): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "    (21): ReLU()\n",
      "    (22): Dropout2d(p=0.3, inplace=False)\n",
      "    (23): Showsize()\n",
      "    (24): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (25): Showsize()\n",
      "    (26): Conv2d(128, 256, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "    (27): ReLU()\n",
      "    (28): Dropout2d(p=0.3, inplace=False)\n",
      "    (29): Showsize()\n",
      "    (30): Conv2d(256, 256, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "    (31): ReLU()\n",
      "    (32): Dropout2d(p=0.3, inplace=False)\n",
      "    (33): Showsize()\n",
      "    (34): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (35): Showsize()\n",
      "  )\n",
      "  (concatenate): Concatenate()\n",
      "  (flatten): Flatten_MEG()\n",
      "  (ff1): Sequential(\n",
      "    (0): Linear(in_features=30720, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Showsize()\n",
      "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Showsize()\n",
      "  )\n",
      "  (ff2): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=2656, out_features=14, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rpsmnet = RPS_MNet(501).cuda()\n",
    "print(rpsmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+\n",
      "|           Modules            | Parameters |\n",
      "+------------------------------+------------+\n",
      "|         conv1.weight         |    9408    |\n",
      "|          bn1.weight          |     64     |\n",
      "|           bn1.bias           |     64     |\n",
      "|    layer1.0.conv1.weight     |   36864    |\n",
      "|     layer1.0.bn1.weight      |     64     |\n",
      "|      layer1.0.bn1.bias       |     64     |\n",
      "|    layer1.0.conv2.weight     |   36864    |\n",
      "|     layer1.0.bn2.weight      |     64     |\n",
      "|      layer1.0.bn2.bias       |     64     |\n",
      "|    layer1.1.conv1.weight     |   36864    |\n",
      "|     layer1.1.bn1.weight      |     64     |\n",
      "|      layer1.1.bn1.bias       |     64     |\n",
      "|    layer1.1.conv2.weight     |   36864    |\n",
      "|     layer1.1.bn2.weight      |     64     |\n",
      "|      layer1.1.bn2.bias       |     64     |\n",
      "|    layer2.0.conv1.weight     |   73728    |\n",
      "|     layer2.0.bn1.weight      |    128     |\n",
      "|      layer2.0.bn1.bias       |    128     |\n",
      "|    layer2.0.conv2.weight     |   147456   |\n",
      "|     layer2.0.bn2.weight      |    128     |\n",
      "|      layer2.0.bn2.bias       |    128     |\n",
      "| layer2.0.downsample.0.weight |    8192    |\n",
      "| layer2.0.downsample.1.weight |    128     |\n",
      "|  layer2.0.downsample.1.bias  |    128     |\n",
      "|    layer2.1.conv1.weight     |   147456   |\n",
      "|     layer2.1.bn1.weight      |    128     |\n",
      "|      layer2.1.bn1.bias       |    128     |\n",
      "|    layer2.1.conv2.weight     |   147456   |\n",
      "|     layer2.1.bn2.weight      |    128     |\n",
      "|      layer2.1.bn2.bias       |    128     |\n",
      "|    layer3.0.conv1.weight     |   294912   |\n",
      "|     layer3.0.bn1.weight      |    256     |\n",
      "|      layer3.0.bn1.bias       |    256     |\n",
      "|    layer3.0.conv2.weight     |   589824   |\n",
      "|     layer3.0.bn2.weight      |    256     |\n",
      "|      layer3.0.bn2.bias       |    256     |\n",
      "| layer3.0.downsample.0.weight |   32768    |\n",
      "| layer3.0.downsample.1.weight |    256     |\n",
      "|  layer3.0.downsample.1.bias  |    256     |\n",
      "|    layer3.1.conv1.weight     |   589824   |\n",
      "|     layer3.1.bn1.weight      |    256     |\n",
      "|      layer3.1.bn1.bias       |    256     |\n",
      "|    layer3.1.conv2.weight     |   589824   |\n",
      "|     layer3.1.bn2.weight      |    256     |\n",
      "|      layer3.1.bn2.bias       |    256     |\n",
      "|    layer4.0.conv1.weight     |  1179648   |\n",
      "|     layer4.0.bn1.weight      |    512     |\n",
      "|      layer4.0.bn1.bias       |    512     |\n",
      "|    layer4.0.conv2.weight     |  2359296   |\n",
      "|     layer4.0.bn2.weight      |    512     |\n",
      "|      layer4.0.bn2.bias       |    512     |\n",
      "| layer4.0.downsample.0.weight |   131072   |\n",
      "| layer4.0.downsample.1.weight |    512     |\n",
      "|  layer4.0.downsample.1.bias  |    512     |\n",
      "|    layer4.1.conv1.weight     |  2359296   |\n",
      "|     layer4.1.bn1.weight      |    512     |\n",
      "|      layer4.1.bn1.bias       |    512     |\n",
      "|    layer4.1.conv2.weight     |  2359296   |\n",
      "|     layer4.1.bn2.weight      |    512     |\n",
      "|      layer4.1.bn2.bias       |    512     |\n",
      "|          fc.weight           |   512000   |\n",
      "|           fc.bias            |    1000    |\n",
      "+------------------------------+------------+\n",
      "Total Trainable Params: 11689512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11689512"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(rpsmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0PFl-B9Gf5G"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "jJcV7N9KHWAU"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "num_epochs=1000\n",
    "train = Data.TensorDataset(X_train_tensors, y_train_tensors)#, bp_train_tensor)\n",
    "test = Data.TensorDataset(X_test_tensors, y_test_tensors)#, bp_test_tensor)\n",
    "train_loader = Data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = Data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "learning_rate = 0.005\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(rpsmnet.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "optimizer = torch.optim.SGD(rpsmnet.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "error",
     "timestamp": 1656336840520,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "lDvOt24MGf5L",
    "outputId": "bf0e73e4-4335-4e8f-c128-dd2635df4a37",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[1, 13] trainning loss: 66.66465449333191\u001b[31m\n",
      "\u001b[32m[1, 2] valid_loss: 5.586925745010376\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5869).  Saving model ...\n",
      "\u001b[31m[2, 13] trainning loss: 35.11576581001282\u001b[31m\n",
      "\u001b[32m[2, 2] valid_loss: 5.322537183761597\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3225).  Saving model ...\n",
      "\u001b[31m[3, 13] trainning loss: 34.90187215805054\u001b[31m\n",
      "\u001b[32m[3, 2] valid_loss: 5.355432748794556\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3554).  Saving model ...\n",
      "\u001b[31m[4, 13] trainning loss: 34.737406969070435\u001b[31m\n",
      "\u001b[32m[4, 2] valid_loss: 5.337846994400024\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3378).  Saving model ...\n",
      "\u001b[31m[5, 13] trainning loss: 34.32135057449341\u001b[31m\n",
      "\u001b[32m[5, 2] valid_loss: 5.325547933578491\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3255).  Saving model ...\n",
      "\u001b[31m[6, 13] trainning loss: 34.011072874069214\u001b[31m\n",
      "\u001b[32m[6, 2] valid_loss: 5.328355550765991\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3284).  Saving model ...\n",
      "\u001b[31m[7, 13] trainning loss: 33.63235950469971\u001b[31m\n",
      "\u001b[32m[7, 2] valid_loss: 5.321324110031128\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3213).  Saving model ...\n",
      "\u001b[31m[8, 13] trainning loss: 33.2021586894989\u001b[31m\n",
      "\u001b[32m[8, 2] valid_loss: 5.319192886352539\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3192).  Saving model ...\n",
      "\u001b[31m[9, 13] trainning loss: 32.78025269508362\u001b[31m\n",
      "\u001b[32m[9, 2] valid_loss: 5.329538106918335\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3295).  Saving model ...\n",
      "\u001b[31m[10, 13] trainning loss: 32.47366976737976\u001b[31m\n",
      "\u001b[32m[10, 2] valid_loss: 5.3304901123046875\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3305).  Saving model ...\n",
      "\u001b[31m[11, 13] trainning loss: 31.934398412704468\u001b[31m\n",
      "\u001b[32m[11, 2] valid_loss: 5.332944393157959\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3329).  Saving model ...\n",
      "\u001b[31m[12, 13] trainning loss: 31.284483194351196\u001b[31m\n",
      "\u001b[32m[12, 2] valid_loss: 5.331892967224121\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3319).  Saving model ...\n",
      "\u001b[31m[13, 13] trainning loss: 30.570389986038208\u001b[31m\n",
      "\u001b[32m[13, 2] valid_loss: 5.337430000305176\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3374).  Saving model ...\n",
      "\u001b[31m[14, 13] trainning loss: 29.63801670074463\u001b[31m\n",
      "\u001b[32m[14, 2] valid_loss: 5.355674505233765\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3557).  Saving model ...\n",
      "\u001b[31m[15, 13] trainning loss: 28.598461985588074\u001b[31m\n",
      "\u001b[32m[15, 2] valid_loss: 5.355066776275635\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3551).  Saving model ...\n",
      "\u001b[31m[16, 13] trainning loss: 27.544935703277588\u001b[31m\n",
      "\u001b[32m[16, 2] valid_loss: 5.36353325843811\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3635).  Saving model ...\n",
      "\u001b[31m[17, 13] trainning loss: 26.125004410743713\u001b[31m\n",
      "\u001b[32m[17, 2] valid_loss: 5.385148763656616\u001b[32m\n",
      "Validation loss decreased (inf --> 5.3851).  Saving model ...\n",
      "\u001b[31m[18, 13] trainning loss: 23.79218018054962\u001b[31m\n",
      "\u001b[32m[18, 2] valid_loss: 5.456417560577393\u001b[32m\n",
      "Validation loss decreased (inf --> 5.4564).  Saving model ...\n",
      "\u001b[31m[19, 13] trainning loss: 20.911933422088623\u001b[31m\n",
      "\u001b[32m[19, 2] valid_loss: 5.531461000442505\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5315).  Saving model ...\n",
      "\u001b[31m[20, 13] trainning loss: 17.353244304656982\u001b[31m\n",
      "\u001b[32m[20, 2] valid_loss: 5.517464876174927\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5175).  Saving model ...\n",
      "\u001b[31m[21, 13] trainning loss: 15.601335763931274\u001b[31m\n",
      "\u001b[32m[21, 2] valid_loss: 5.522885799407959\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5229).  Saving model ...\n",
      "\u001b[31m[22, 13] trainning loss: 14.73874419927597\u001b[31m\n",
      "\u001b[32m[22, 2] valid_loss: 5.540980100631714\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5410).  Saving model ...\n",
      "\u001b[31m[23, 13] trainning loss: 14.099944710731506\u001b[31m\n",
      "\u001b[32m[23, 2] valid_loss: 5.549380779266357\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5494).  Saving model ...\n",
      "\u001b[31m[24, 13] trainning loss: 13.478795945644379\u001b[31m\n",
      "\u001b[32m[24, 2] valid_loss: 5.561941862106323\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5619).  Saving model ...\n",
      "\u001b[31m[25, 13] trainning loss: 12.896373093128204\u001b[31m\n",
      "\u001b[32m[25, 2] valid_loss: 5.572376251220703\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5724).  Saving model ...\n",
      "\u001b[31m[26, 13] trainning loss: 12.34074318408966\u001b[31m\n",
      "\u001b[32m[26, 2] valid_loss: 5.589398622512817\u001b[32m\n",
      "Validation loss decreased (inf --> 5.5894).  Saving model ...\n",
      "\u001b[31m[27, 13] trainning loss: 11.798800826072693\u001b[31m\n",
      "\u001b[32m[27, 2] valid_loss: 5.604095935821533\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6041).  Saving model ...\n",
      "\u001b[31m[28, 13] trainning loss: 11.275161504745483\u001b[31m\n",
      "\u001b[32m[28, 2] valid_loss: 5.619028568267822\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6190).  Saving model ...\n",
      "\u001b[31m[29, 13] trainning loss: 10.769761770963669\u001b[31m\n",
      "\u001b[32m[29, 2] valid_loss: 5.634336233139038\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6343).  Saving model ...\n",
      "\u001b[31m[30, 13] trainning loss: 10.280967950820923\u001b[31m\n",
      "\u001b[32m[30, 2] valid_loss: 5.650506258010864\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6505).  Saving model ...\n",
      "\u001b[31m[31, 13] trainning loss: 9.920197188854218\u001b[31m\n",
      "\u001b[32m[31, 2] valid_loss: 5.653308629989624\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6533).  Saving model ...\n",
      "\u001b[31m[32, 13] trainning loss: 9.866453021764755\u001b[31m\n",
      "\u001b[32m[32, 2] valid_loss: 5.656393766403198\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6564).  Saving model ...\n",
      "\u001b[31m[33, 13] trainning loss: 9.818815797567368\u001b[31m\n",
      "\u001b[32m[33, 2] valid_loss: 5.658586263656616\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6586).  Saving model ...\n",
      "\u001b[31m[34, 13] trainning loss: 9.771198004484177\u001b[31m\n",
      "\u001b[32m[34, 2] valid_loss: 5.660106182098389\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6601).  Saving model ...\n",
      "\u001b[31m[35, 13] trainning loss: 9.725032895803452\u001b[31m\n",
      "\u001b[32m[35, 2] valid_loss: 5.662465810775757\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6625).  Saving model ...\n",
      "\u001b[31m[36, 13] trainning loss: 9.678901314735413\u001b[31m\n",
      "\u001b[32m[36, 2] valid_loss: 5.664176940917969\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6642).  Saving model ...\n",
      "\u001b[31m[37, 13] trainning loss: 9.63294106721878\u001b[31m\n",
      "\u001b[32m[37, 2] valid_loss: 5.665762186050415\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6658).  Saving model ...\n",
      "\u001b[31m[38, 13] trainning loss: 9.58704188466072\u001b[31m\n",
      "\u001b[32m[38, 2] valid_loss: 5.6677634716033936\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6678).  Saving model ...\n",
      "\u001b[31m[39, 13] trainning loss: 9.541813313961029\u001b[31m\n",
      "\u001b[32m[39, 2] valid_loss: 5.669740200042725\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6697).  Saving model ...\n",
      "\u001b[31m[40, 13] trainning loss: 9.496240645647049\u001b[31m\n",
      "\u001b[32m[40, 2] valid_loss: 5.671241998672485\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6712).  Saving model ...\n",
      "\u001b[31m[41, 13] trainning loss: 9.451275110244751\u001b[31m\n",
      "\u001b[32m[41, 2] valid_loss: 5.673389673233032\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6734).  Saving model ...\n",
      "\u001b[31m[42, 13] trainning loss: 9.418179035186768\u001b[31m\n",
      "\u001b[32m[42, 2] valid_loss: 5.673634052276611\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6736).  Saving model ...\n",
      "\u001b[31m[43, 13] trainning loss: 9.41368493437767\u001b[31m\n",
      "\u001b[32m[43, 2] valid_loss: 5.673734664916992\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6737).  Saving model ...\n",
      "\u001b[31m[44, 13] trainning loss: 9.409084022045135\u001b[31m\n",
      "\u001b[32m[44, 2] valid_loss: 5.6737892627716064\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6738).  Saving model ...\n",
      "\u001b[31m[45, 13] trainning loss: 9.404651045799255\u001b[31m\n",
      "\u001b[32m[45, 2] valid_loss: 5.674022197723389\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6740).  Saving model ...\n",
      "\u001b[31m[46, 13] trainning loss: 9.400089889764786\u001b[31m\n",
      "\u001b[32m[46, 2] valid_loss: 5.674169063568115\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6742).  Saving model ...\n",
      "\u001b[31m[47, 13] trainning loss: 9.395584434270859\u001b[31m\n",
      "\u001b[32m[47, 2] valid_loss: 5.674444675445557\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6744).  Saving model ...\n",
      "\u001b[31m[48, 13] trainning loss: 9.391201555728912\u001b[31m\n",
      "\u001b[32m[48, 2] valid_loss: 5.674582481384277\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6746).  Saving model ...\n",
      "\u001b[31m[49, 13] trainning loss: 9.38646885752678\u001b[31m\n",
      "\u001b[32m[49, 2] valid_loss: 5.674661636352539\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6747).  Saving model ...\n",
      "\u001b[31m[50, 13] trainning loss: 9.382226884365082\u001b[31m\n",
      "\u001b[32m[50, 2] valid_loss: 5.674875259399414\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6749).  Saving model ...\n",
      "\u001b[31m[51, 13] trainning loss: 9.377840787172318\u001b[31m\n",
      "\u001b[32m[51, 2] valid_loss: 5.675078630447388\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6751).  Saving model ...\n",
      "\u001b[31m[52, 13] trainning loss: 9.37338075041771\u001b[31m\n",
      "\u001b[32m[52, 2] valid_loss: 5.6753294467926025\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[53, 13] trainning loss: 9.370038717985153\u001b[31m\n",
      "\u001b[32m[53, 2] valid_loss: 5.675292015075684\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[54, 13] trainning loss: 9.36954540014267\u001b[31m\n",
      "\u001b[32m[54, 2] valid_loss: 5.675359487533569\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[55, 13] trainning loss: 9.369182735681534\u001b[31m\n",
      "\u001b[32m[55, 2] valid_loss: 5.675207853317261\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6752).  Saving model ...\n",
      "\u001b[31m[56, 13] trainning loss: 9.368814826011658\u001b[31m\n",
      "\u001b[32m[56, 2] valid_loss: 5.675374269485474\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[57, 13] trainning loss: 9.3684022128582\u001b[31m\n",
      "\u001b[32m[57, 2] valid_loss: 5.6753270626068115\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[58, 13] trainning loss: 9.36812248826027\u001b[31m\n",
      "\u001b[32m[58, 2] valid_loss: 5.675522565841675\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[59, 13] trainning loss: 9.367586821317673\u001b[31m\n",
      "\u001b[32m[59, 2] valid_loss: 5.675304174423218\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[60, 13] trainning loss: 9.367355793714523\u001b[31m\n",
      "\u001b[32m[60, 2] valid_loss: 5.675463676452637\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[61, 13] trainning loss: 9.36679893732071\u001b[31m\n",
      "\u001b[32m[61, 2] valid_loss: 5.675475597381592\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[62, 13] trainning loss: 9.366436153650284\u001b[31m\n",
      "\u001b[32m[62, 2] valid_loss: 5.675493001937866\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[63, 13] trainning loss: 9.366020888090134\u001b[31m\n",
      "\u001b[32m[63, 2] valid_loss: 5.675207614898682\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6752).  Saving model ...\n",
      "\u001b[31m[64, 13] trainning loss: 9.365566790103912\u001b[31m\n",
      "\u001b[32m[64, 2] valid_loss: 5.675224304199219\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6752).  Saving model ...\n",
      "\u001b[31m[65, 13] trainning loss: 9.365705728530884\u001b[31m\n",
      "\u001b[32m[65, 2] valid_loss: 5.675361633300781\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[66, 13] trainning loss: 9.365659475326538\u001b[31m\n",
      "\u001b[32m[66, 2] valid_loss: 5.67510199546814\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6751).  Saving model ...\n",
      "\u001b[31m[67, 13] trainning loss: 9.365652322769165\u001b[31m\n",
      "\u001b[32m[67, 2] valid_loss: 5.675380229949951\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[68, 13] trainning loss: 9.365648835897446\u001b[31m\n",
      "\u001b[32m[68, 2] valid_loss: 5.675429344177246\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[69, 13] trainning loss: 9.365517675876617\u001b[31m\n",
      "\u001b[32m[69, 2] valid_loss: 5.675326347351074\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[70, 13] trainning loss: 9.365604966878891\u001b[31m\n",
      "\u001b[32m[70, 2] valid_loss: 5.675341367721558\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[71, 13] trainning loss: 9.365537375211716\u001b[31m\n",
      "\u001b[32m[71, 2] valid_loss: 5.675322532653809\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[72, 13] trainning loss: 9.36549386382103\u001b[31m\n",
      "\u001b[32m[72, 2] valid_loss: 5.675297260284424\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[73, 13] trainning loss: 9.365514367818832\u001b[31m\n",
      "\u001b[32m[73, 2] valid_loss: 5.6754677295684814\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[74, 13] trainning loss: 9.365507572889328\u001b[31m\n",
      "\u001b[32m[74, 2] valid_loss: 5.6755146980285645\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[75, 13] trainning loss: 9.365567296743393\u001b[31m\n",
      "\u001b[32m[75, 2] valid_loss: 5.675344228744507\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[76, 13] trainning loss: 9.365536361932755\u001b[31m\n",
      "\u001b[32m[76, 2] valid_loss: 5.6753528118133545\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[77, 13] trainning loss: 9.36552357673645\u001b[31m\n",
      "\u001b[32m[77, 2] valid_loss: 5.6753973960876465\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[78, 13] trainning loss: 9.365464866161346\u001b[31m\n",
      "\u001b[32m[78, 2] valid_loss: 5.675370931625366\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[79, 13] trainning loss: 9.365497797727585\u001b[31m\n",
      "\u001b[32m[79, 2] valid_loss: 5.67546010017395\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[80, 13] trainning loss: 9.365521788597107\u001b[31m\n",
      "\u001b[32m[80, 2] valid_loss: 5.675373315811157\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[81, 13] trainning loss: 9.365429282188416\u001b[31m\n",
      "\u001b[32m[81, 2] valid_loss: 5.675427198410034\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[82, 13] trainning loss: 9.365535616874695\u001b[31m\n",
      "\u001b[32m[82, 2] valid_loss: 5.675380229949951\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[83, 13] trainning loss: 9.365548491477966\u001b[31m\n",
      "\u001b[32m[83, 2] valid_loss: 5.67536187171936\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[84, 13] trainning loss: 9.365437775850296\u001b[31m\n",
      "\u001b[32m[84, 2] valid_loss: 5.6754539012908936\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[85, 13] trainning loss: 9.365580767393112\u001b[31m\n",
      "\u001b[32m[85, 2] valid_loss: 5.675326347351074\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[86, 13] trainning loss: 9.365515857934952\u001b[31m\n",
      "\u001b[32m[86, 2] valid_loss: 5.675281763076782\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[87, 13] trainning loss: 9.365554392337799\u001b[31m\n",
      "\u001b[32m[87, 2] valid_loss: 5.675474166870117\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[88, 13] trainning loss: 9.36550885438919\u001b[31m\n",
      "\u001b[32m[88, 2] valid_loss: 5.675444841384888\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[89, 13] trainning loss: 9.36548736691475\u001b[31m\n",
      "\u001b[32m[89, 2] valid_loss: 5.675428152084351\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[90, 13] trainning loss: 9.36551433801651\u001b[31m\n",
      "\u001b[32m[90, 2] valid_loss: 5.675450563430786\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[91, 13] trainning loss: 9.365531653165817\u001b[31m\n",
      "\u001b[32m[91, 2] valid_loss: 5.675403118133545\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[92, 13] trainning loss: 9.365449905395508\u001b[31m\n",
      "\u001b[32m[92, 2] valid_loss: 5.6755170822143555\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[93, 13] trainning loss: 9.365570724010468\u001b[31m\n",
      "\u001b[32m[93, 2] valid_loss: 5.67555046081543\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[94, 13] trainning loss: 9.365495890378952\u001b[31m\n",
      "\u001b[32m[94, 2] valid_loss: 5.675475597381592\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[95, 13] trainning loss: 9.365512490272522\u001b[31m\n",
      "\u001b[32m[95, 2] valid_loss: 5.675570487976074\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[96, 13] trainning loss: 9.365537315607071\u001b[31m\n",
      "\u001b[32m[96, 2] valid_loss: 5.675431728363037\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[97, 13] trainning loss: 9.365515232086182\u001b[31m\n",
      "\u001b[32m[97, 2] valid_loss: 5.67548131942749\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[98, 13] trainning loss: 9.365510731935501\u001b[31m\n",
      "\u001b[32m[98, 2] valid_loss: 5.675400972366333\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[99, 13] trainning loss: 9.365546584129333\u001b[31m\n",
      "\u001b[32m[99, 2] valid_loss: 5.675597667694092\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[100, 13] trainning loss: 9.365538448095322\u001b[31m\n",
      "\u001b[32m[100, 2] valid_loss: 5.675533294677734\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[101, 13] trainning loss: 9.36564752459526\u001b[31m\n",
      "\u001b[32m[101, 2] valid_loss: 5.675554990768433\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[102, 13] trainning loss: 9.36549997329712\u001b[31m\n",
      "\u001b[32m[102, 2] valid_loss: 5.675508737564087\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[103, 13] trainning loss: 9.365564346313477\u001b[31m\n",
      "\u001b[32m[103, 2] valid_loss: 5.675519704818726\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[104, 13] trainning loss: 9.36558112502098\u001b[31m\n",
      "\u001b[32m[104, 2] valid_loss: 5.675544500350952\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[105, 13] trainning loss: 9.365524709224701\u001b[31m\n",
      "\u001b[32m[105, 2] valid_loss: 5.675541162490845\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[106, 13] trainning loss: 9.365522474050522\u001b[31m\n",
      "\u001b[32m[106, 2] valid_loss: 5.675464630126953\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[107, 13] trainning loss: 9.36547476053238\u001b[31m\n",
      "\u001b[32m[107, 2] valid_loss: 5.675443172454834\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[108, 13] trainning loss: 9.365594118833542\u001b[31m\n",
      "\u001b[32m[108, 2] valid_loss: 5.675442457199097\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[109, 13] trainning loss: 9.365644335746765\u001b[31m\n",
      "\u001b[32m[109, 2] valid_loss: 5.675471067428589\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[110, 13] trainning loss: 9.365621000528336\u001b[31m\n",
      "\u001b[32m[110, 2] valid_loss: 5.675539970397949\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[111, 13] trainning loss: 9.365604788064957\u001b[31m\n",
      "\u001b[32m[111, 2] valid_loss: 5.675479173660278\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[112, 13] trainning loss: 9.36560544371605\u001b[31m\n",
      "\u001b[32m[112, 2] valid_loss: 5.675539493560791\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[113, 13] trainning loss: 9.365702271461487\u001b[31m\n",
      "\u001b[32m[113, 2] valid_loss: 5.675534963607788\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[114, 13] trainning loss: 9.365613341331482\u001b[31m\n",
      "\u001b[32m[114, 2] valid_loss: 5.675374269485474\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[115, 13] trainning loss: 9.365551471710205\u001b[31m\n",
      "\u001b[32m[115, 2] valid_loss: 5.675503730773926\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[116, 13] trainning loss: 9.365649610757828\u001b[31m\n",
      "\u001b[32m[116, 2] valid_loss: 5.6755945682525635\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[117, 13] trainning loss: 9.36555141210556\u001b[31m\n",
      "\u001b[32m[117, 2] valid_loss: 5.675598382949829\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[118, 13] trainning loss: 9.36557736992836\u001b[31m\n",
      "\u001b[32m[118, 2] valid_loss: 5.675446033477783\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[119, 13] trainning loss: 9.365572154521942\u001b[31m\n",
      "\u001b[32m[119, 2] valid_loss: 5.675591230392456\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[120, 13] trainning loss: 9.36563789844513\u001b[31m\n",
      "\u001b[32m[120, 2] valid_loss: 5.675663709640503\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[121, 13] trainning loss: 9.365580886602402\u001b[31m\n",
      "\u001b[32m[121, 2] valid_loss: 5.675614595413208\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[122, 13] trainning loss: 9.365540653467178\u001b[31m\n",
      "\u001b[32m[122, 2] valid_loss: 5.675649642944336\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[123, 13] trainning loss: 9.365660727024078\u001b[31m\n",
      "\u001b[32m[123, 2] valid_loss: 5.675510406494141\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[124, 13] trainning loss: 9.365649431943893\u001b[31m\n",
      "\u001b[32m[124, 2] valid_loss: 5.6754844188690186\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[125, 13] trainning loss: 9.365691035985947\u001b[31m\n",
      "\u001b[32m[125, 2] valid_loss: 5.6755499839782715\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[126, 13] trainning loss: 9.365639418363571\u001b[31m\n",
      "\u001b[32m[126, 2] valid_loss: 5.675436496734619\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[127, 13] trainning loss: 9.365605622529984\u001b[31m\n",
      "\u001b[32m[127, 2] valid_loss: 5.675431251525879\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[128, 13] trainning loss: 9.365559130907059\u001b[31m\n",
      "\u001b[32m[128, 2] valid_loss: 5.675462245941162\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[129, 13] trainning loss: 9.365638822317123\u001b[31m\n",
      "\u001b[32m[129, 2] valid_loss: 5.675570011138916\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[130, 13] trainning loss: 9.365576177835464\u001b[31m\n",
      "\u001b[32m[130, 2] valid_loss: 5.6755595207214355\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[131, 13] trainning loss: 9.365550935268402\u001b[31m\n",
      "\u001b[32m[131, 2] valid_loss: 5.675419092178345\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[132, 13] trainning loss: 9.365545809268951\u001b[31m\n",
      "\u001b[32m[132, 2] valid_loss: 5.675535202026367\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[133, 13] trainning loss: 9.365647494792938\u001b[31m\n",
      "\u001b[32m[133, 2] valid_loss: 5.675473213195801\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[134, 13] trainning loss: 9.365594893693924\u001b[31m\n",
      "\u001b[32m[134, 2] valid_loss: 5.675515174865723\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[135, 13] trainning loss: 9.365520298480988\u001b[31m\n",
      "\u001b[32m[135, 2] valid_loss: 5.675500392913818\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[136, 13] trainning loss: 9.365583896636963\u001b[31m\n",
      "\u001b[32m[136, 2] valid_loss: 5.67550253868103\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[137, 13] trainning loss: 9.365480482578278\u001b[31m\n",
      "\u001b[32m[137, 2] valid_loss: 5.675518751144409\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[138, 13] trainning loss: 9.365520805120468\u001b[31m\n",
      "\u001b[32m[138, 2] valid_loss: 5.675559997558594\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[139, 13] trainning loss: 9.365486532449722\u001b[31m\n",
      "\u001b[32m[139, 2] valid_loss: 5.675481557846069\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[140, 13] trainning loss: 9.365574598312378\u001b[31m\n",
      "\u001b[32m[140, 2] valid_loss: 5.675556182861328\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[141, 13] trainning loss: 9.365446001291275\u001b[31m\n",
      "\u001b[32m[141, 2] valid_loss: 5.6755406856536865\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[142, 13] trainning loss: 9.365518987178802\u001b[31m\n",
      "\u001b[32m[142, 2] valid_loss: 5.675477027893066\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[143, 13] trainning loss: 9.365472555160522\u001b[31m\n",
      "\u001b[32m[143, 2] valid_loss: 5.6754560470581055\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[144, 13] trainning loss: 9.36545506119728\u001b[31m\n",
      "\u001b[32m[144, 2] valid_loss: 5.675527811050415\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[145, 13] trainning loss: 9.365559101104736\u001b[31m\n",
      "\u001b[32m[145, 2] valid_loss: 5.675508975982666\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[146, 13] trainning loss: 9.365555584430695\u001b[31m\n",
      "\u001b[32m[146, 2] valid_loss: 5.675616025924683\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[147, 13] trainning loss: 9.365602493286133\u001b[31m\n",
      "\u001b[32m[147, 2] valid_loss: 5.675559997558594\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[148, 13] trainning loss: 9.365509420633316\u001b[31m\n",
      "\u001b[32m[148, 2] valid_loss: 5.675522327423096\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[149, 13] trainning loss: 9.365570843219757\u001b[31m\n",
      "\u001b[32m[149, 2] valid_loss: 5.675604820251465\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[150, 13] trainning loss: 9.365554809570312\u001b[31m\n",
      "\u001b[32m[150, 2] valid_loss: 5.675633668899536\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[151, 13] trainning loss: 9.365575820207596\u001b[31m\n",
      "\u001b[32m[151, 2] valid_loss: 5.675563812255859\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[152, 13] trainning loss: 9.365577727556229\u001b[31m\n",
      "\u001b[32m[152, 2] valid_loss: 5.675705671310425\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[153, 13] trainning loss: 9.365556865930557\u001b[31m\n",
      "\u001b[32m[153, 2] valid_loss: 5.675609827041626\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[154, 13] trainning loss: 9.36551907658577\u001b[31m\n",
      "\u001b[32m[154, 2] valid_loss: 5.675629615783691\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[155, 13] trainning loss: 9.365448504686356\u001b[31m\n",
      "\u001b[32m[155, 2] valid_loss: 5.675493001937866\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[156, 13] trainning loss: 9.365578085184097\u001b[31m\n",
      "\u001b[32m[156, 2] valid_loss: 5.675538063049316\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[157, 13] trainning loss: 9.365602135658264\u001b[31m\n",
      "\u001b[32m[157, 2] valid_loss: 5.675657033920288\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[158, 13] trainning loss: 9.36553743481636\u001b[31m\n",
      "\u001b[32m[158, 2] valid_loss: 5.67563796043396\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[159, 13] trainning loss: 9.365640044212341\u001b[31m\n",
      "\u001b[32m[159, 2] valid_loss: 5.675434827804565\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[160, 13] trainning loss: 9.365510016679764\u001b[31m\n",
      "\u001b[32m[160, 2] valid_loss: 5.675676107406616\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[161, 13] trainning loss: 9.365540355443954\u001b[31m\n",
      "\u001b[32m[161, 2] valid_loss: 5.675557613372803\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[162, 13] trainning loss: 9.365563809871674\u001b[31m\n",
      "\u001b[32m[162, 2] valid_loss: 5.675564527511597\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[163, 13] trainning loss: 9.36561068892479\u001b[31m\n",
      "\u001b[32m[163, 2] valid_loss: 5.675500392913818\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[164, 13] trainning loss: 9.365617722272873\u001b[31m\n",
      "\u001b[32m[164, 2] valid_loss: 5.675493001937866\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[165, 13] trainning loss: 9.365519732236862\u001b[31m\n",
      "\u001b[32m[165, 2] valid_loss: 5.675513029098511\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[166, 13] trainning loss: 9.365521401166916\u001b[31m\n",
      "\u001b[32m[166, 2] valid_loss: 5.675541639328003\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[167, 13] trainning loss: 9.365657597780228\u001b[31m\n",
      "\u001b[32m[167, 2] valid_loss: 5.675662279129028\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[168, 13] trainning loss: 9.365523546934128\u001b[31m\n",
      "\u001b[32m[168, 2] valid_loss: 5.67552924156189\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[169, 13] trainning loss: 9.365643084049225\u001b[31m\n",
      "\u001b[32m[169, 2] valid_loss: 5.675565958023071\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[170, 13] trainning loss: 9.365549743175507\u001b[31m\n",
      "\u001b[32m[170, 2] valid_loss: 5.675515413284302\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[171, 13] trainning loss: 9.365589320659637\u001b[31m\n",
      "\u001b[32m[171, 2] valid_loss: 5.675471305847168\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[172, 13] trainning loss: 9.36569920182228\u001b[31m\n",
      "\u001b[32m[172, 2] valid_loss: 5.675610065460205\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[173, 13] trainning loss: 9.365558922290802\u001b[31m\n",
      "\u001b[32m[173, 2] valid_loss: 5.67562460899353\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[174, 13] trainning loss: 9.365607649087906\u001b[31m\n",
      "\u001b[32m[174, 2] valid_loss: 5.675667762756348\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[175, 13] trainning loss: 9.365548998117447\u001b[31m\n",
      "\u001b[32m[175, 2] valid_loss: 5.675504446029663\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[176, 13] trainning loss: 9.36556726694107\u001b[31m\n",
      "\u001b[32m[176, 2] valid_loss: 5.675523281097412\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[177, 13] trainning loss: 9.365512907505035\u001b[31m\n",
      "\u001b[32m[177, 2] valid_loss: 5.675626754760742\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[178, 13] trainning loss: 9.365678608417511\u001b[31m\n",
      "\u001b[32m[178, 2] valid_loss: 5.675590991973877\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[179, 13] trainning loss: 9.365499585866928\u001b[31m\n",
      "\u001b[32m[179, 2] valid_loss: 5.675424575805664\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[180, 13] trainning loss: 9.365620017051697\u001b[31m\n",
      "\u001b[32m[180, 2] valid_loss: 5.675567150115967\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[181, 13] trainning loss: 9.365563988685608\u001b[31m\n",
      "\u001b[32m[181, 2] valid_loss: 5.675584316253662\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[182, 13] trainning loss: 9.365581423044205\u001b[31m\n",
      "\u001b[32m[182, 2] valid_loss: 5.67557954788208\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[183, 13] trainning loss: 9.365646779537201\u001b[31m\n",
      "\u001b[32m[183, 2] valid_loss: 5.675655126571655\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[184, 13] trainning loss: 9.365615278482437\u001b[31m\n",
      "\u001b[32m[184, 2] valid_loss: 5.675655126571655\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[185, 13] trainning loss: 9.365452736616135\u001b[31m\n",
      "\u001b[32m[185, 2] valid_loss: 5.675527095794678\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[186, 13] trainning loss: 9.365585327148438\u001b[31m\n",
      "\u001b[32m[186, 2] valid_loss: 5.675421714782715\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[187, 13] trainning loss: 9.365601122379303\u001b[31m\n",
      "\u001b[32m[187, 2] valid_loss: 5.67549467086792\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[188, 13] trainning loss: 9.365603357553482\u001b[31m\n",
      "\u001b[32m[188, 2] valid_loss: 5.675457954406738\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[189, 13] trainning loss: 9.365488529205322\u001b[31m\n",
      "\u001b[32m[189, 2] valid_loss: 5.675478219985962\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[190, 13] trainning loss: 9.365645974874496\u001b[31m\n",
      "\u001b[32m[190, 2] valid_loss: 5.675463438034058\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[191, 13] trainning loss: 9.36560520529747\u001b[31m\n",
      "\u001b[32m[191, 2] valid_loss: 5.675527811050415\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[192, 13] trainning loss: 9.36562004685402\u001b[31m\n",
      "\u001b[32m[192, 2] valid_loss: 5.675468683242798\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[193, 13] trainning loss: 9.3655626475811\u001b[31m\n",
      "\u001b[32m[193, 2] valid_loss: 5.675628900527954\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[194, 13] trainning loss: 9.365572333335876\u001b[31m\n",
      "\u001b[32m[194, 2] valid_loss: 5.675489187240601\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[195, 13] trainning loss: 9.365630447864532\u001b[31m\n",
      "\u001b[32m[195, 2] valid_loss: 5.675451278686523\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[196, 13] trainning loss: 9.365547984838486\u001b[31m\n",
      "\u001b[32m[196, 2] valid_loss: 5.675408124923706\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[197, 13] trainning loss: 9.365526467561722\u001b[31m\n",
      "\u001b[32m[197, 2] valid_loss: 5.6754796504974365\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[198, 13] trainning loss: 9.36551359295845\u001b[31m\n",
      "\u001b[32m[198, 2] valid_loss: 5.675427198410034\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[199, 13] trainning loss: 9.365546524524689\u001b[31m\n",
      "\u001b[32m[199, 2] valid_loss: 5.675491571426392\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[200, 13] trainning loss: 9.365526556968689\u001b[31m\n",
      "\u001b[32m[200, 2] valid_loss: 5.67546534538269\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[201, 13] trainning loss: 9.365505009889603\u001b[31m\n",
      "\u001b[32m[201, 2] valid_loss: 5.675458669662476\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[202, 13] trainning loss: 9.365542322397232\u001b[31m\n",
      "\u001b[32m[202, 2] valid_loss: 5.675537586212158\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[203, 13] trainning loss: 9.36546528339386\u001b[31m\n",
      "\u001b[32m[203, 2] valid_loss: 5.675483465194702\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[204, 13] trainning loss: 9.365516304969788\u001b[31m\n",
      "\u001b[32m[204, 2] valid_loss: 5.675445795059204\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[205, 13] trainning loss: 9.365463972091675\u001b[31m\n",
      "\u001b[32m[205, 2] valid_loss: 5.675427436828613\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[206, 13] trainning loss: 9.365603625774384\u001b[31m\n",
      "\u001b[32m[206, 2] valid_loss: 5.675558090209961\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[207, 13] trainning loss: 9.365554630756378\u001b[31m\n",
      "\u001b[32m[207, 2] valid_loss: 5.675474643707275\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[208, 13] trainning loss: 9.365486562252045\u001b[31m\n",
      "\u001b[32m[208, 2] valid_loss: 5.675452947616577\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[209, 13] trainning loss: 9.365506827831268\u001b[31m\n",
      "\u001b[32m[209, 2] valid_loss: 5.675574541091919\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[210, 13] trainning loss: 9.365425378084183\u001b[31m\n",
      "\u001b[32m[210, 2] valid_loss: 5.675449371337891\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[211, 13] trainning loss: 9.365533858537674\u001b[31m\n",
      "\u001b[32m[211, 2] valid_loss: 5.675554990768433\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[212, 13] trainning loss: 9.365497201681137\u001b[31m\n",
      "\u001b[32m[212, 2] valid_loss: 5.675461292266846\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[213, 13] trainning loss: 9.365490704774857\u001b[31m\n",
      "\u001b[32m[213, 2] valid_loss: 5.675519227981567\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[214, 13] trainning loss: 9.365462601184845\u001b[31m\n",
      "\u001b[32m[214, 2] valid_loss: 5.675643682479858\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[215, 13] trainning loss: 9.36547327041626\u001b[31m\n",
      "\u001b[32m[215, 2] valid_loss: 5.675565004348755\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[216, 13] trainning loss: 9.36551421880722\u001b[31m\n",
      "\u001b[32m[216, 2] valid_loss: 5.675620079040527\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[217, 13] trainning loss: 9.365496218204498\u001b[31m\n",
      "\u001b[32m[217, 2] valid_loss: 5.675532341003418\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[218, 13] trainning loss: 9.365494012832642\u001b[31m\n",
      "\u001b[32m[218, 2] valid_loss: 5.6756391525268555\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[219, 13] trainning loss: 9.365494459867477\u001b[31m\n",
      "\u001b[32m[219, 2] valid_loss: 5.675488471984863\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[220, 13] trainning loss: 9.365472078323364\u001b[31m\n",
      "\u001b[32m[220, 2] valid_loss: 5.675402402877808\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[221, 13] trainning loss: 9.365462958812714\u001b[31m\n",
      "\u001b[32m[221, 2] valid_loss: 5.675332307815552\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[222, 13] trainning loss: 9.365473449230194\u001b[31m\n",
      "\u001b[32m[222, 2] valid_loss: 5.675419569015503\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[223, 13] trainning loss: 9.365473836660385\u001b[31m\n",
      "\u001b[32m[223, 2] valid_loss: 5.675496578216553\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[224, 13] trainning loss: 9.36552083492279\u001b[31m\n",
      "\u001b[32m[224, 2] valid_loss: 5.675414800643921\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[225, 13] trainning loss: 9.365478932857513\u001b[31m\n",
      "\u001b[32m[225, 2] valid_loss: 5.675370931625366\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[226, 13] trainning loss: 9.365557610988617\u001b[31m\n",
      "\u001b[32m[226, 2] valid_loss: 5.6754150390625\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[227, 13] trainning loss: 9.365558236837387\u001b[31m\n",
      "\u001b[32m[227, 2] valid_loss: 5.675536155700684\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[228, 13] trainning loss: 9.365564703941345\u001b[31m\n",
      "\u001b[32m[228, 2] valid_loss: 5.675461530685425\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[229, 13] trainning loss: 9.36550360918045\u001b[31m\n",
      "\u001b[32m[229, 2] valid_loss: 5.67538595199585\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[230, 13] trainning loss: 9.365559875965118\u001b[31m\n",
      "\u001b[32m[230, 2] valid_loss: 5.675488471984863\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[231, 13] trainning loss: 9.365511536598206\u001b[31m\n",
      "\u001b[32m[231, 2] valid_loss: 5.675373554229736\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[232, 13] trainning loss: 9.36547076702118\u001b[31m\n",
      "\u001b[32m[232, 2] valid_loss: 5.675410985946655\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[233, 13] trainning loss: 9.365523904561996\u001b[31m\n",
      "\u001b[32m[233, 2] valid_loss: 5.6753249168396\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[234, 13] trainning loss: 9.365519046783447\u001b[31m\n",
      "\u001b[32m[234, 2] valid_loss: 5.675448894500732\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[235, 13] trainning loss: 9.365505427122116\u001b[31m\n",
      "\u001b[32m[235, 2] valid_loss: 5.675449848175049\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[236, 13] trainning loss: 9.3655526638031\u001b[31m\n",
      "\u001b[32m[236, 2] valid_loss: 5.675495147705078\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[237, 13] trainning loss: 9.365489423274994\u001b[31m\n",
      "\u001b[32m[237, 2] valid_loss: 5.675501585006714\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[238, 13] trainning loss: 9.365469843149185\u001b[31m\n",
      "\u001b[32m[238, 2] valid_loss: 5.675342798233032\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[239, 13] trainning loss: 9.365483403205872\u001b[31m\n",
      "\u001b[32m[239, 2] valid_loss: 5.675323486328125\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[240, 13] trainning loss: 9.365513354539871\u001b[31m\n",
      "\u001b[32m[240, 2] valid_loss: 5.67530632019043\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[241, 13] trainning loss: 9.365550130605698\u001b[31m\n",
      "\u001b[32m[241, 2] valid_loss: 5.675494194030762\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[242, 13] trainning loss: 9.365437120199203\u001b[31m\n",
      "\u001b[32m[242, 2] valid_loss: 5.675434112548828\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[243, 13] trainning loss: 9.365429520606995\u001b[31m\n",
      "\u001b[32m[243, 2] valid_loss: 5.675483465194702\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[244, 13] trainning loss: 9.36547264456749\u001b[31m\n",
      "\u001b[32m[244, 2] valid_loss: 5.675442695617676\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[245, 13] trainning loss: 9.365463525056839\u001b[31m\n",
      "\u001b[32m[245, 2] valid_loss: 5.675362825393677\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[246, 13] trainning loss: 9.365558087825775\u001b[31m\n",
      "\u001b[32m[246, 2] valid_loss: 5.675329208374023\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[247, 13] trainning loss: 9.365523546934128\u001b[31m\n",
      "\u001b[32m[247, 2] valid_loss: 5.675455093383789\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[248, 13] trainning loss: 9.365568906068802\u001b[31m\n",
      "\u001b[32m[248, 2] valid_loss: 5.675458669662476\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[249, 13] trainning loss: 9.365441709756851\u001b[31m\n",
      "\u001b[32m[249, 2] valid_loss: 5.6754844188690186\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[250, 13] trainning loss: 9.365535110235214\u001b[31m\n",
      "\u001b[32m[250, 2] valid_loss: 5.675489902496338\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[251, 13] trainning loss: 9.365616112947464\u001b[31m\n",
      "\u001b[32m[251, 2] valid_loss: 5.67547869682312\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[252, 13] trainning loss: 9.36555102467537\u001b[31m\n",
      "\u001b[32m[252, 2] valid_loss: 5.675459861755371\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[253, 13] trainning loss: 9.365508645772934\u001b[31m\n",
      "\u001b[32m[253, 2] valid_loss: 5.675506830215454\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[254, 13] trainning loss: 9.365496635437012\u001b[31m\n",
      "\u001b[32m[254, 2] valid_loss: 5.6754701137542725\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[255, 13] trainning loss: 9.365501403808594\u001b[31m\n",
      "\u001b[32m[255, 2] valid_loss: 5.675475358963013\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[256, 13] trainning loss: 9.365511417388916\u001b[31m\n",
      "\u001b[32m[256, 2] valid_loss: 5.67545223236084\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[257, 13] trainning loss: 9.365440279245377\u001b[31m\n",
      "\u001b[32m[257, 2] valid_loss: 5.675528049468994\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[258, 13] trainning loss: 9.36556214094162\u001b[31m\n",
      "\u001b[32m[258, 2] valid_loss: 5.675503253936768\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[259, 13] trainning loss: 9.365495920181274\u001b[31m\n",
      "\u001b[32m[259, 2] valid_loss: 5.675502777099609\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[260, 13] trainning loss: 9.36553543806076\u001b[31m\n",
      "\u001b[32m[260, 2] valid_loss: 5.675494909286499\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[261, 13] trainning loss: 9.365602314472198\u001b[31m\n",
      "\u001b[32m[261, 2] valid_loss: 5.675562143325806\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[262, 13] trainning loss: 9.365524530410767\u001b[31m\n",
      "\u001b[32m[262, 2] valid_loss: 5.675457239151001\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[263, 13] trainning loss: 9.365628480911255\u001b[31m\n",
      "\u001b[32m[263, 2] valid_loss: 5.675455570220947\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[264, 13] trainning loss: 9.365584820508957\u001b[31m\n",
      "\u001b[32m[264, 2] valid_loss: 5.675383567810059\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[265, 13] trainning loss: 9.365479618310928\u001b[31m\n",
      "\u001b[32m[265, 2] valid_loss: 5.675532817840576\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[266, 13] trainning loss: 9.365503400564194\u001b[31m\n",
      "\u001b[32m[266, 2] valid_loss: 5.675479173660278\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[267, 13] trainning loss: 9.365542978048325\u001b[31m\n",
      "\u001b[32m[267, 2] valid_loss: 5.675416946411133\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[268, 13] trainning loss: 9.365569293498993\u001b[31m\n",
      "\u001b[32m[268, 2] valid_loss: 5.675405025482178\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[269, 13] trainning loss: 9.365481555461884\u001b[31m\n",
      "\u001b[32m[269, 2] valid_loss: 5.675516366958618\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[270, 13] trainning loss: 9.365485578775406\u001b[31m\n",
      "\u001b[32m[270, 2] valid_loss: 5.6755876541137695\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[271, 13] trainning loss: 9.365510255098343\u001b[31m\n",
      "\u001b[32m[271, 2] valid_loss: 5.675533294677734\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[272, 13] trainning loss: 9.365484684705734\u001b[31m\n",
      "\u001b[32m[272, 2] valid_loss: 5.675567626953125\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[273, 13] trainning loss: 9.365442901849747\u001b[31m\n",
      "\u001b[32m[273, 2] valid_loss: 5.675470590591431\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[274, 13] trainning loss: 9.365481495857239\u001b[31m\n",
      "\u001b[32m[274, 2] valid_loss: 5.675611972808838\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[275, 13] trainning loss: 9.36548176407814\u001b[31m\n",
      "\u001b[32m[275, 2] valid_loss: 5.675492286682129\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[276, 13] trainning loss: 9.365445047616959\u001b[31m\n",
      "\u001b[32m[276, 2] valid_loss: 5.675585746765137\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[277, 13] trainning loss: 9.365524560213089\u001b[31m\n",
      "\u001b[32m[277, 2] valid_loss: 5.675531625747681\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[278, 13] trainning loss: 9.365471512079239\u001b[31m\n",
      "\u001b[32m[278, 2] valid_loss: 5.675428152084351\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[279, 13] trainning loss: 9.365480929613113\u001b[31m\n",
      "\u001b[32m[279, 2] valid_loss: 5.675475597381592\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[280, 13] trainning loss: 9.365408569574356\u001b[31m\n",
      "\u001b[32m[280, 2] valid_loss: 5.675586223602295\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[281, 13] trainning loss: 9.365545898675919\u001b[31m\n",
      "\u001b[32m[281, 2] valid_loss: 5.6753551959991455\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[282, 13] trainning loss: 9.3655506670475\u001b[31m\n",
      "\u001b[32m[282, 2] valid_loss: 5.675410270690918\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[283, 13] trainning loss: 9.365505516529083\u001b[31m\n",
      "\u001b[32m[283, 2] valid_loss: 5.675455093383789\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[284, 13] trainning loss: 9.365509629249573\u001b[31m\n",
      "\u001b[32m[284, 2] valid_loss: 5.675414562225342\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[285, 13] trainning loss: 9.365483194589615\u001b[31m\n",
      "\u001b[32m[285, 2] valid_loss: 5.675504922866821\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[286, 13] trainning loss: 9.365439713001251\u001b[31m\n",
      "\u001b[32m[286, 2] valid_loss: 5.675443172454834\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[287, 13] trainning loss: 9.365503877401352\u001b[31m\n",
      "\u001b[32m[287, 2] valid_loss: 5.675493478775024\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[288, 13] trainning loss: 9.36545506119728\u001b[31m\n",
      "\u001b[32m[288, 2] valid_loss: 5.675491094589233\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[289, 13] trainning loss: 9.365467220544815\u001b[31m\n",
      "\u001b[32m[289, 2] valid_loss: 5.675433874130249\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[290, 13] trainning loss: 9.365485608577728\u001b[31m\n",
      "\u001b[32m[290, 2] valid_loss: 5.675565719604492\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[291, 13] trainning loss: 9.365477919578552\u001b[31m\n",
      "\u001b[32m[291, 2] valid_loss: 5.675432443618774\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[292, 13] trainning loss: 9.365476340055466\u001b[31m\n",
      "\u001b[32m[292, 2] valid_loss: 5.675565242767334\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[293, 13] trainning loss: 9.365546703338623\u001b[31m\n",
      "\u001b[32m[293, 2] valid_loss: 5.675572156906128\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[294, 13] trainning loss: 9.36544445157051\u001b[31m\n",
      "\u001b[32m[294, 2] valid_loss: 5.675556659698486\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[295, 13] trainning loss: 9.36548537015915\u001b[31m\n",
      "\u001b[32m[295, 2] valid_loss: 5.675508975982666\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[296, 13] trainning loss: 9.365453064441681\u001b[31m\n",
      "\u001b[32m[296, 2] valid_loss: 5.675511598587036\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[297, 13] trainning loss: 9.365433752536774\u001b[31m\n",
      "\u001b[32m[297, 2] valid_loss: 5.675426483154297\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[298, 13] trainning loss: 9.365387350320816\u001b[31m\n",
      "\u001b[32m[298, 2] valid_loss: 5.6755146980285645\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[299, 13] trainning loss: 9.365493535995483\u001b[31m\n",
      "\u001b[32m[299, 2] valid_loss: 5.675548076629639\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[300, 13] trainning loss: 9.365495026111603\u001b[31m\n",
      "\u001b[32m[300, 2] valid_loss: 5.675525903701782\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[301, 13] trainning loss: 9.365539699792862\u001b[31m\n",
      "\u001b[32m[301, 2] valid_loss: 5.675543546676636\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[302, 13] trainning loss: 9.365434467792511\u001b[31m\n",
      "\u001b[32m[302, 2] valid_loss: 5.675477504730225\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[303, 13] trainning loss: 9.36542296409607\u001b[31m\n",
      "\u001b[32m[303, 2] valid_loss: 5.675568103790283\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[304, 13] trainning loss: 9.365352183580399\u001b[31m\n",
      "\u001b[32m[304, 2] valid_loss: 5.6755499839782715\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[305, 13] trainning loss: 9.36539688706398\u001b[31m\n",
      "\u001b[32m[305, 2] valid_loss: 5.675519704818726\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[306, 13] trainning loss: 9.365462899208069\u001b[31m\n",
      "\u001b[32m[306, 2] valid_loss: 5.675420522689819\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[307, 13] trainning loss: 9.365440875291824\u001b[31m\n",
      "\u001b[32m[307, 2] valid_loss: 5.67563009262085\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[308, 13] trainning loss: 9.365469127893448\u001b[31m\n",
      "\u001b[32m[308, 2] valid_loss: 5.675442218780518\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[309, 13] trainning loss: 9.365477859973907\u001b[31m\n",
      "\u001b[32m[309, 2] valid_loss: 5.6754982471466064\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[310, 13] trainning loss: 9.365514576435089\u001b[31m\n",
      "\u001b[32m[310, 2] valid_loss: 5.675449848175049\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[311, 13] trainning loss: 9.365485399961472\u001b[31m\n",
      "\u001b[32m[311, 2] valid_loss: 5.675540924072266\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[312, 13] trainning loss: 9.365449666976929\u001b[31m\n",
      "\u001b[32m[312, 2] valid_loss: 5.675597906112671\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[313, 13] trainning loss: 9.365526169538498\u001b[31m\n",
      "\u001b[32m[313, 2] valid_loss: 5.6755712032318115\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[314, 13] trainning loss: 9.365454971790314\u001b[31m\n",
      "\u001b[32m[314, 2] valid_loss: 5.675490140914917\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[315, 13] trainning loss: 9.365567773580551\u001b[31m\n",
      "\u001b[32m[315, 2] valid_loss: 5.6755242347717285\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[316, 13] trainning loss: 9.365557372570038\u001b[31m\n",
      "\u001b[32m[316, 2] valid_loss: 5.675547122955322\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[317, 13] trainning loss: 9.365417182445526\u001b[31m\n",
      "\u001b[32m[317, 2] valid_loss: 5.675557851791382\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[318, 13] trainning loss: 9.36548176407814\u001b[31m\n",
      "\u001b[32m[318, 2] valid_loss: 5.6755452156066895\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[319, 13] trainning loss: 9.365456819534302\u001b[31m\n",
      "\u001b[32m[319, 2] valid_loss: 5.675532817840576\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[320, 13] trainning loss: 9.365591168403625\u001b[31m\n",
      "\u001b[32m[320, 2] valid_loss: 5.675438404083252\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[321, 13] trainning loss: 9.365456342697144\u001b[31m\n",
      "\u001b[32m[321, 2] valid_loss: 5.675432443618774\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[322, 13] trainning loss: 9.365542590618134\u001b[31m\n",
      "\u001b[32m[322, 2] valid_loss: 5.675486087799072\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[323, 13] trainning loss: 9.365592151880264\u001b[31m\n",
      "\u001b[32m[323, 2] valid_loss: 5.675568342208862\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[324, 13] trainning loss: 9.365564942359924\u001b[31m\n",
      "\u001b[32m[324, 2] valid_loss: 5.675537586212158\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[325, 13] trainning loss: 9.365510791540146\u001b[31m\n",
      "\u001b[32m[325, 2] valid_loss: 5.675621032714844\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[326, 13] trainning loss: 9.365479081869125\u001b[31m\n",
      "\u001b[32m[326, 2] valid_loss: 5.675518035888672\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[327, 13] trainning loss: 9.365581154823303\u001b[31m\n",
      "\u001b[32m[327, 2] valid_loss: 5.675460338592529\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[328, 13] trainning loss: 9.365465939044952\u001b[31m\n",
      "\u001b[32m[328, 2] valid_loss: 5.675461530685425\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[329, 13] trainning loss: 9.36544606089592\u001b[31m\n",
      "\u001b[32m[329, 2] valid_loss: 5.675514459609985\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[330, 13] trainning loss: 9.365420669317245\u001b[31m\n",
      "\u001b[32m[330, 2] valid_loss: 5.675386190414429\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[331, 13] trainning loss: 9.365443229675293\u001b[31m\n",
      "\u001b[32m[331, 2] valid_loss: 5.67542028427124\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[332, 13] trainning loss: 9.365407049655914\u001b[31m\n",
      "\u001b[32m[332, 2] valid_loss: 5.675477504730225\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[333, 13] trainning loss: 9.365442305803299\u001b[31m\n",
      "\u001b[32m[333, 2] valid_loss: 5.67532205581665\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[334, 13] trainning loss: 9.365436136722565\u001b[31m\n",
      "\u001b[32m[334, 2] valid_loss: 5.67551326751709\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[335, 13] trainning loss: 9.365458101034164\u001b[31m\n",
      "\u001b[32m[335, 2] valid_loss: 5.6754584312438965\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[336, 13] trainning loss: 9.365466326475143\u001b[31m\n",
      "\u001b[32m[336, 2] valid_loss: 5.675438642501831\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[337, 13] trainning loss: 9.365445226430893\u001b[31m\n",
      "\u001b[32m[337, 2] valid_loss: 5.6754162311553955\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[338, 13] trainning loss: 9.36552420258522\u001b[31m\n",
      "\u001b[32m[338, 2] valid_loss: 5.675516843795776\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[339, 13] trainning loss: 9.365451753139496\u001b[31m\n",
      "\u001b[32m[339, 2] valid_loss: 5.67546010017395\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[340, 13] trainning loss: 9.36540099978447\u001b[31m\n",
      "\u001b[32m[340, 2] valid_loss: 5.675501346588135\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[341, 13] trainning loss: 9.365408629179\u001b[31m\n",
      "\u001b[32m[341, 2] valid_loss: 5.675602912902832\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[342, 13] trainning loss: 9.365389704704285\u001b[31m\n",
      "\u001b[32m[342, 2] valid_loss: 5.675403356552124\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[343, 13] trainning loss: 9.36544817686081\u001b[31m\n",
      "\u001b[32m[343, 2] valid_loss: 5.675573110580444\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[344, 13] trainning loss: 9.36554503440857\u001b[31m\n",
      "\u001b[32m[344, 2] valid_loss: 5.675597190856934\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[345, 13] trainning loss: 9.365500628948212\u001b[31m\n",
      "\u001b[32m[345, 2] valid_loss: 5.67553973197937\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[346, 13] trainning loss: 9.365429133176804\u001b[31m\n",
      "\u001b[32m[346, 2] valid_loss: 5.675563097000122\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[347, 13] trainning loss: 9.365483462810516\u001b[31m\n",
      "\u001b[32m[347, 2] valid_loss: 5.675519704818726\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[348, 13] trainning loss: 9.365569353103638\u001b[31m\n",
      "\u001b[32m[348, 2] valid_loss: 5.675542116165161\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[349, 13] trainning loss: 9.365495562553406\u001b[31m\n",
      "\u001b[32m[349, 2] valid_loss: 5.675529479980469\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[350, 13] trainning loss: 9.365506142377853\u001b[31m\n",
      "\u001b[32m[350, 2] valid_loss: 5.6755359172821045\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[351, 13] trainning loss: 9.365424185991287\u001b[31m\n",
      "\u001b[32m[351, 2] valid_loss: 5.675447702407837\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[352, 13] trainning loss: 9.365359485149384\u001b[31m\n",
      "\u001b[32m[352, 2] valid_loss: 5.675566911697388\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[353, 13] trainning loss: 9.365407228469849\u001b[31m\n",
      "\u001b[32m[353, 2] valid_loss: 5.675424575805664\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[354, 13] trainning loss: 9.36528491973877\u001b[31m\n",
      "\u001b[32m[354, 2] valid_loss: 5.67544150352478\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[355, 13] trainning loss: 9.365368813276291\u001b[31m\n",
      "\u001b[32m[355, 2] valid_loss: 5.67560076713562\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[356, 13] trainning loss: 9.365440100431442\u001b[31m\n",
      "\u001b[32m[356, 2] valid_loss: 5.675436496734619\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[357, 13] trainning loss: 9.365377128124237\u001b[31m\n",
      "\u001b[32m[357, 2] valid_loss: 5.675436019897461\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[358, 13] trainning loss: 9.365397900342941\u001b[31m\n",
      "\u001b[32m[358, 2] valid_loss: 5.675350189208984\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[359, 13] trainning loss: 9.365430474281311\u001b[31m\n",
      "\u001b[32m[359, 2] valid_loss: 5.6754841804504395\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[360, 13] trainning loss: 9.365356236696243\u001b[31m\n",
      "\u001b[32m[360, 2] valid_loss: 5.6755475997924805\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[361, 13] trainning loss: 9.36534658074379\u001b[31m\n",
      "\u001b[32m[361, 2] valid_loss: 5.67539644241333\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[362, 13] trainning loss: 9.365409106016159\u001b[31m\n",
      "\u001b[32m[362, 2] valid_loss: 5.67549729347229\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[363, 13] trainning loss: 9.36541536450386\u001b[31m\n",
      "\u001b[32m[363, 2] valid_loss: 5.675506353378296\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[364, 13] trainning loss: 9.365358650684357\u001b[31m\n",
      "\u001b[32m[364, 2] valid_loss: 5.675602197647095\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[365, 13] trainning loss: 9.365432053804398\u001b[31m\n",
      "\u001b[32m[365, 2] valid_loss: 5.675456762313843\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[366, 13] trainning loss: 9.365337669849396\u001b[31m\n",
      "\u001b[32m[366, 2] valid_loss: 5.675384044647217\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[367, 13] trainning loss: 9.365479409694672\u001b[31m\n",
      "\u001b[32m[367, 2] valid_loss: 5.675480604171753\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[368, 13] trainning loss: 9.365438878536224\u001b[31m\n",
      "\u001b[32m[368, 2] valid_loss: 5.675528287887573\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[369, 13] trainning loss: 9.365445613861084\u001b[31m\n",
      "\u001b[32m[369, 2] valid_loss: 5.675523281097412\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[370, 13] trainning loss: 9.365367025136948\u001b[31m\n",
      "\u001b[32m[370, 2] valid_loss: 5.675522804260254\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[371, 13] trainning loss: 9.36552608013153\u001b[31m\n",
      "\u001b[32m[371, 2] valid_loss: 5.675554513931274\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[372, 13] trainning loss: 9.365461498498917\u001b[31m\n",
      "\u001b[32m[372, 2] valid_loss: 5.675559043884277\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[373, 13] trainning loss: 9.365350097417831\u001b[31m\n",
      "\u001b[32m[373, 2] valid_loss: 5.675520181655884\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[374, 13] trainning loss: 9.365423738956451\u001b[31m\n",
      "\u001b[32m[374, 2] valid_loss: 5.675482988357544\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[375, 13] trainning loss: 9.365367829799652\u001b[31m\n",
      "\u001b[32m[375, 2] valid_loss: 5.675537347793579\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[376, 13] trainning loss: 9.365365266799927\u001b[31m\n",
      "\u001b[32m[376, 2] valid_loss: 5.675507068634033\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[377, 13] trainning loss: 9.365470856428146\u001b[31m\n",
      "\u001b[32m[377, 2] valid_loss: 5.675504207611084\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[378, 13] trainning loss: 9.365457355976105\u001b[31m\n",
      "\u001b[32m[378, 2] valid_loss: 5.675414800643921\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[379, 13] trainning loss: 9.365373760461807\u001b[31m\n",
      "\u001b[32m[379, 2] valid_loss: 5.675613880157471\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[380, 13] trainning loss: 9.365478724241257\u001b[31m\n",
      "\u001b[32m[380, 2] valid_loss: 5.675530433654785\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[381, 13] trainning loss: 9.36546504497528\u001b[31m\n",
      "\u001b[32m[381, 2] valid_loss: 5.6752610206604\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[382, 13] trainning loss: 9.365481317043304\u001b[31m\n",
      "\u001b[32m[382, 2] valid_loss: 5.67546534538269\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[383, 13] trainning loss: 9.365530461072922\u001b[31m\n",
      "\u001b[32m[383, 2] valid_loss: 5.675537347793579\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[384, 13] trainning loss: 9.36547303199768\u001b[31m\n",
      "\u001b[32m[384, 2] valid_loss: 5.6755053997039795\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[385, 13] trainning loss: 9.365571230649948\u001b[31m\n",
      "\u001b[32m[385, 2] valid_loss: 5.675371408462524\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[386, 13] trainning loss: 9.365547627210617\u001b[31m\n",
      "\u001b[32m[386, 2] valid_loss: 5.675429582595825\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[387, 13] trainning loss: 9.36543482542038\u001b[31m\n",
      "\u001b[32m[387, 2] valid_loss: 5.675557851791382\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[388, 13] trainning loss: 9.365506649017334\u001b[31m\n",
      "\u001b[32m[388, 2] valid_loss: 5.675427436828613\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[389, 13] trainning loss: 9.365534007549286\u001b[31m\n",
      "\u001b[32m[389, 2] valid_loss: 5.675344705581665\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[390, 13] trainning loss: 9.365555256605148\u001b[31m\n",
      "\u001b[32m[390, 2] valid_loss: 5.675348520278931\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[391, 13] trainning loss: 9.365550339221954\u001b[31m\n",
      "\u001b[32m[391, 2] valid_loss: 5.675418138504028\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[392, 13] trainning loss: 9.36549261212349\u001b[31m\n",
      "\u001b[32m[392, 2] valid_loss: 5.675461530685425\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[393, 13] trainning loss: 9.365522593259811\u001b[31m\n",
      "\u001b[32m[393, 2] valid_loss: 5.675426721572876\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[394, 13] trainning loss: 9.365492522716522\u001b[31m\n",
      "\u001b[32m[394, 2] valid_loss: 5.675539493560791\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[395, 13] trainning loss: 9.365521103143692\u001b[31m\n",
      "\u001b[32m[395, 2] valid_loss: 5.675550222396851\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[396, 13] trainning loss: 9.365483224391937\u001b[31m\n",
      "\u001b[32m[396, 2] valid_loss: 5.675426244735718\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[397, 13] trainning loss: 9.365534335374832\u001b[31m\n",
      "\u001b[32m[397, 2] valid_loss: 5.675575017929077\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[398, 13] trainning loss: 9.365480571985245\u001b[31m\n",
      "\u001b[32m[398, 2] valid_loss: 5.6754350662231445\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[399, 13] trainning loss: 9.36542147397995\u001b[31m\n",
      "\u001b[32m[399, 2] valid_loss: 5.675464630126953\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[400, 13] trainning loss: 9.365527302026749\u001b[31m\n",
      "\u001b[32m[400, 2] valid_loss: 5.6754186153411865\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[401, 13] trainning loss: 9.365501821041107\u001b[31m\n",
      "\u001b[32m[401, 2] valid_loss: 5.675449371337891\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[402, 13] trainning loss: 9.365538358688354\u001b[31m\n",
      "\u001b[32m[402, 2] valid_loss: 5.675387859344482\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[403, 13] trainning loss: 9.36543893814087\u001b[31m\n",
      "\u001b[32m[403, 2] valid_loss: 5.675383567810059\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[404, 13] trainning loss: 9.365504503250122\u001b[31m\n",
      "\u001b[32m[404, 2] valid_loss: 5.6754150390625\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[405, 13] trainning loss: 9.365477502346039\u001b[31m\n",
      "\u001b[32m[405, 2] valid_loss: 5.675434827804565\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[406, 13] trainning loss: 9.365474909543991\u001b[31m\n",
      "\u001b[32m[406, 2] valid_loss: 5.675384283065796\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[407, 13] trainning loss: 9.365475445985794\u001b[31m\n",
      "\u001b[32m[407, 2] valid_loss: 5.6754279136657715\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[408, 13] trainning loss: 9.36542958021164\u001b[31m\n",
      "\u001b[32m[408, 2] valid_loss: 5.675318479537964\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[409, 13] trainning loss: 9.365416437387466\u001b[31m\n",
      "\u001b[32m[409, 2] valid_loss: 5.675398826599121\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[410, 13] trainning loss: 9.365480482578278\u001b[31m\n",
      "\u001b[32m[410, 2] valid_loss: 5.675309419631958\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[411, 13] trainning loss: 9.365509897470474\u001b[31m\n",
      "\u001b[32m[411, 2] valid_loss: 5.67533802986145\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[412, 13] trainning loss: 9.365415096282959\u001b[31m\n",
      "\u001b[32m[412, 2] valid_loss: 5.675294876098633\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[413, 13] trainning loss: 9.365426361560822\u001b[31m\n",
      "\u001b[32m[413, 2] valid_loss: 5.6753926277160645\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[414, 13] trainning loss: 9.365477800369263\u001b[31m\n",
      "\u001b[32m[414, 2] valid_loss: 5.675259828567505\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[415, 13] trainning loss: 9.365420311689377\u001b[31m\n",
      "\u001b[32m[415, 2] valid_loss: 5.675443410873413\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[416, 13] trainning loss: 9.365471303462982\u001b[31m\n",
      "\u001b[32m[416, 2] valid_loss: 5.6753270626068115\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[417, 13] trainning loss: 9.365410059690475\u001b[31m\n",
      "\u001b[32m[417, 2] valid_loss: 5.675391912460327\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[418, 13] trainning loss: 9.36541736125946\u001b[31m\n",
      "\u001b[32m[418, 2] valid_loss: 5.675523519515991\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[419, 13] trainning loss: 9.365368634462357\u001b[31m\n",
      "\u001b[32m[419, 2] valid_loss: 5.675351619720459\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[420, 13] trainning loss: 9.36539602279663\u001b[31m\n",
      "\u001b[32m[420, 2] valid_loss: 5.675487995147705\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[421, 13] trainning loss: 9.3653644323349\u001b[31m\n",
      "\u001b[32m[421, 2] valid_loss: 5.675424814224243\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[422, 13] trainning loss: 9.36551347374916\u001b[31m\n",
      "\u001b[32m[422, 2] valid_loss: 5.6753761768341064\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[423, 13] trainning loss: 9.365433722734451\u001b[31m\n",
      "\u001b[32m[423, 2] valid_loss: 5.675563812255859\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[424, 13] trainning loss: 9.365398526191711\u001b[31m\n",
      "\u001b[32m[424, 2] valid_loss: 5.675420761108398\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[425, 13] trainning loss: 9.365350782871246\u001b[31m\n",
      "\u001b[32m[425, 2] valid_loss: 5.675406455993652\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[426, 13] trainning loss: 9.365349560976028\u001b[31m\n",
      "\u001b[32m[426, 2] valid_loss: 5.675405740737915\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[427, 13] trainning loss: 9.365420818328857\u001b[31m\n",
      "\u001b[32m[427, 2] valid_loss: 5.6754162311553955\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[428, 13] trainning loss: 9.365458279848099\u001b[31m\n",
      "\u001b[32m[428, 2] valid_loss: 5.675519704818726\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[429, 13] trainning loss: 9.365376472473145\u001b[31m\n",
      "\u001b[32m[429, 2] valid_loss: 5.6754326820373535\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[430, 13] trainning loss: 9.365409970283508\u001b[31m\n",
      "\u001b[32m[430, 2] valid_loss: 5.675497055053711\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[431, 13] trainning loss: 9.365434527397156\u001b[31m\n",
      "\u001b[32m[431, 2] valid_loss: 5.675400018692017\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[432, 13] trainning loss: 9.365335643291473\u001b[31m\n",
      "\u001b[32m[432, 2] valid_loss: 5.675364017486572\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[433, 13] trainning loss: 9.36544206738472\u001b[31m\n",
      "\u001b[32m[433, 2] valid_loss: 5.67535924911499\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[434, 13] trainning loss: 9.36540573835373\u001b[31m\n",
      "\u001b[32m[434, 2] valid_loss: 5.675400495529175\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[435, 13] trainning loss: 9.365443587303162\u001b[31m\n",
      "\u001b[32m[435, 2] valid_loss: 5.675389289855957\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[436, 13] trainning loss: 9.365435898303986\u001b[31m\n",
      "\u001b[32m[436, 2] valid_loss: 5.675512790679932\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[437, 13] trainning loss: 9.365390032529831\u001b[31m\n",
      "\u001b[32m[437, 2] valid_loss: 5.675383567810059\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[438, 13] trainning loss: 9.365429610013962\u001b[31m\n",
      "\u001b[32m[438, 2] valid_loss: 5.675348520278931\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[439, 13] trainning loss: 9.365468055009842\u001b[31m\n",
      "\u001b[32m[439, 2] valid_loss: 5.675380229949951\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[440, 13] trainning loss: 9.365383684635162\u001b[31m\n",
      "\u001b[32m[440, 2] valid_loss: 5.675472736358643\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[441, 13] trainning loss: 9.365436017513275\u001b[31m\n",
      "\u001b[32m[441, 2] valid_loss: 5.675533056259155\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[442, 13] trainning loss: 9.36532524228096\u001b[31m\n",
      "\u001b[32m[442, 2] valid_loss: 5.675320863723755\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[443, 13] trainning loss: 9.365412294864655\u001b[31m\n",
      "\u001b[32m[443, 2] valid_loss: 5.675437688827515\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[444, 13] trainning loss: 9.365402042865753\u001b[31m\n",
      "\u001b[32m[444, 2] valid_loss: 5.675400733947754\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[445, 13] trainning loss: 9.36540001630783\u001b[31m\n",
      "\u001b[32m[445, 2] valid_loss: 5.675562620162964\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[446, 13] trainning loss: 9.365307450294495\u001b[31m\n",
      "\u001b[32m[446, 2] valid_loss: 5.67542839050293\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[447, 13] trainning loss: 9.365353167057037\u001b[31m\n",
      "\u001b[32m[447, 2] valid_loss: 5.6754889488220215\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[448, 13] trainning loss: 9.36546903848648\u001b[31m\n",
      "\u001b[32m[448, 2] valid_loss: 5.67542839050293\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[449, 13] trainning loss: 9.365373373031616\u001b[31m\n",
      "\u001b[32m[449, 2] valid_loss: 5.675485134124756\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[450, 13] trainning loss: 9.365366458892822\u001b[31m\n",
      "\u001b[32m[450, 2] valid_loss: 5.6754772663116455\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[451, 13] trainning loss: 9.365380108356476\u001b[31m\n",
      "\u001b[32m[451, 2] valid_loss: 5.675609588623047\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[452, 13] trainning loss: 9.36543157696724\u001b[31m\n",
      "\u001b[32m[452, 2] valid_loss: 5.675502300262451\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[453, 13] trainning loss: 9.365340948104858\u001b[31m\n",
      "\u001b[32m[453, 2] valid_loss: 5.675563812255859\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[454, 13] trainning loss: 9.365433037281036\u001b[31m\n",
      "\u001b[32m[454, 2] valid_loss: 5.675432443618774\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[455, 13] trainning loss: 9.365303486585617\u001b[31m\n",
      "\u001b[32m[455, 2] valid_loss: 5.675516366958618\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[456, 13] trainning loss: 9.365440130233765\u001b[31m\n",
      "\u001b[32m[456, 2] valid_loss: 5.6755006313323975\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[457, 13] trainning loss: 9.36541622877121\u001b[31m\n",
      "\u001b[32m[457, 2] valid_loss: 5.675492763519287\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[458, 13] trainning loss: 9.365394592285156\u001b[31m\n",
      "\u001b[32m[458, 2] valid_loss: 5.67543625831604\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[459, 13] trainning loss: 9.365481793880463\u001b[31m\n",
      "\u001b[32m[459, 2] valid_loss: 5.675461530685425\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[460, 13] trainning loss: 9.365376770496368\u001b[31m\n",
      "\u001b[32m[460, 2] valid_loss: 5.675470590591431\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[461, 13] trainning loss: 9.365405857563019\u001b[31m\n",
      "\u001b[32m[461, 2] valid_loss: 5.675539493560791\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[462, 13] trainning loss: 9.365484714508057\u001b[31m\n",
      "\u001b[32m[462, 2] valid_loss: 5.675417900085449\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[463, 13] trainning loss: 9.365430295467377\u001b[31m\n",
      "\u001b[32m[463, 2] valid_loss: 5.675487756729126\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[464, 13] trainning loss: 9.365429550409317\u001b[31m\n",
      "\u001b[32m[464, 2] valid_loss: 5.675446271896362\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[465, 13] trainning loss: 9.365472674369812\u001b[31m\n",
      "\u001b[32m[465, 2] valid_loss: 5.675438642501831\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[466, 13] trainning loss: 9.365528225898743\u001b[31m\n",
      "\u001b[32m[466, 2] valid_loss: 5.675481796264648\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[467, 13] trainning loss: 9.365498781204224\u001b[31m\n",
      "\u001b[32m[467, 2] valid_loss: 5.675501108169556\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[468, 13] trainning loss: 9.365402102470398\u001b[31m\n",
      "\u001b[32m[468, 2] valid_loss: 5.675550699234009\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[469, 13] trainning loss: 9.365475177764893\u001b[31m\n",
      "\u001b[32m[469, 2] valid_loss: 5.675443172454834\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[470, 13] trainning loss: 9.365486949682236\u001b[31m\n",
      "\u001b[32m[470, 2] valid_loss: 5.675459384918213\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[471, 13] trainning loss: 9.365414470434189\u001b[31m\n",
      "\u001b[32m[471, 2] valid_loss: 5.675411939620972\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[472, 13] trainning loss: 9.365566730499268\u001b[31m\n",
      "\u001b[32m[472, 2] valid_loss: 5.6753990650177\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[473, 13] trainning loss: 9.365467101335526\u001b[31m\n",
      "\u001b[32m[473, 2] valid_loss: 5.675349712371826\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[474, 13] trainning loss: 9.365428775548935\u001b[31m\n",
      "\u001b[32m[474, 2] valid_loss: 5.675428152084351\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[475, 13] trainning loss: 9.365392565727234\u001b[31m\n",
      "\u001b[32m[475, 2] valid_loss: 5.675402641296387\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[476, 13] trainning loss: 9.365396440029144\u001b[31m\n",
      "\u001b[32m[476, 2] valid_loss: 5.675426244735718\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[477, 13] trainning loss: 9.365430295467377\u001b[31m\n",
      "\u001b[32m[477, 2] valid_loss: 5.675414085388184\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[478, 13] trainning loss: 9.365433871746063\u001b[31m\n",
      "\u001b[32m[478, 2] valid_loss: 5.675562143325806\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[479, 13] trainning loss: 9.365443766117096\u001b[31m\n",
      "\u001b[32m[479, 2] valid_loss: 5.67540717124939\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[480, 13] trainning loss: 9.365397453308105\u001b[31m\n",
      "\u001b[32m[480, 2] valid_loss: 5.675442934036255\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[481, 13] trainning loss: 9.365455508232117\u001b[31m\n",
      "\u001b[32m[481, 2] valid_loss: 5.675414800643921\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[482, 13] trainning loss: 9.365395843982697\u001b[31m\n",
      "\u001b[32m[482, 2] valid_loss: 5.675280332565308\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[483, 13] trainning loss: 9.365386664867401\u001b[31m\n",
      "\u001b[32m[483, 2] valid_loss: 5.675321340560913\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[484, 13] trainning loss: 9.365551143884659\u001b[31m\n",
      "\u001b[32m[484, 2] valid_loss: 5.675426959991455\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[485, 13] trainning loss: 9.365437775850296\u001b[31m\n",
      "\u001b[32m[485, 2] valid_loss: 5.675382614135742\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[486, 13] trainning loss: 9.365474998950958\u001b[31m\n",
      "\u001b[32m[486, 2] valid_loss: 5.675415515899658\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[487, 13] trainning loss: 9.365489274263382\u001b[31m\n",
      "\u001b[32m[487, 2] valid_loss: 5.67534327507019\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[488, 13] trainning loss: 9.3654845058918\u001b[31m\n",
      "\u001b[32m[488, 2] valid_loss: 5.675393581390381\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[489, 13] trainning loss: 9.365482032299042\u001b[31m\n",
      "\u001b[32m[489, 2] valid_loss: 5.675369024276733\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[490, 13] trainning loss: 9.365508019924164\u001b[31m\n",
      "\u001b[32m[490, 2] valid_loss: 5.675394058227539\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[491, 13] trainning loss: 9.365538716316223\u001b[31m\n",
      "\u001b[32m[491, 2] valid_loss: 5.67545485496521\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[492, 13] trainning loss: 9.365469217300415\u001b[31m\n",
      "\u001b[32m[492, 2] valid_loss: 5.6753318309783936\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[493, 13] trainning loss: 9.36548525094986\u001b[31m\n",
      "\u001b[32m[493, 2] valid_loss: 5.675245046615601\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6752).  Saving model ...\n",
      "\u001b[31m[494, 13] trainning loss: 9.365406900644302\u001b[31m\n",
      "\u001b[32m[494, 2] valid_loss: 5.675451040267944\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[495, 13] trainning loss: 9.365482151508331\u001b[31m\n",
      "\u001b[32m[495, 2] valid_loss: 5.675381898880005\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[496, 13] trainning loss: 9.365496039390564\u001b[31m\n",
      "\u001b[32m[496, 2] valid_loss: 5.675262451171875\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[497, 13] trainning loss: 9.365482449531555\u001b[31m\n",
      "\u001b[32m[497, 2] valid_loss: 5.675418138504028\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[498, 13] trainning loss: 9.365479171276093\u001b[31m\n",
      "\u001b[32m[498, 2] valid_loss: 5.675284385681152\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[499, 13] trainning loss: 9.365477681159973\u001b[31m\n",
      "\u001b[32m[499, 2] valid_loss: 5.675351619720459\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[500, 13] trainning loss: 9.365391373634338\u001b[31m\n",
      "\u001b[32m[500, 2] valid_loss: 5.675381898880005\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[501, 13] trainning loss: 9.365464955568314\u001b[31m\n",
      "\u001b[32m[501, 2] valid_loss: 5.67540717124939\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[502, 13] trainning loss: 9.365441530942917\u001b[31m\n",
      "\u001b[32m[502, 2] valid_loss: 5.675337553024292\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[503, 13] trainning loss: 9.365453720092773\u001b[31m\n",
      "\u001b[32m[503, 2] valid_loss: 5.675354957580566\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[504, 13] trainning loss: 9.365392535924911\u001b[31m\n",
      "\u001b[32m[504, 2] valid_loss: 5.675373792648315\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[505, 13] trainning loss: 9.365500450134277\u001b[31m\n",
      "\u001b[32m[505, 2] valid_loss: 5.6754560470581055\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[506, 13] trainning loss: 9.36548399925232\u001b[31m\n",
      "\u001b[32m[506, 2] valid_loss: 5.6753761768341064\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[507, 13] trainning loss: 9.365539640188217\u001b[31m\n",
      "\u001b[32m[507, 2] valid_loss: 5.6754045486450195\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[508, 13] trainning loss: 9.365524709224701\u001b[31m\n",
      "\u001b[32m[508, 2] valid_loss: 5.6753830909729\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[509, 13] trainning loss: 9.365422546863556\u001b[31m\n",
      "\u001b[32m[509, 2] valid_loss: 5.675416707992554\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[510, 13] trainning loss: 9.365422874689102\u001b[31m\n",
      "\u001b[32m[510, 2] valid_loss: 5.675480127334595\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[511, 13] trainning loss: 9.36543121933937\u001b[31m\n",
      "\u001b[32m[511, 2] valid_loss: 5.6754138469696045\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[512, 13] trainning loss: 9.365394949913025\u001b[31m\n",
      "\u001b[32m[512, 2] valid_loss: 5.675537347793579\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[513, 13] trainning loss: 9.365440338850021\u001b[31m\n",
      "\u001b[32m[513, 2] valid_loss: 5.675473928451538\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[514, 13] trainning loss: 9.36550122499466\u001b[31m\n",
      "\u001b[32m[514, 2] valid_loss: 5.675498962402344\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[515, 13] trainning loss: 9.36544218659401\u001b[31m\n",
      "\u001b[32m[515, 2] valid_loss: 5.675289630889893\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[516, 13] trainning loss: 9.365317702293396\u001b[31m\n",
      "\u001b[32m[516, 2] valid_loss: 5.675395250320435\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[517, 13] trainning loss: 9.365450859069824\u001b[31m\n",
      "\u001b[32m[517, 2] valid_loss: 5.675414323806763\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[518, 13] trainning loss: 9.365436971187592\u001b[31m\n",
      "\u001b[32m[518, 2] valid_loss: 5.675594091415405\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[519, 13] trainning loss: 9.365418195724487\u001b[31m\n",
      "\u001b[32m[519, 2] valid_loss: 5.675555229187012\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[520, 13] trainning loss: 9.365493178367615\u001b[31m\n",
      "\u001b[32m[520, 2] valid_loss: 5.675349235534668\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[521, 13] trainning loss: 9.365431159734726\u001b[31m\n",
      "\u001b[32m[521, 2] valid_loss: 5.675344705581665\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[522, 13] trainning loss: 9.365510106086731\u001b[31m\n",
      "\u001b[32m[522, 2] valid_loss: 5.6753623485565186\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[523, 13] trainning loss: 9.36546590924263\u001b[31m\n",
      "\u001b[32m[523, 2] valid_loss: 5.675492763519287\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[524, 13] trainning loss: 9.365306973457336\u001b[31m\n",
      "\u001b[32m[524, 2] valid_loss: 5.675567626953125\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[525, 13] trainning loss: 9.365349680185318\u001b[31m\n",
      "\u001b[32m[525, 2] valid_loss: 5.675543308258057\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[526, 13] trainning loss: 9.365358412265778\u001b[31m\n",
      "\u001b[32m[526, 2] valid_loss: 5.675362586975098\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[527, 13] trainning loss: 9.365370720624924\u001b[31m\n",
      "\u001b[32m[527, 2] valid_loss: 5.675500392913818\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[528, 13] trainning loss: 9.36542072892189\u001b[31m\n",
      "\u001b[32m[528, 2] valid_loss: 5.675654172897339\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6757).  Saving model ...\n",
      "\u001b[31m[529, 13] trainning loss: 9.365505188703537\u001b[31m\n",
      "\u001b[32m[529, 2] valid_loss: 5.675438165664673\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[530, 13] trainning loss: 9.365455240011215\u001b[31m\n",
      "\u001b[32m[530, 2] valid_loss: 5.675501108169556\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[531, 13] trainning loss: 9.365440785884857\u001b[31m\n",
      "\u001b[32m[531, 2] valid_loss: 5.675466060638428\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[532, 13] trainning loss: 9.36533921957016\u001b[31m\n",
      "\u001b[32m[532, 2] valid_loss: 5.675473213195801\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[533, 13] trainning loss: 9.365429848432541\u001b[31m\n",
      "\u001b[32m[533, 2] valid_loss: 5.675476789474487\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[534, 13] trainning loss: 9.365473419427872\u001b[31m\n",
      "\u001b[32m[534, 2] valid_loss: 5.675404071807861\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[535, 13] trainning loss: 9.365482300519943\u001b[31m\n",
      "\u001b[32m[535, 2] valid_loss: 5.6753973960876465\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[536, 13] trainning loss: 9.365462243556976\u001b[31m\n",
      "\u001b[32m[536, 2] valid_loss: 5.6754491329193115\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[537, 13] trainning loss: 9.365458399057388\u001b[31m\n",
      "\u001b[32m[537, 2] valid_loss: 5.675442218780518\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[538, 13] trainning loss: 9.36547839641571\u001b[31m\n",
      "\u001b[32m[538, 2] valid_loss: 5.67535400390625\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[539, 13] trainning loss: 9.36549311876297\u001b[31m\n",
      "\u001b[32m[539, 2] valid_loss: 5.675434589385986\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[540, 13] trainning loss: 9.36553281545639\u001b[31m\n",
      "\u001b[32m[540, 2] valid_loss: 5.675394058227539\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[541, 13] trainning loss: 9.36556738615036\u001b[31m\n",
      "\u001b[32m[541, 2] valid_loss: 5.675430774688721\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[542, 13] trainning loss: 9.365443646907806\u001b[31m\n",
      "\u001b[32m[542, 2] valid_loss: 5.675527095794678\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[543, 13] trainning loss: 9.365494519472122\u001b[31m\n",
      "\u001b[32m[543, 2] valid_loss: 5.6755211353302\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[544, 13] trainning loss: 9.365498900413513\u001b[31m\n",
      "\u001b[32m[544, 2] valid_loss: 5.675431251525879\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[545, 13] trainning loss: 9.365481197834015\u001b[31m\n",
      "\u001b[32m[545, 2] valid_loss: 5.6754937171936035\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[546, 13] trainning loss: 9.365459948778152\u001b[31m\n",
      "\u001b[32m[546, 2] valid_loss: 5.675440311431885\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[547, 13] trainning loss: 9.365453690290451\u001b[31m\n",
      "\u001b[32m[547, 2] valid_loss: 5.675575494766235\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[548, 13] trainning loss: 9.36549398303032\u001b[31m\n",
      "\u001b[32m[548, 2] valid_loss: 5.675333738327026\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[549, 13] trainning loss: 9.365544438362122\u001b[31m\n",
      "\u001b[32m[549, 2] valid_loss: 5.675572633743286\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[550, 13] trainning loss: 9.365432560443878\u001b[31m\n",
      "\u001b[32m[550, 2] valid_loss: 5.675481796264648\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[551, 13] trainning loss: 9.365484476089478\u001b[31m\n",
      "\u001b[32m[551, 2] valid_loss: 5.675411224365234\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[552, 13] trainning loss: 9.365539282560349\u001b[31m\n",
      "\u001b[32m[552, 2] valid_loss: 5.6754796504974365\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[553, 13] trainning loss: 9.365528762340546\u001b[31m\n",
      "\u001b[32m[553, 2] valid_loss: 5.675450325012207\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[554, 13] trainning loss: 9.36544182896614\u001b[31m\n",
      "\u001b[32m[554, 2] valid_loss: 5.675442457199097\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[555, 13] trainning loss: 9.365494817495346\u001b[31m\n",
      "\u001b[32m[555, 2] valid_loss: 5.675398826599121\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[556, 13] trainning loss: 9.365550369024277\u001b[31m\n",
      "\u001b[32m[556, 2] valid_loss: 5.675421237945557\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[557, 13] trainning loss: 9.36545506119728\u001b[31m\n",
      "\u001b[32m[557, 2] valid_loss: 5.675440073013306\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[558, 13] trainning loss: 9.365405231714249\u001b[31m\n",
      "\u001b[32m[558, 2] valid_loss: 5.675398826599121\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[559, 13] trainning loss: 9.365416318178177\u001b[31m\n",
      "\u001b[32m[559, 2] valid_loss: 5.675296783447266\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[560, 13] trainning loss: 9.365435630083084\u001b[31m\n",
      "\u001b[32m[560, 2] valid_loss: 5.6753785610198975\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[561, 13] trainning loss: 9.365505635738373\u001b[31m\n",
      "\u001b[32m[561, 2] valid_loss: 5.675355672836304\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[562, 13] trainning loss: 9.365537077188492\u001b[31m\n",
      "\u001b[32m[562, 2] valid_loss: 5.675221681594849\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6752).  Saving model ...\n",
      "\u001b[31m[563, 13] trainning loss: 9.365484595298767\u001b[31m\n",
      "\u001b[32m[563, 2] valid_loss: 5.675386428833008\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[564, 13] trainning loss: 9.365376055240631\u001b[31m\n",
      "\u001b[32m[564, 2] valid_loss: 5.675379753112793\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[565, 13] trainning loss: 9.36544543504715\u001b[31m\n",
      "\u001b[32m[565, 2] valid_loss: 5.675541877746582\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[566, 13] trainning loss: 9.365447580814362\u001b[31m\n",
      "\u001b[32m[566, 2] valid_loss: 5.675378322601318\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[567, 13] trainning loss: 9.365386217832565\u001b[31m\n",
      "\u001b[32m[567, 2] valid_loss: 5.675464630126953\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[568, 13] trainning loss: 9.365515410900116\u001b[31m\n",
      "\u001b[32m[568, 2] valid_loss: 5.675482511520386\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[569, 13] trainning loss: 9.365452945232391\u001b[31m\n",
      "\u001b[32m[569, 2] valid_loss: 5.675355672836304\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[570, 13] trainning loss: 9.365485072135925\u001b[31m\n",
      "\u001b[32m[570, 2] valid_loss: 5.675375461578369\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[571, 13] trainning loss: 9.365392833948135\u001b[31m\n",
      "\u001b[32m[571, 2] valid_loss: 5.675424575805664\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[572, 13] trainning loss: 9.365389615297318\u001b[31m\n",
      "\u001b[32m[572, 2] valid_loss: 5.675457954406738\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[573, 13] trainning loss: 9.365389585494995\u001b[31m\n",
      "\u001b[32m[573, 2] valid_loss: 5.675414562225342\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[574, 13] trainning loss: 9.36537218093872\u001b[31m\n",
      "\u001b[32m[574, 2] valid_loss: 5.675506114959717\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[575, 13] trainning loss: 9.365356147289276\u001b[31m\n",
      "\u001b[32m[575, 2] valid_loss: 5.675536632537842\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[576, 13] trainning loss: 9.365379929542542\u001b[31m\n",
      "\u001b[32m[576, 2] valid_loss: 5.675605058670044\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[577, 13] trainning loss: 9.365380853414536\u001b[31m\n",
      "\u001b[32m[577, 2] valid_loss: 5.675453186035156\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[578, 13] trainning loss: 9.365432322025299\u001b[31m\n",
      "\u001b[32m[578, 2] valid_loss: 5.675473928451538\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[579, 13] trainning loss: 9.36549386382103\u001b[31m\n",
      "\u001b[32m[579, 2] valid_loss: 5.675481557846069\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[580, 13] trainning loss: 9.36533722281456\u001b[31m\n",
      "\u001b[32m[580, 2] valid_loss: 5.6754984855651855\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[581, 13] trainning loss: 9.365386724472046\u001b[31m\n",
      "\u001b[32m[581, 2] valid_loss: 5.675473213195801\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[582, 13] trainning loss: 9.365403532981873\u001b[31m\n",
      "\u001b[32m[582, 2] valid_loss: 5.675443172454834\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[583, 13] trainning loss: 9.3653564453125\u001b[31m\n",
      "\u001b[32m[583, 2] valid_loss: 5.675500154495239\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[584, 13] trainning loss: 9.365441471338272\u001b[31m\n",
      "\u001b[32m[584, 2] valid_loss: 5.675378084182739\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[585, 13] trainning loss: 9.36552095413208\u001b[31m\n",
      "\u001b[32m[585, 2] valid_loss: 5.675421714782715\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[586, 13] trainning loss: 9.365465492010117\u001b[31m\n",
      "\u001b[32m[586, 2] valid_loss: 5.675440073013306\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[587, 13] trainning loss: 9.365364670753479\u001b[31m\n",
      "\u001b[32m[587, 2] valid_loss: 5.675506591796875\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[588, 13] trainning loss: 9.365412443876266\u001b[31m\n",
      "\u001b[32m[588, 2] valid_loss: 5.675437688827515\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[589, 13] trainning loss: 9.365334421396255\u001b[31m\n",
      "\u001b[32m[589, 2] valid_loss: 5.675455808639526\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[590, 13] trainning loss: 9.365462571382523\u001b[31m\n",
      "\u001b[32m[590, 2] valid_loss: 5.6754090785980225\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[591, 13] trainning loss: 9.365495353937149\u001b[31m\n",
      "\u001b[32m[591, 2] valid_loss: 5.675464630126953\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[592, 13] trainning loss: 9.365376859903336\u001b[31m\n",
      "\u001b[32m[592, 2] valid_loss: 5.675454139709473\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[593, 13] trainning loss: 9.365461319684982\u001b[31m\n",
      "\u001b[32m[593, 2] valid_loss: 5.675511837005615\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[594, 13] trainning loss: 9.36542022228241\u001b[31m\n",
      "\u001b[32m[594, 2] valid_loss: 5.6755475997924805\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[595, 13] trainning loss: 9.365516483783722\u001b[31m\n",
      "\u001b[32m[595, 2] valid_loss: 5.675460338592529\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[596, 13] trainning loss: 9.365463942289352\u001b[31m\n",
      "\u001b[32m[596, 2] valid_loss: 5.67539644241333\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[597, 13] trainning loss: 9.365510135889053\u001b[31m\n",
      "\u001b[32m[597, 2] valid_loss: 5.675494194030762\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[598, 13] trainning loss: 9.365379869937897\u001b[31m\n",
      "\u001b[32m[598, 2] valid_loss: 5.6753623485565186\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[599, 13] trainning loss: 9.365443676710129\u001b[31m\n",
      "\u001b[32m[599, 2] valid_loss: 5.675434589385986\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[600, 13] trainning loss: 9.365473210811615\u001b[31m\n",
      "\u001b[32m[600, 2] valid_loss: 5.675546646118164\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[601, 13] trainning loss: 9.365468859672546\u001b[31m\n",
      "\u001b[32m[601, 2] valid_loss: 5.6754724979400635\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[602, 13] trainning loss: 9.365423619747162\u001b[31m\n",
      "\u001b[32m[602, 2] valid_loss: 5.6753456592559814\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[603, 13] trainning loss: 9.365419000387192\u001b[31m\n",
      "\u001b[32m[603, 2] valid_loss: 5.675431966781616\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[604, 13] trainning loss: 9.365500330924988\u001b[31m\n",
      "\u001b[32m[604, 2] valid_loss: 5.675398349761963\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[605, 13] trainning loss: 9.36544156074524\u001b[31m\n",
      "\u001b[32m[605, 2] valid_loss: 5.675458669662476\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[606, 13] trainning loss: 9.365520179271698\u001b[31m\n",
      "\u001b[32m[606, 2] valid_loss: 5.675430774688721\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[607, 13] trainning loss: 9.365428060293198\u001b[31m\n",
      "\u001b[32m[607, 2] valid_loss: 5.675314903259277\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[608, 13] trainning loss: 9.365366578102112\u001b[31m\n",
      "\u001b[32m[608, 2] valid_loss: 5.675462484359741\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[609, 13] trainning loss: 9.365452885627747\u001b[31m\n",
      "\u001b[32m[609, 2] valid_loss: 5.675412178039551\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[610, 13] trainning loss: 9.36540088057518\u001b[31m\n",
      "\u001b[32m[610, 2] valid_loss: 5.675292253494263\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[611, 13] trainning loss: 9.365331590175629\u001b[31m\n",
      "\u001b[32m[611, 2] valid_loss: 5.675395727157593\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[612, 13] trainning loss: 9.365387856960297\u001b[31m\n",
      "\u001b[32m[612, 2] valid_loss: 5.675339221954346\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[613, 13] trainning loss: 9.365359038114548\u001b[31m\n",
      "\u001b[32m[613, 2] valid_loss: 5.675444602966309\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[614, 13] trainning loss: 9.36544418334961\u001b[31m\n",
      "\u001b[32m[614, 2] valid_loss: 5.67531681060791\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[615, 13] trainning loss: 9.365436047315598\u001b[31m\n",
      "\u001b[32m[615, 2] valid_loss: 5.675384998321533\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[616, 13] trainning loss: 9.36545330286026\u001b[31m\n",
      "\u001b[32m[616, 2] valid_loss: 5.675334215164185\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[617, 13] trainning loss: 9.365387082099915\u001b[31m\n",
      "\u001b[32m[617, 2] valid_loss: 5.675507545471191\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[618, 13] trainning loss: 9.365429520606995\u001b[31m\n",
      "\u001b[32m[618, 2] valid_loss: 5.675323963165283\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[619, 13] trainning loss: 9.365351885557175\u001b[31m\n",
      "\u001b[32m[619, 2] valid_loss: 5.675457954406738\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[620, 13] trainning loss: 9.365469574928284\u001b[31m\n",
      "\u001b[32m[620, 2] valid_loss: 5.675414085388184\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[621, 13] trainning loss: 9.365501642227173\u001b[31m\n",
      "\u001b[32m[621, 2] valid_loss: 5.675447463989258\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[622, 13] trainning loss: 9.365519613027573\u001b[31m\n",
      "\u001b[32m[622, 2] valid_loss: 5.675515413284302\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[623, 13] trainning loss: 9.36543557047844\u001b[31m\n",
      "\u001b[32m[623, 2] valid_loss: 5.6754372119903564\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[624, 13] trainning loss: 9.365397691726685\u001b[31m\n",
      "\u001b[32m[624, 2] valid_loss: 5.67542028427124\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[625, 13] trainning loss: 9.365456730127335\u001b[31m\n",
      "\u001b[32m[625, 2] valid_loss: 5.675479412078857\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[626, 13] trainning loss: 9.365439385175705\u001b[31m\n",
      "\u001b[32m[626, 2] valid_loss: 5.6755688190460205\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[627, 13] trainning loss: 9.365413933992386\u001b[31m\n",
      "\u001b[32m[627, 2] valid_loss: 5.6754443645477295\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[628, 13] trainning loss: 9.365423828363419\u001b[31m\n",
      "\u001b[32m[628, 2] valid_loss: 5.67548394203186\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[629, 13] trainning loss: 9.365415573120117\u001b[31m\n",
      "\u001b[32m[629, 2] valid_loss: 5.675615310668945\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[630, 13] trainning loss: 9.365359544754028\u001b[31m\n",
      "\u001b[32m[630, 2] valid_loss: 5.675461769104004\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[631, 13] trainning loss: 9.365460634231567\u001b[31m\n",
      "\u001b[32m[631, 2] valid_loss: 5.6755211353302\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[632, 13] trainning loss: 9.365434050559998\u001b[31m\n",
      "\u001b[32m[632, 2] valid_loss: 5.675540924072266\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[633, 13] trainning loss: 9.365500718355179\u001b[31m\n",
      "\u001b[32m[633, 2] valid_loss: 5.675485610961914\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[634, 13] trainning loss: 9.365407288074493\u001b[31m\n",
      "\u001b[32m[634, 2] valid_loss: 5.675530433654785\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[635, 13] trainning loss: 9.365403294563293\u001b[31m\n",
      "\u001b[32m[635, 2] valid_loss: 5.675434827804565\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[636, 13] trainning loss: 9.365445643663406\u001b[31m\n",
      "\u001b[32m[636, 2] valid_loss: 5.675452947616577\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[637, 13] trainning loss: 9.365484803915024\u001b[31m\n",
      "\u001b[32m[637, 2] valid_loss: 5.675511360168457\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[638, 13] trainning loss: 9.365566164255142\u001b[31m\n",
      "\u001b[32m[638, 2] valid_loss: 5.675488471984863\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[639, 13] trainning loss: 9.365534156560898\u001b[31m\n",
      "\u001b[32m[639, 2] valid_loss: 5.675355434417725\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[640, 13] trainning loss: 9.365510493516922\u001b[31m\n",
      "\u001b[32m[640, 2] valid_loss: 5.67542576789856\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[641, 13] trainning loss: 9.365486472845078\u001b[31m\n",
      "\u001b[32m[641, 2] valid_loss: 5.6754491329193115\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[642, 13] trainning loss: 9.365421026945114\u001b[31m\n",
      "\u001b[32m[642, 2] valid_loss: 5.675482749938965\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[643, 13] trainning loss: 9.365391939878464\u001b[31m\n",
      "\u001b[32m[643, 2] valid_loss: 5.6753623485565186\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[644, 13] trainning loss: 9.365565538406372\u001b[31m\n",
      "\u001b[32m[644, 2] valid_loss: 5.675405979156494\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[645, 13] trainning loss: 9.36549961566925\u001b[31m\n",
      "\u001b[32m[645, 2] valid_loss: 5.675368547439575\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[646, 13] trainning loss: 9.365447968244553\u001b[31m\n",
      "\u001b[32m[646, 2] valid_loss: 5.675429105758667\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[647, 13] trainning loss: 9.365344762802124\u001b[31m\n",
      "\u001b[32m[647, 2] valid_loss: 5.675438642501831\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[648, 13] trainning loss: 9.365506440401077\u001b[31m\n",
      "\u001b[32m[648, 2] valid_loss: 5.675504922866821\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[649, 13] trainning loss: 9.365504652261734\u001b[31m\n",
      "\u001b[32m[649, 2] valid_loss: 5.675472259521484\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[650, 13] trainning loss: 9.365516126155853\u001b[31m\n",
      "\u001b[32m[650, 2] valid_loss: 5.675449371337891\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[651, 13] trainning loss: 9.365477532148361\u001b[31m\n",
      "\u001b[32m[651, 2] valid_loss: 5.675384998321533\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[652, 13] trainning loss: 9.36548724770546\u001b[31m\n",
      "\u001b[32m[652, 2] valid_loss: 5.675456762313843\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[653, 13] trainning loss: 9.36551383137703\u001b[31m\n",
      "\u001b[32m[653, 2] valid_loss: 5.675391435623169\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[654, 13] trainning loss: 9.365481972694397\u001b[31m\n",
      "\u001b[32m[654, 2] valid_loss: 5.675482749938965\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[655, 13] trainning loss: 9.365494459867477\u001b[31m\n",
      "\u001b[32m[655, 2] valid_loss: 5.675506830215454\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[656, 13] trainning loss: 9.365560591220856\u001b[31m\n",
      "\u001b[32m[656, 2] valid_loss: 5.675468444824219\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[657, 13] trainning loss: 9.36540400981903\u001b[31m\n",
      "\u001b[32m[657, 2] valid_loss: 5.675582647323608\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[658, 13] trainning loss: 9.365466207265854\u001b[31m\n",
      "\u001b[32m[658, 2] valid_loss: 5.675504207611084\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[659, 13] trainning loss: 9.365433782339096\u001b[31m\n",
      "\u001b[32m[659, 2] valid_loss: 5.675509452819824\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[660, 13] trainning loss: 9.365404814481735\u001b[31m\n",
      "\u001b[32m[660, 2] valid_loss: 5.67545223236084\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[661, 13] trainning loss: 9.36536231637001\u001b[31m\n",
      "\u001b[32m[661, 2] valid_loss: 5.6754395961761475\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[662, 13] trainning loss: 9.365394085645676\u001b[31m\n",
      "\u001b[32m[662, 2] valid_loss: 5.675518989562988\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[663, 13] trainning loss: 9.365377813577652\u001b[31m\n",
      "\u001b[32m[663, 2] valid_loss: 5.6755759716033936\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[664, 13] trainning loss: 9.3654206097126\u001b[31m\n",
      "\u001b[32m[664, 2] valid_loss: 5.675472259521484\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[665, 13] trainning loss: 9.36540287733078\u001b[31m\n",
      "\u001b[32m[665, 2] valid_loss: 5.675631046295166\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[666, 13] trainning loss: 9.365439057350159\u001b[31m\n",
      "\u001b[32m[666, 2] valid_loss: 5.675520658493042\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[667, 13] trainning loss: 9.36541160941124\u001b[31m\n",
      "\u001b[32m[667, 2] valid_loss: 5.67539119720459\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[668, 13] trainning loss: 9.365453541278839\u001b[31m\n",
      "\u001b[32m[668, 2] valid_loss: 5.6754374504089355\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[669, 13] trainning loss: 9.36541086435318\u001b[31m\n",
      "\u001b[32m[669, 2] valid_loss: 5.675481796264648\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[670, 13] trainning loss: 9.36548376083374\u001b[31m\n",
      "\u001b[32m[670, 2] valid_loss: 5.675433397293091\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[671, 13] trainning loss: 9.365400850772858\u001b[31m\n",
      "\u001b[32m[671, 2] valid_loss: 5.6755383014678955\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[672, 13] trainning loss: 9.365470200777054\u001b[31m\n",
      "\u001b[32m[672, 2] valid_loss: 5.675420522689819\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[673, 13] trainning loss: 9.365453630685806\u001b[31m\n",
      "\u001b[32m[673, 2] valid_loss: 5.675320863723755\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[674, 13] trainning loss: 9.365422517061234\u001b[31m\n",
      "\u001b[32m[674, 2] valid_loss: 5.675379276275635\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[675, 13] trainning loss: 9.365474164485931\u001b[31m\n",
      "\u001b[32m[675, 2] valid_loss: 5.675354242324829\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[676, 13] trainning loss: 9.365444362163544\u001b[31m\n",
      "\u001b[32m[676, 2] valid_loss: 5.675373315811157\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[677, 13] trainning loss: 9.365459889173508\u001b[31m\n",
      "\u001b[32m[677, 2] valid_loss: 5.675405979156494\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[678, 13] trainning loss: 9.365563869476318\u001b[31m\n",
      "\u001b[32m[678, 2] valid_loss: 5.6754539012908936\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[679, 13] trainning loss: 9.36543893814087\u001b[31m\n",
      "\u001b[32m[679, 2] valid_loss: 5.675420761108398\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[680, 13] trainning loss: 9.365507870912552\u001b[31m\n",
      "\u001b[32m[680, 2] valid_loss: 5.6753623485565186\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[681, 13] trainning loss: 9.3655503988266\u001b[31m\n",
      "\u001b[32m[681, 2] valid_loss: 5.675314664840698\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[682, 13] trainning loss: 9.365484237670898\u001b[31m\n",
      "\u001b[32m[682, 2] valid_loss: 5.675363779067993\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[683, 13] trainning loss: 9.365499049425125\u001b[31m\n",
      "\u001b[32m[683, 2] valid_loss: 5.675380229949951\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[684, 13] trainning loss: 9.365530580282211\u001b[31m\n",
      "\u001b[32m[684, 2] valid_loss: 5.675369024276733\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[685, 13] trainning loss: 9.365514576435089\u001b[31m\n",
      "\u001b[32m[685, 2] valid_loss: 5.6754090785980225\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[686, 13] trainning loss: 9.365557432174683\u001b[31m\n",
      "\u001b[32m[686, 2] valid_loss: 5.675485134124756\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[687, 13] trainning loss: 9.365620702505112\u001b[31m\n",
      "\u001b[32m[687, 2] valid_loss: 5.675463914871216\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[688, 13] trainning loss: 9.365573942661285\u001b[31m\n",
      "\u001b[32m[688, 2] valid_loss: 5.675432920455933\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[689, 13] trainning loss: 9.36540800333023\u001b[31m\n",
      "\u001b[32m[689, 2] valid_loss: 5.675311326980591\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[690, 13] trainning loss: 9.365580320358276\u001b[31m\n",
      "\u001b[32m[690, 2] valid_loss: 5.675367832183838\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[691, 13] trainning loss: 9.365500539541245\u001b[31m\n",
      "\u001b[32m[691, 2] valid_loss: 5.675353527069092\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[692, 13] trainning loss: 9.365494102239609\u001b[31m\n",
      "\u001b[32m[692, 2] valid_loss: 5.675408601760864\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[693, 13] trainning loss: 9.36547914147377\u001b[31m\n",
      "\u001b[32m[693, 2] valid_loss: 5.675478935241699\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[694, 13] trainning loss: 9.365512490272522\u001b[31m\n",
      "\u001b[32m[694, 2] valid_loss: 5.675333261489868\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[695, 13] trainning loss: 9.3655287027359\u001b[31m\n",
      "\u001b[32m[695, 2] valid_loss: 5.675364255905151\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[696, 13] trainning loss: 9.36557188630104\u001b[31m\n",
      "\u001b[32m[696, 2] valid_loss: 5.675413370132446\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[697, 13] trainning loss: 9.3655726313591\u001b[31m\n",
      "\u001b[32m[697, 2] valid_loss: 5.675403118133545\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[698, 13] trainning loss: 9.365496784448624\u001b[31m\n",
      "\u001b[32m[698, 2] valid_loss: 5.675448417663574\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[699, 13] trainning loss: 9.365456461906433\u001b[31m\n",
      "\u001b[32m[699, 2] valid_loss: 5.675329923629761\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[700, 13] trainning loss: 9.365425288677216\u001b[31m\n",
      "\u001b[32m[700, 2] valid_loss: 5.675430059432983\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[701, 13] trainning loss: 9.365566492080688\u001b[31m\n",
      "\u001b[32m[701, 2] valid_loss: 5.675386667251587\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[702, 13] trainning loss: 9.36543396115303\u001b[31m\n",
      "\u001b[32m[702, 2] valid_loss: 5.675414562225342\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[703, 13] trainning loss: 9.365502774715424\u001b[31m\n",
      "\u001b[32m[703, 2] valid_loss: 5.675338268280029\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[704, 13] trainning loss: 9.365518808364868\u001b[31m\n",
      "\u001b[32m[704, 2] valid_loss: 5.675450086593628\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[705, 13] trainning loss: 9.365542888641357\u001b[31m\n",
      "\u001b[32m[705, 2] valid_loss: 5.67541241645813\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[706, 13] trainning loss: 9.365517973899841\u001b[31m\n",
      "\u001b[32m[706, 2] valid_loss: 5.675474166870117\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[707, 13] trainning loss: 9.36544093489647\u001b[31m\n",
      "\u001b[32m[707, 2] valid_loss: 5.6754796504974365\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[708, 13] trainning loss: 9.365434139966965\u001b[31m\n",
      "\u001b[32m[708, 2] valid_loss: 5.675443410873413\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[709, 13] trainning loss: 9.365473181009293\u001b[31m\n",
      "\u001b[32m[709, 2] valid_loss: 5.675454616546631\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[710, 13] trainning loss: 9.365536212921143\u001b[31m\n",
      "\u001b[32m[710, 2] valid_loss: 5.675363302230835\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[711, 13] trainning loss: 9.365556508302689\u001b[31m\n",
      "\u001b[32m[711, 2] valid_loss: 5.675400733947754\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[712, 13] trainning loss: 9.365464836359024\u001b[31m\n",
      "\u001b[32m[712, 2] valid_loss: 5.675470590591431\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[713, 13] trainning loss: 9.365442335605621\u001b[31m\n",
      "\u001b[32m[713, 2] valid_loss: 5.67535400390625\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[714, 13] trainning loss: 9.365427523851395\u001b[31m\n",
      "\u001b[32m[714, 2] valid_loss: 5.675431251525879\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[715, 13] trainning loss: 9.365494191646576\u001b[31m\n",
      "\u001b[32m[715, 2] valid_loss: 5.675368070602417\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[716, 13] trainning loss: 9.365462273359299\u001b[31m\n",
      "\u001b[32m[716, 2] valid_loss: 5.675417900085449\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[717, 13] trainning loss: 9.365449219942093\u001b[31m\n",
      "\u001b[32m[717, 2] valid_loss: 5.675400495529175\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[718, 13] trainning loss: 9.365503281354904\u001b[31m\n",
      "\u001b[32m[718, 2] valid_loss: 5.6753928661346436\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[719, 13] trainning loss: 9.365545868873596\u001b[31m\n",
      "\u001b[32m[719, 2] valid_loss: 5.675364255905151\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[720, 13] trainning loss: 9.36549299955368\u001b[31m\n",
      "\u001b[32m[720, 2] valid_loss: 5.675309896469116\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[721, 13] trainning loss: 9.365436017513275\u001b[31m\n",
      "\u001b[32m[721, 2] valid_loss: 5.675384759902954\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[722, 13] trainning loss: 9.36550173163414\u001b[31m\n",
      "\u001b[32m[722, 2] valid_loss: 5.6752707958221436\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[723, 13] trainning loss: 9.36551284790039\u001b[31m\n",
      "\u001b[32m[723, 2] valid_loss: 5.675369024276733\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[724, 13] trainning loss: 9.365452140569687\u001b[31m\n",
      "\u001b[32m[724, 2] valid_loss: 5.675426959991455\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[725, 13] trainning loss: 9.365457892417908\u001b[31m\n",
      "\u001b[32m[725, 2] valid_loss: 5.6753830909729\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[726, 13] trainning loss: 9.365524739027023\u001b[31m\n",
      "\u001b[32m[726, 2] valid_loss: 5.6754231452941895\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[727, 13] trainning loss: 9.365554928779602\u001b[31m\n",
      "\u001b[32m[727, 2] valid_loss: 5.67553448677063\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[728, 13] trainning loss: 9.365481436252594\u001b[31m\n",
      "\u001b[32m[728, 2] valid_loss: 5.675426006317139\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[729, 13] trainning loss: 9.365604609251022\u001b[31m\n",
      "\u001b[32m[729, 2] valid_loss: 5.675304412841797\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[730, 13] trainning loss: 9.365497469902039\u001b[31m\n",
      "\u001b[32m[730, 2] valid_loss: 5.6752846240997314\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[731, 13] trainning loss: 9.365545779466629\u001b[31m\n",
      "\u001b[32m[731, 2] valid_loss: 5.675587892532349\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[732, 13] trainning loss: 9.365513443946838\u001b[31m\n",
      "\u001b[32m[732, 2] valid_loss: 5.675381183624268\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[733, 13] trainning loss: 9.365500062704086\u001b[31m\n",
      "\u001b[32m[733, 2] valid_loss: 5.675454378128052\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[734, 13] trainning loss: 9.365380465984344\u001b[31m\n",
      "\u001b[32m[734, 2] valid_loss: 5.675534248352051\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[735, 13] trainning loss: 9.365515112876892\u001b[31m\n",
      "\u001b[32m[735, 2] valid_loss: 5.675544261932373\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[736, 13] trainning loss: 9.365574985742569\u001b[31m\n",
      "\u001b[32m[736, 2] valid_loss: 5.675564765930176\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[737, 13] trainning loss: 9.365472376346588\u001b[31m\n",
      "\u001b[32m[737, 2] valid_loss: 5.675389528274536\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[738, 13] trainning loss: 9.365493953227997\u001b[31m\n",
      "\u001b[32m[738, 2] valid_loss: 5.675521612167358\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[739, 13] trainning loss: 9.365496814250946\u001b[31m\n",
      "\u001b[32m[739, 2] valid_loss: 5.6753692626953125\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[740, 13] trainning loss: 9.365485697984695\u001b[31m\n",
      "\u001b[32m[740, 2] valid_loss: 5.67533278465271\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[741, 13] trainning loss: 9.365465074777603\u001b[31m\n",
      "\u001b[32m[741, 2] valid_loss: 5.675376653671265\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[742, 13] trainning loss: 9.365468800067902\u001b[31m\n",
      "\u001b[32m[742, 2] valid_loss: 5.675487279891968\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[743, 13] trainning loss: 9.365484327077866\u001b[31m\n",
      "\u001b[32m[743, 2] valid_loss: 5.675439119338989\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[744, 13] trainning loss: 9.365432649850845\u001b[31m\n",
      "\u001b[32m[744, 2] valid_loss: 5.675472974777222\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[745, 13] trainning loss: 9.365448325872421\u001b[31m\n",
      "\u001b[32m[745, 2] valid_loss: 5.675468921661377\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[746, 13] trainning loss: 9.36545342206955\u001b[31m\n",
      "\u001b[32m[746, 2] valid_loss: 5.675324201583862\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[747, 13] trainning loss: 9.365446597337723\u001b[31m\n",
      "\u001b[32m[747, 2] valid_loss: 5.675513029098511\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[748, 13] trainning loss: 9.365427792072296\u001b[31m\n",
      "\u001b[32m[748, 2] valid_loss: 5.6755359172821045\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[749, 13] trainning loss: 9.365525096654892\u001b[31m\n",
      "\u001b[32m[749, 2] valid_loss: 5.675410985946655\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[750, 13] trainning loss: 9.365496724843979\u001b[31m\n",
      "\u001b[32m[750, 2] valid_loss: 5.675475358963013\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[751, 13] trainning loss: 9.365511000156403\u001b[31m\n",
      "\u001b[32m[751, 2] valid_loss: 5.675403356552124\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[752, 13] trainning loss: 9.3655104637146\u001b[31m\n",
      "\u001b[32m[752, 2] valid_loss: 5.675339221954346\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[753, 13] trainning loss: 9.365405887365341\u001b[31m\n",
      "\u001b[32m[753, 2] valid_loss: 5.675313949584961\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[754, 13] trainning loss: 9.365522295236588\u001b[31m\n",
      "\u001b[32m[754, 2] valid_loss: 5.675498962402344\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[755, 13] trainning loss: 9.365371257066727\u001b[31m\n",
      "\u001b[32m[755, 2] valid_loss: 5.675440788269043\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[756, 13] trainning loss: 9.36543533205986\u001b[31m\n",
      "\u001b[32m[756, 2] valid_loss: 5.675503492355347\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[757, 13] trainning loss: 9.365414768457413\u001b[31m\n",
      "\u001b[32m[757, 2] valid_loss: 5.675452947616577\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[758, 13] trainning loss: 9.365483582019806\u001b[31m\n",
      "\u001b[32m[758, 2] valid_loss: 5.675303936004639\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[759, 13] trainning loss: 9.36549574136734\u001b[31m\n",
      "\u001b[32m[759, 2] valid_loss: 5.6754114627838135\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[760, 13] trainning loss: 9.365474045276642\u001b[31m\n",
      "\u001b[32m[760, 2] valid_loss: 5.675471544265747\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[761, 13] trainning loss: 9.365447461605072\u001b[31m\n",
      "\u001b[32m[761, 2] valid_loss: 5.6753857135772705\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[762, 13] trainning loss: 9.365400850772858\u001b[31m\n",
      "\u001b[32m[762, 2] valid_loss: 5.67547869682312\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[763, 13] trainning loss: 9.365438967943192\u001b[31m\n",
      "\u001b[32m[763, 2] valid_loss: 5.67547607421875\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[764, 13] trainning loss: 9.365507185459137\u001b[31m\n",
      "\u001b[32m[764, 2] valid_loss: 5.675570964813232\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[765, 13] trainning loss: 9.36547377705574\u001b[31m\n",
      "\u001b[32m[765, 2] valid_loss: 5.675498962402344\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[766, 13] trainning loss: 9.365487664937973\u001b[31m\n",
      "\u001b[32m[766, 2] valid_loss: 5.675468683242798\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[767, 13] trainning loss: 9.365426868200302\u001b[31m\n",
      "\u001b[32m[767, 2] valid_loss: 5.675500869750977\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[768, 13] trainning loss: 9.365417510271072\u001b[31m\n",
      "\u001b[32m[768, 2] valid_loss: 5.6755051612854\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[769, 13] trainning loss: 9.365527480840683\u001b[31m\n",
      "\u001b[32m[769, 2] valid_loss: 5.675371170043945\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[770, 13] trainning loss: 9.365463554859161\u001b[31m\n",
      "\u001b[32m[770, 2] valid_loss: 5.675445079803467\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[771, 13] trainning loss: 9.36541286110878\u001b[31m\n",
      "\u001b[32m[771, 2] valid_loss: 5.675472259521484\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[772, 13] trainning loss: 9.36545529961586\u001b[31m\n",
      "\u001b[32m[772, 2] valid_loss: 5.6754865646362305\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[773, 13] trainning loss: 9.365488022565842\u001b[31m\n",
      "\u001b[32m[773, 2] valid_loss: 5.6753761768341064\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[774, 13] trainning loss: 9.365478903055191\u001b[31m\n",
      "\u001b[32m[774, 2] valid_loss: 5.675604820251465\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[775, 13] trainning loss: 9.3654665350914\u001b[31m\n",
      "\u001b[32m[775, 2] valid_loss: 5.67559289932251\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[776, 13] trainning loss: 9.36545741558075\u001b[31m\n",
      "\u001b[32m[776, 2] valid_loss: 5.675536632537842\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[777, 13] trainning loss: 9.365535974502563\u001b[31m\n",
      "\u001b[32m[777, 2] valid_loss: 5.675493001937866\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[778, 13] trainning loss: 9.365501821041107\u001b[31m\n",
      "\u001b[32m[778, 2] valid_loss: 5.67546010017395\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[779, 13] trainning loss: 9.365500211715698\u001b[31m\n",
      "\u001b[32m[779, 2] valid_loss: 5.675589323043823\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[780, 13] trainning loss: 9.365522563457489\u001b[31m\n",
      "\u001b[32m[780, 2] valid_loss: 5.675564765930176\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[781, 13] trainning loss: 9.365454405546188\u001b[31m\n",
      "\u001b[32m[781, 2] valid_loss: 5.6755077838897705\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[782, 13] trainning loss: 9.365469008684158\u001b[31m\n",
      "\u001b[32m[782, 2] valid_loss: 5.675608158111572\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[783, 13] trainning loss: 9.365434676408768\u001b[31m\n",
      "\u001b[32m[783, 2] valid_loss: 5.675480127334595\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[784, 13] trainning loss: 9.365525305271149\u001b[31m\n",
      "\u001b[32m[784, 2] valid_loss: 5.675363540649414\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[785, 13] trainning loss: 9.365540772676468\u001b[31m\n",
      "\u001b[32m[785, 2] valid_loss: 5.675378799438477\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[786, 13] trainning loss: 9.365460693836212\u001b[31m\n",
      "\u001b[32m[786, 2] valid_loss: 5.675430774688721\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[787, 13] trainning loss: 9.36549288034439\u001b[31m\n",
      "\u001b[32m[787, 2] valid_loss: 5.675492525100708\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[788, 13] trainning loss: 9.365521967411041\u001b[31m\n",
      "\u001b[32m[788, 2] valid_loss: 5.675361633300781\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[789, 13] trainning loss: 9.365497410297394\u001b[31m\n",
      "\u001b[32m[789, 2] valid_loss: 5.675429344177246\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[790, 13] trainning loss: 9.365506887435913\u001b[31m\n",
      "\u001b[32m[790, 2] valid_loss: 5.675381660461426\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[791, 13] trainning loss: 9.365540862083435\u001b[31m\n",
      "\u001b[32m[791, 2] valid_loss: 5.675498962402344\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[792, 13] trainning loss: 9.365468740463257\u001b[31m\n",
      "\u001b[32m[792, 2] valid_loss: 5.675336122512817\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[793, 13] trainning loss: 9.365500628948212\u001b[31m\n",
      "\u001b[32m[793, 2] valid_loss: 5.675261497497559\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[794, 13] trainning loss: 9.365525007247925\u001b[31m\n",
      "\u001b[32m[794, 2] valid_loss: 5.675402879714966\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[795, 13] trainning loss: 9.365536630153656\u001b[31m\n",
      "\u001b[32m[795, 2] valid_loss: 5.675441265106201\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[796, 13] trainning loss: 9.365583151578903\u001b[31m\n",
      "\u001b[32m[796, 2] valid_loss: 5.675411224365234\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[797, 13] trainning loss: 9.365535646677017\u001b[31m\n",
      "\u001b[32m[797, 2] valid_loss: 5.675347566604614\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[798, 13] trainning loss: 9.365541905164719\u001b[31m\n",
      "\u001b[32m[798, 2] valid_loss: 5.6754372119903564\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[799, 13] trainning loss: 9.365414589643478\u001b[31m\n",
      "\u001b[32m[799, 2] valid_loss: 5.675317049026489\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[800, 13] trainning loss: 9.36540150642395\u001b[31m\n",
      "\u001b[32m[800, 2] valid_loss: 5.675413370132446\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[801, 13] trainning loss: 9.365421086549759\u001b[31m\n",
      "\u001b[32m[801, 2] valid_loss: 5.675411701202393\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[802, 13] trainning loss: 9.365509808063507\u001b[31m\n",
      "\u001b[32m[802, 2] valid_loss: 5.675388336181641\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[803, 13] trainning loss: 9.365403234958649\u001b[31m\n",
      "\u001b[32m[803, 2] valid_loss: 5.675409555435181\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[804, 13] trainning loss: 9.365556329488754\u001b[31m\n",
      "\u001b[32m[804, 2] valid_loss: 5.675476789474487\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[805, 13] trainning loss: 9.365587174892426\u001b[31m\n",
      "\u001b[32m[805, 2] valid_loss: 5.675373077392578\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[806, 13] trainning loss: 9.365484416484833\u001b[31m\n",
      "\u001b[32m[806, 2] valid_loss: 5.6754608154296875\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[807, 13] trainning loss: 9.36559084057808\u001b[31m\n",
      "\u001b[32m[807, 2] valid_loss: 5.675525903701782\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[808, 13] trainning loss: 9.365478664636612\u001b[31m\n",
      "\u001b[32m[808, 2] valid_loss: 5.675348997116089\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[809, 13] trainning loss: 9.365518897771835\u001b[31m\n",
      "\u001b[32m[809, 2] valid_loss: 5.675459146499634\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[810, 13] trainning loss: 9.365578800439835\u001b[31m\n",
      "\u001b[32m[810, 2] valid_loss: 5.675349950790405\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[811, 13] trainning loss: 9.365523934364319\u001b[31m\n",
      "\u001b[32m[811, 2] valid_loss: 5.675366163253784\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[812, 13] trainning loss: 9.365414321422577\u001b[31m\n",
      "\u001b[32m[812, 2] valid_loss: 5.675424575805664\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[813, 13] trainning loss: 9.365439414978027\u001b[31m\n",
      "\u001b[32m[813, 2] valid_loss: 5.675526142120361\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[814, 13] trainning loss: 9.365355432033539\u001b[31m\n",
      "\u001b[32m[814, 2] valid_loss: 5.675435304641724\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[815, 13] trainning loss: 9.365441888570786\u001b[31m\n",
      "\u001b[32m[815, 2] valid_loss: 5.675442695617676\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[816, 13] trainning loss: 9.365441471338272\u001b[31m\n",
      "\u001b[32m[816, 2] valid_loss: 5.675440788269043\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[817, 13] trainning loss: 9.365441679954529\u001b[31m\n",
      "\u001b[32m[817, 2] valid_loss: 5.675480127334595\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[818, 13] trainning loss: 9.365372747182846\u001b[31m\n",
      "\u001b[32m[818, 2] valid_loss: 5.675558567047119\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[819, 13] trainning loss: 9.36534208059311\u001b[31m\n",
      "\u001b[32m[819, 2] valid_loss: 5.675530672073364\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[820, 13] trainning loss: 9.365354508161545\u001b[31m\n",
      "\u001b[32m[820, 2] valid_loss: 5.675519227981567\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[821, 13] trainning loss: 9.365437746047974\u001b[31m\n",
      "\u001b[32m[821, 2] valid_loss: 5.675487041473389\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[822, 13] trainning loss: 9.3654143512249\u001b[31m\n",
      "\u001b[32m[822, 2] valid_loss: 5.675474405288696\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[823, 13] trainning loss: 9.365504920482635\u001b[31m\n",
      "\u001b[32m[823, 2] valid_loss: 5.675481796264648\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[824, 13] trainning loss: 9.365489840507507\u001b[31m\n",
      "\u001b[32m[824, 2] valid_loss: 5.675572633743286\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[825, 13] trainning loss: 9.365395218133926\u001b[31m\n",
      "\u001b[32m[825, 2] valid_loss: 5.675487756729126\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[826, 13] trainning loss: 9.365477323532104\u001b[31m\n",
      "\u001b[32m[826, 2] valid_loss: 5.675419569015503\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[827, 13] trainning loss: 9.365447640419006\u001b[31m\n",
      "\u001b[32m[827, 2] valid_loss: 5.675535202026367\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[828, 13] trainning loss: 9.365362733602524\u001b[31m\n",
      "\u001b[32m[828, 2] valid_loss: 5.675462245941162\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[829, 13] trainning loss: 9.365379929542542\u001b[31m\n",
      "\u001b[32m[829, 2] valid_loss: 5.675465106964111\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[830, 13] trainning loss: 9.365427315235138\u001b[31m\n",
      "\u001b[32m[830, 2] valid_loss: 5.67546820640564\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[831, 13] trainning loss: 9.365413844585419\u001b[31m\n",
      "\u001b[32m[831, 2] valid_loss: 5.675522565841675\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[832, 13] trainning loss: 9.365373283624649\u001b[31m\n",
      "\u001b[32m[832, 2] valid_loss: 5.675386905670166\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[833, 13] trainning loss: 9.365378260612488\u001b[31m\n",
      "\u001b[32m[833, 2] valid_loss: 5.67542839050293\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[834, 13] trainning loss: 9.36542072892189\u001b[31m\n",
      "\u001b[32m[834, 2] valid_loss: 5.675508260726929\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[835, 13] trainning loss: 9.365396589040756\u001b[31m\n",
      "\u001b[32m[835, 2] valid_loss: 5.675353527069092\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[836, 13] trainning loss: 9.365428060293198\u001b[31m\n",
      "\u001b[32m[836, 2] valid_loss: 5.675415277481079\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[837, 13] trainning loss: 9.36538752913475\u001b[31m\n",
      "\u001b[32m[837, 2] valid_loss: 5.6754584312438965\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[838, 13] trainning loss: 9.365491211414337\u001b[31m\n",
      "\u001b[32m[838, 2] valid_loss: 5.675441741943359\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[839, 13] trainning loss: 9.365445494651794\u001b[31m\n",
      "\u001b[32m[839, 2] valid_loss: 5.675443649291992\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[840, 13] trainning loss: 9.365399211645126\u001b[31m\n",
      "\u001b[32m[840, 2] valid_loss: 5.675398349761963\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[841, 13] trainning loss: 9.365436255931854\u001b[31m\n",
      "\u001b[32m[841, 2] valid_loss: 5.675466775894165\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[842, 13] trainning loss: 9.365405023097992\u001b[31m\n",
      "\u001b[32m[842, 2] valid_loss: 5.675482988357544\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[843, 13] trainning loss: 9.365448325872421\u001b[31m\n",
      "\u001b[32m[843, 2] valid_loss: 5.67539119720459\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[844, 13] trainning loss: 9.365428984165192\u001b[31m\n",
      "\u001b[32m[844, 2] valid_loss: 5.675332307815552\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[845, 13] trainning loss: 9.365396529436111\u001b[31m\n",
      "\u001b[32m[845, 2] valid_loss: 5.6754372119903564\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[846, 13] trainning loss: 9.36538651585579\u001b[31m\n",
      "\u001b[32m[846, 2] valid_loss: 5.675325155258179\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[847, 13] trainning loss: 9.36541411280632\u001b[31m\n",
      "\u001b[32m[847, 2] valid_loss: 5.6754560470581055\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[848, 13] trainning loss: 9.365469872951508\u001b[31m\n",
      "\u001b[32m[848, 2] valid_loss: 5.675562620162964\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[849, 13] trainning loss: 9.365463078022003\u001b[31m\n",
      "\u001b[32m[849, 2] valid_loss: 5.675456285476685\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[850, 13] trainning loss: 9.365559488534927\u001b[31m\n",
      "\u001b[32m[850, 2] valid_loss: 5.6754310131073\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[851, 13] trainning loss: 9.365415692329407\u001b[31m\n",
      "\u001b[32m[851, 2] valid_loss: 5.675341844558716\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[852, 13] trainning loss: 9.365414500236511\u001b[31m\n",
      "\u001b[32m[852, 2] valid_loss: 5.675424814224243\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[853, 13] trainning loss: 9.365413755178452\u001b[31m\n",
      "\u001b[32m[853, 2] valid_loss: 5.675368070602417\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[854, 13] trainning loss: 9.365484654903412\u001b[31m\n",
      "\u001b[32m[854, 2] valid_loss: 5.6754584312438965\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[855, 13] trainning loss: 9.365495324134827\u001b[31m\n",
      "\u001b[32m[855, 2] valid_loss: 5.675304651260376\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[856, 13] trainning loss: 9.365481704473495\u001b[31m\n",
      "\u001b[32m[856, 2] valid_loss: 5.675374984741211\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[857, 13] trainning loss: 9.365415126085281\u001b[31m\n",
      "\u001b[32m[857, 2] valid_loss: 5.675413608551025\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[858, 13] trainning loss: 9.365550875663757\u001b[31m\n",
      "\u001b[32m[858, 2] valid_loss: 5.675477504730225\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[859, 13] trainning loss: 9.365533530712128\u001b[31m\n",
      "\u001b[32m[859, 2] valid_loss: 5.67548680305481\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[860, 13] trainning loss: 9.365449786186218\u001b[31m\n",
      "\u001b[32m[860, 2] valid_loss: 5.67531156539917\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[861, 13] trainning loss: 9.365496784448624\u001b[31m\n",
      "\u001b[32m[861, 2] valid_loss: 5.675389528274536\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[862, 13] trainning loss: 9.365493386983871\u001b[31m\n",
      "\u001b[32m[862, 2] valid_loss: 5.675345420837402\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[863, 13] trainning loss: 9.36538890004158\u001b[31m\n",
      "\u001b[32m[863, 2] valid_loss: 5.675333738327026\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[864, 13] trainning loss: 9.365381211042404\u001b[31m\n",
      "\u001b[32m[864, 2] valid_loss: 5.675379037857056\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[865, 13] trainning loss: 9.36548736691475\u001b[31m\n",
      "\u001b[32m[865, 2] valid_loss: 5.675368309020996\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[866, 13] trainning loss: 9.3654226064682\u001b[31m\n",
      "\u001b[32m[866, 2] valid_loss: 5.675420045852661\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[867, 13] trainning loss: 9.365467458963394\u001b[31m\n",
      "\u001b[32m[867, 2] valid_loss: 5.675544500350952\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[868, 13] trainning loss: 9.365515559911728\u001b[31m\n",
      "\u001b[32m[868, 2] valid_loss: 5.675481557846069\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[869, 13] trainning loss: 9.365513652563095\u001b[31m\n",
      "\u001b[32m[869, 2] valid_loss: 5.6755006313323975\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[870, 13] trainning loss: 9.365544140338898\u001b[31m\n",
      "\u001b[32m[870, 2] valid_loss: 5.675495386123657\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[871, 13] trainning loss: 9.365466982126236\u001b[31m\n",
      "\u001b[32m[871, 2] valid_loss: 5.675370216369629\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[872, 13] trainning loss: 9.365432471036911\u001b[31m\n",
      "\u001b[32m[872, 2] valid_loss: 5.675539493560791\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[873, 13] trainning loss: 9.365410834550858\u001b[31m\n",
      "\u001b[32m[873, 2] valid_loss: 5.6753809452056885\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[874, 13] trainning loss: 9.36543133854866\u001b[31m\n",
      "\u001b[32m[874, 2] valid_loss: 5.675501108169556\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[875, 13] trainning loss: 9.365435749292374\u001b[31m\n",
      "\u001b[32m[875, 2] valid_loss: 5.675482988357544\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[876, 13] trainning loss: 9.365475177764893\u001b[31m\n",
      "\u001b[32m[876, 2] valid_loss: 5.675451040267944\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[877, 13] trainning loss: 9.365544140338898\u001b[31m\n",
      "\u001b[32m[877, 2] valid_loss: 5.675443649291992\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[878, 13] trainning loss: 9.365480095148087\u001b[31m\n",
      "\u001b[32m[878, 2] valid_loss: 5.675405979156494\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[879, 13] trainning loss: 9.365422278642654\u001b[31m\n",
      "\u001b[32m[879, 2] valid_loss: 5.67540431022644\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[880, 13] trainning loss: 9.365501701831818\u001b[31m\n",
      "\u001b[32m[880, 2] valid_loss: 5.675487995147705\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[881, 13] trainning loss: 9.365475535392761\u001b[31m\n",
      "\u001b[32m[881, 2] valid_loss: 5.675541162490845\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[882, 13] trainning loss: 9.365505516529083\u001b[31m\n",
      "\u001b[32m[882, 2] valid_loss: 5.675362586975098\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[883, 13] trainning loss: 9.365535855293274\u001b[31m\n",
      "\u001b[32m[883, 2] valid_loss: 5.675384998321533\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[884, 13] trainning loss: 9.36542135477066\u001b[31m\n",
      "\u001b[32m[884, 2] valid_loss: 5.6753880977630615\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[885, 13] trainning loss: 9.365409255027771\u001b[31m\n",
      "\u001b[32m[885, 2] valid_loss: 5.675462484359741\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[886, 13] trainning loss: 9.365430802106857\u001b[31m\n",
      "\u001b[32m[886, 2] valid_loss: 5.67538046836853\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[887, 13] trainning loss: 9.365433365106583\u001b[31m\n",
      "\u001b[32m[887, 2] valid_loss: 5.675216913223267\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6752).  Saving model ...\n",
      "\u001b[31m[888, 13] trainning loss: 9.365395337343216\u001b[31m\n",
      "\u001b[32m[888, 2] valid_loss: 5.675395965576172\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[889, 13] trainning loss: 9.365354508161545\u001b[31m\n",
      "\u001b[32m[889, 2] valid_loss: 5.675397634506226\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[890, 13] trainning loss: 9.365450412034988\u001b[31m\n",
      "\u001b[32m[890, 2] valid_loss: 5.675418853759766\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[891, 13] trainning loss: 9.365239053964615\u001b[31m\n",
      "\u001b[32m[891, 2] valid_loss: 5.675222635269165\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6752).  Saving model ...\n",
      "\u001b[31m[892, 13] trainning loss: 9.365417867898941\u001b[31m\n",
      "\u001b[32m[892, 2] valid_loss: 5.675335168838501\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[893, 13] trainning loss: 9.365350842475891\u001b[31m\n",
      "\u001b[32m[893, 2] valid_loss: 5.675354480743408\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[894, 13] trainning loss: 9.365400642156601\u001b[31m\n",
      "\u001b[32m[894, 2] valid_loss: 5.675355672836304\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[895, 13] trainning loss: 9.365443706512451\u001b[31m\n",
      "\u001b[32m[895, 2] valid_loss: 5.6753599643707275\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[896, 13] trainning loss: 9.365414083003998\u001b[31m\n",
      "\u001b[32m[896, 2] valid_loss: 5.675379276275635\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[897, 13] trainning loss: 9.365386694669724\u001b[31m\n",
      "\u001b[32m[897, 2] valid_loss: 5.675358533859253\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[898, 13] trainning loss: 9.365322887897491\u001b[31m\n",
      "\u001b[32m[898, 2] valid_loss: 5.675360918045044\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[899, 13] trainning loss: 9.365350365638733\u001b[31m\n",
      "\u001b[32m[899, 2] valid_loss: 5.675343036651611\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[900, 13] trainning loss: 9.365390598773956\u001b[31m\n",
      "\u001b[32m[900, 2] valid_loss: 5.675545692443848\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[901, 13] trainning loss: 9.365396916866302\u001b[31m\n",
      "\u001b[32m[901, 2] valid_loss: 5.675458192825317\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[902, 13] trainning loss: 9.365366369485855\u001b[31m\n",
      "\u001b[32m[902, 2] valid_loss: 5.675233364105225\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6752).  Saving model ...\n",
      "\u001b[31m[903, 13] trainning loss: 9.36537218093872\u001b[31m\n",
      "\u001b[32m[903, 2] valid_loss: 5.675327301025391\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[904, 13] trainning loss: 9.365248411893845\u001b[31m\n",
      "\u001b[32m[904, 2] valid_loss: 5.675367116928101\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[905, 13] trainning loss: 9.36535033583641\u001b[31m\n",
      "\u001b[32m[905, 2] valid_loss: 5.67536187171936\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[906, 13] trainning loss: 9.365350186824799\u001b[31m\n",
      "\u001b[32m[906, 2] valid_loss: 5.675385236740112\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[907, 13] trainning loss: 9.365379989147186\u001b[31m\n",
      "\u001b[32m[907, 2] valid_loss: 5.675407409667969\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[908, 13] trainning loss: 9.36539602279663\u001b[31m\n",
      "\u001b[32m[908, 2] valid_loss: 5.675328493118286\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[909, 13] trainning loss: 9.36535593867302\u001b[31m\n",
      "\u001b[32m[909, 2] valid_loss: 5.675313711166382\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[910, 13] trainning loss: 9.365352362394333\u001b[31m\n",
      "\u001b[32m[910, 2] valid_loss: 5.67541766166687\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[911, 13] trainning loss: 9.36534121632576\u001b[31m\n",
      "\u001b[32m[911, 2] valid_loss: 5.675293445587158\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[912, 13] trainning loss: 9.365357786417007\u001b[31m\n",
      "\u001b[32m[912, 2] valid_loss: 5.6754255294799805\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[913, 13] trainning loss: 9.365319937467575\u001b[31m\n",
      "\u001b[32m[913, 2] valid_loss: 5.675348997116089\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[914, 13] trainning loss: 9.36538177728653\u001b[31m\n",
      "\u001b[32m[914, 2] valid_loss: 5.675381422042847\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[915, 13] trainning loss: 9.36536979675293\u001b[31m\n",
      "\u001b[32m[915, 2] valid_loss: 5.675436496734619\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[916, 13] trainning loss: 9.36537691950798\u001b[31m\n",
      "\u001b[32m[916, 2] valid_loss: 5.675405025482178\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[917, 13] trainning loss: 9.365275651216507\u001b[31m\n",
      "\u001b[32m[917, 2] valid_loss: 5.675333023071289\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[918, 13] trainning loss: 9.365322142839432\u001b[31m\n",
      "\u001b[32m[918, 2] valid_loss: 5.67534065246582\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[919, 13] trainning loss: 9.365328878164291\u001b[31m\n",
      "\u001b[32m[919, 2] valid_loss: 5.675456285476685\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[920, 13] trainning loss: 9.365306615829468\u001b[31m\n",
      "\u001b[32m[920, 2] valid_loss: 5.675337553024292\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[921, 13] trainning loss: 9.365330427885056\u001b[31m\n",
      "\u001b[32m[921, 2] valid_loss: 5.675406217575073\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[922, 13] trainning loss: 9.365257680416107\u001b[31m\n",
      "\u001b[32m[922, 2] valid_loss: 5.675443649291992\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[923, 13] trainning loss: 9.365262806415558\u001b[31m\n",
      "\u001b[32m[923, 2] valid_loss: 5.675533294677734\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[924, 13] trainning loss: 9.36534669995308\u001b[31m\n",
      "\u001b[32m[924, 2] valid_loss: 5.6754372119903564\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[925, 13] trainning loss: 9.365336954593658\u001b[31m\n",
      "\u001b[32m[925, 2] valid_loss: 5.675503253936768\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[926, 13] trainning loss: 9.365360736846924\u001b[31m\n",
      "\u001b[32m[926, 2] valid_loss: 5.675524950027466\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[927, 13] trainning loss: 9.365324556827545\u001b[31m\n",
      "\u001b[32m[927, 2] valid_loss: 5.6755170822143555\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[928, 13] trainning loss: 9.36521565914154\u001b[31m\n",
      "\u001b[32m[928, 2] valid_loss: 5.675549268722534\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[929, 13] trainning loss: 9.36537054181099\u001b[31m\n",
      "\u001b[32m[929, 2] valid_loss: 5.6754069328308105\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[930, 13] trainning loss: 9.365303069353104\u001b[31m\n",
      "\u001b[32m[930, 2] valid_loss: 5.675544738769531\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[931, 13] trainning loss: 9.365321010351181\u001b[31m\n",
      "\u001b[32m[931, 2] valid_loss: 5.675590515136719\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[932, 13] trainning loss: 9.365320950746536\u001b[31m\n",
      "\u001b[32m[932, 2] valid_loss: 5.675480604171753\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[933, 13] trainning loss: 9.365238457918167\u001b[31m\n",
      "\u001b[32m[933, 2] valid_loss: 5.675294637680054\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[934, 13] trainning loss: 9.365328848361969\u001b[31m\n",
      "\u001b[32m[934, 2] valid_loss: 5.675389766693115\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[935, 13] trainning loss: 9.365248173475266\u001b[31m\n",
      "\u001b[32m[935, 2] valid_loss: 5.675270318984985\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[936, 13] trainning loss: 9.365377962589264\u001b[31m\n",
      "\u001b[32m[936, 2] valid_loss: 5.675390243530273\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[937, 13] trainning loss: 9.365326404571533\u001b[31m\n",
      "\u001b[32m[937, 2] valid_loss: 5.675338506698608\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6753).  Saving model ...\n",
      "\u001b[31m[938, 13] trainning loss: 9.36537390947342\u001b[31m\n",
      "\u001b[32m[938, 2] valid_loss: 5.675434827804565\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[939, 13] trainning loss: 9.365297585725784\u001b[31m\n",
      "\u001b[32m[939, 2] valid_loss: 5.6754865646362305\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[940, 13] trainning loss: 9.365277290344238\u001b[31m\n",
      "\u001b[32m[940, 2] valid_loss: 5.675374269485474\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[941, 13] trainning loss: 9.365314215421677\u001b[31m\n",
      "\u001b[32m[941, 2] valid_loss: 5.675402641296387\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[942, 13] trainning loss: 9.365284115076065\u001b[31m\n",
      "\u001b[32m[942, 2] valid_loss: 5.675475358963013\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[943, 13] trainning loss: 9.365311741828918\u001b[31m\n",
      "\u001b[32m[943, 2] valid_loss: 5.675478935241699\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[944, 13] trainning loss: 9.365287065505981\u001b[31m\n",
      "\u001b[32m[944, 2] valid_loss: 5.6755547523498535\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[945, 13] trainning loss: 9.365277767181396\u001b[31m\n",
      "\u001b[32m[945, 2] valid_loss: 5.675410509109497\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[946, 13] trainning loss: 9.365307033061981\u001b[31m\n",
      "\u001b[32m[946, 2] valid_loss: 5.675588130950928\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[947, 13] trainning loss: 9.36534497141838\u001b[31m\n",
      "\u001b[32m[947, 2] valid_loss: 5.675580739974976\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[948, 13] trainning loss: 9.365332752466202\u001b[31m\n",
      "\u001b[32m[948, 2] valid_loss: 5.675416469573975\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[949, 13] trainning loss: 9.365285784006119\u001b[31m\n",
      "\u001b[32m[949, 2] valid_loss: 5.675421476364136\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[950, 13] trainning loss: 9.365331590175629\u001b[31m\n",
      "\u001b[32m[950, 2] valid_loss: 5.675402879714966\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[951, 13] trainning loss: 9.365327954292297\u001b[31m\n",
      "\u001b[32m[951, 2] valid_loss: 5.675511837005615\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[952, 13] trainning loss: 9.365316152572632\u001b[31m\n",
      "\u001b[32m[952, 2] valid_loss: 5.67550253868103\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[953, 13] trainning loss: 9.365306466817856\u001b[31m\n",
      "\u001b[32m[953, 2] valid_loss: 5.6753926277160645\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[954, 13] trainning loss: 9.365362256765366\u001b[31m\n",
      "\u001b[32m[954, 2] valid_loss: 5.675462484359741\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[955, 13] trainning loss: 9.3652985394001\u001b[31m\n",
      "\u001b[32m[955, 2] valid_loss: 5.675485372543335\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[956, 13] trainning loss: 9.365289628505707\u001b[31m\n",
      "\u001b[32m[956, 2] valid_loss: 5.675504684448242\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[957, 13] trainning loss: 9.365332901477814\u001b[31m\n",
      "\u001b[32m[957, 2] valid_loss: 5.67538046836853\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[958, 13] trainning loss: 9.365350097417831\u001b[31m\n",
      "\u001b[32m[958, 2] valid_loss: 5.675364255905151\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[959, 13] trainning loss: 9.365301579236984\u001b[31m\n",
      "\u001b[32m[959, 2] valid_loss: 5.675470590591431\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[960, 13] trainning loss: 9.365263909101486\u001b[31m\n",
      "\u001b[32m[960, 2] valid_loss: 5.675530672073364\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[961, 13] trainning loss: 9.365314543247223\u001b[31m\n",
      "\u001b[32m[961, 2] valid_loss: 5.675470590591431\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[962, 13] trainning loss: 9.36536455154419\u001b[31m\n",
      "\u001b[32m[962, 2] valid_loss: 5.675490379333496\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[963, 13] trainning loss: 9.365344494581223\u001b[31m\n",
      "\u001b[32m[963, 2] valid_loss: 5.675424337387085\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[964, 13] trainning loss: 9.365395605564117\u001b[31m\n",
      "\u001b[32m[964, 2] valid_loss: 5.675435781478882\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[965, 13] trainning loss: 9.365360677242279\u001b[31m\n",
      "\u001b[32m[965, 2] valid_loss: 5.675468444824219\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[966, 13] trainning loss: 9.365348279476166\u001b[31m\n",
      "\u001b[32m[966, 2] valid_loss: 5.67536473274231\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[967, 13] trainning loss: 9.365381836891174\u001b[31m\n",
      "\u001b[32m[967, 2] valid_loss: 5.6754021644592285\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[968, 13] trainning loss: 9.365290582180023\u001b[31m\n",
      "\u001b[32m[968, 2] valid_loss: 5.675371408462524\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[969, 13] trainning loss: 9.365247696638107\u001b[31m\n",
      "\u001b[32m[969, 2] valid_loss: 5.675523042678833\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[970, 13] trainning loss: 9.365337818861008\u001b[31m\n",
      "\u001b[32m[970, 2] valid_loss: 5.675475835800171\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[971, 13] trainning loss: 9.365334451198578\u001b[31m\n",
      "\u001b[32m[971, 2] valid_loss: 5.675514459609985\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[972, 13] trainning loss: 9.365363746881485\u001b[31m\n",
      "\u001b[32m[972, 2] valid_loss: 5.675503492355347\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[973, 13] trainning loss: 9.365266382694244\u001b[31m\n",
      "\u001b[32m[973, 2] valid_loss: 5.67535400390625\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[974, 13] trainning loss: 9.365390241146088\u001b[31m\n",
      "\u001b[32m[974, 2] valid_loss: 5.675359725952148\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[975, 13] trainning loss: 9.365340054035187\u001b[31m\n",
      "\u001b[32m[975, 2] valid_loss: 5.675484895706177\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[976, 13] trainning loss: 9.36530402302742\u001b[31m\n",
      "\u001b[32m[976, 2] valid_loss: 5.675390720367432\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[977, 13] trainning loss: 9.36530002951622\u001b[31m\n",
      "\u001b[32m[977, 2] valid_loss: 5.67545747756958\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[978, 13] trainning loss: 9.365341454744339\u001b[31m\n",
      "\u001b[32m[978, 2] valid_loss: 5.675387144088745\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[979, 13] trainning loss: 9.365324795246124\u001b[31m\n",
      "\u001b[32m[979, 2] valid_loss: 5.675461530685425\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[980, 13] trainning loss: 9.365338265895844\u001b[31m\n",
      "\u001b[32m[980, 2] valid_loss: 5.675354242324829\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[981, 13] trainning loss: 9.365304857492447\u001b[31m\n",
      "\u001b[32m[981, 2] valid_loss: 5.675479888916016\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[982, 13] trainning loss: 9.365379393100739\u001b[31m\n",
      "\u001b[32m[982, 2] valid_loss: 5.675522327423096\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[983, 13] trainning loss: 9.365242332220078\u001b[31m\n",
      "\u001b[32m[983, 2] valid_loss: 5.67544150352478\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[984, 13] trainning loss: 9.365305006504059\u001b[31m\n",
      "\u001b[32m[984, 2] valid_loss: 5.675522565841675\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[985, 13] trainning loss: 9.365327388048172\u001b[31m\n",
      "\u001b[32m[985, 2] valid_loss: 5.675431251525879\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[986, 13] trainning loss: 9.365317165851593\u001b[31m\n",
      "\u001b[32m[986, 2] valid_loss: 5.675547122955322\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[987, 13] trainning loss: 9.365299671888351\u001b[31m\n",
      "\u001b[32m[987, 2] valid_loss: 5.675490856170654\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[988, 13] trainning loss: 9.36537104845047\u001b[31m\n",
      "\u001b[32m[988, 2] valid_loss: 5.6754279136657715\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[989, 13] trainning loss: 9.36534458398819\u001b[31m\n",
      "\u001b[32m[989, 2] valid_loss: 5.675420522689819\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[990, 13] trainning loss: 9.365311324596405\u001b[31m\n",
      "\u001b[32m[990, 2] valid_loss: 5.675519704818726\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[991, 13] trainning loss: 9.36523088812828\u001b[31m\n",
      "\u001b[32m[991, 2] valid_loss: 5.6754865646362305\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[992, 13] trainning loss: 9.365244895219803\u001b[31m\n",
      "\u001b[32m[992, 2] valid_loss: 5.67550802230835\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[993, 13] trainning loss: 9.365257024765015\u001b[31m\n",
      "\u001b[32m[993, 2] valid_loss: 5.675570487976074\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[994, 13] trainning loss: 9.365314692258835\u001b[31m\n",
      "\u001b[32m[994, 2] valid_loss: 5.675566911697388\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[995, 13] trainning loss: 9.365374356508255\u001b[31m\n",
      "\u001b[32m[995, 2] valid_loss: 5.675540208816528\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[996, 13] trainning loss: 9.365278780460358\u001b[31m\n",
      "\u001b[32m[996, 2] valid_loss: 5.675497055053711\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "\u001b[31m[997, 13] trainning loss: 9.365207433700562\u001b[31m\n",
      "\u001b[32m[997, 2] valid_loss: 5.6753623485565186\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6754).  Saving model ...\n",
      "\u001b[31m[998, 13] trainning loss: 9.365277379751205\u001b[31m\n",
      "\u001b[32m[998, 2] valid_loss: 5.675591468811035\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[999, 13] trainning loss: 9.365299493074417\u001b[31m\n",
      "\u001b[32m[999, 2] valid_loss: 5.675564527511597\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6756).  Saving model ...\n",
      "\u001b[31m[1000, 13] trainning loss: 9.365279793739319\u001b[31m\n",
      "\u001b[32m[1000, 2] valid_loss: 5.675540208816528\u001b[32m\n",
      "Validation loss decreased (inf --> 5.6755).  Saving model ...\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "summary_writer = SummaryWriter(f'./models/test')\n",
    "# valid_loss = []\n",
    "running_loss = 0\n",
    "door_for_test = 1\n",
    "rpsmnet.train()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    t_loss = 0\n",
    "    train_loss = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "#         get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data #, bp\n",
    "        # print(inputs.shape)\n",
    "        inputs, labels= inputs.to(device), labels.type(torch.LongTensor).to(device)# , bp  , bp.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = rpsmnet(inputs)\n",
    "        # print(outputs)\n",
    "        with torch.autocast('cuda'):\n",
    "        # loss = criterion(outputs, torch.tensor(labels).cuda())\n",
    "            loss = criterion(outputs, labels)\n",
    "        # l2_lambda = 0.001\n",
    "        # l2_reg = torch.tensor(0.).cuda()\n",
    "        # for param in rpsmnet.parameters():\n",
    "        #     l2_reg += torch.norm(param)\n",
    "        # loss += l2_lambda * l2_reg\n",
    "        \n",
    "        # l1_lambda = 0.001\n",
    "        # l1_norm = sum(torch.linalg.norm(p, 1) for p in rpsmnet.parameters())\n",
    "        # # l2_norm = sum(torch.linalg.norm(p, 2) for p in model.parameters())\n",
    "        # loss = loss + l1_lambda * l1_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "        # print(t_loss)\n",
    "        train_loss.append(np.mean(t_loss))\n",
    "    \n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        # if i % (math.ceil(900*Split/BATCH_SIZE)-1) == 0 and i !=0:\n",
    "        print(CRED+ f'[{epoch + 1}, {i + 1}] trainning loss: {train_loss[-1]}'+ CRED)\n",
    "    summary_writer.add_scalar('train_loss', train_loss[-1], epoch)\n",
    "    \n",
    "    if door_for_test == 1:\n",
    "        if epoch % 1 == 0:\n",
    "            # rpsmnet.eval()\n",
    "            valid_loss = []\n",
    "            va_loss = 0\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                i_list = []\n",
    "                i_list.append(i)\n",
    "                val_x, val_y= data #, bp \n",
    "                val_x, val_y = val_x.to(device), val_y.type(torch.LongTensor).to(device)#, bp.to(device) #, bp\n",
    "                Testoutput = rpsmnet(val_x)# , bp\n",
    "                # v_loss = criterion(Testoutput, val_y, torch.Tensor(Testoutput.size(0)).cuda().fill_(1.0))\n",
    "                v_loss = criterion(Testoutput, val_y) #loss\n",
    "                va_loss += v_loss.item()\n",
    "                valid_loss.append(np.mean(va_loss))\n",
    "                # if i = i_list[-1]:    # print every first\n",
    "            print(CGREEN+f'[{epoch + 1}, {i + 1}] valid_loss: {valid_loss[-1]}'+CGREEN)\n",
    "            # rpsmnet.train()\n",
    "        summary_writer.add_scalar('valid_loss', valid_loss[-1], epoch)\n",
    "    scheduler.step(valid_loss[-1])\n",
    "\n",
    "  # if epoch % 1 == 0:\n",
    "  #   GETcorrectnumber(train_loader,CYELLOW)      \n",
    "  #   GETcorrectnumber(test_loader,CBLUE)\n",
    "  # aoemnet.train()\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "    early_stopping(valid_loss[-1], rpsmnet)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 26, 30])\n"
     ]
    }
   ],
   "source": [
    "test1 = torch.rand(64, 256, 26, 30)\n",
    "test2 = torch.rand(64, 256, 26, 30)\n",
    "test3 = test1*test2\n",
    "print(test3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WubBEV-ctmdF"
   },
   "source": [
    "Confusion matrix and mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3h6xSCd_riOV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy = 0.0625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3334350/2522279504.py:24: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)\n",
      "/tmp/ipykernel_3334350/2522279504.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy = 0.11538461538461539\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "confusion_matrix_test_loader = Data.DataLoader(test, batch_size = 64, shuffle = False)\n",
    "with torch.no_grad():\n",
    "    list_mean_accuracy = []\n",
    "    for i, data in enumerate(confusion_matrix_test_loader, 0):\n",
    "        # inputs, labels, bp = data\n",
    "        # inputs, labels, bp = inputs.to(device), labels.type(torch.LongTensor).to(device), bp.to(device)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.type(torch.LongTensor).to(device)\n",
    "        # outputs = rpsmnet(inputs, bp)\n",
    "        outputs = rpsmnet(inputs)\n",
    "        optimizer.step()          \n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        # predicted = torch.max(outputs)\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "        labels = labels.numpy()\n",
    "        predicted = predicted.numpy()\n",
    "        # mean_conf_mat = confusion_matrix(labels, predicted)\n",
    "        # mean_accuracy = accuracy_score(labels[labels != 99], predicted[predicted != 99])\n",
    "        # mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)  # normalise\n",
    "        mean_conf_mat = confusion_matrix(labels, predicted)\n",
    "        mean_accuracy = accuracy_score(labels[labels != 99], predicted[predicted != 99])\n",
    "        mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1) \n",
    "        list_mean_accuracy.append(mean_accuracy)\n",
    "        print(\"Mean accuracy = {0}\".format(mean_accuracy))\n",
    "        ConfusionMatrixDisplay.from_predictions(labels, predicted)\n",
    "        # plt.savefig('/content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/notebooks/templates/save_folder/fig-{}.png'.format(session_id), dpi=600)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([7.1546, 7.4989, 7.2406, 7.4986, 7.0637, 7.2464, 6.8946, 7.2835, 6.9954,\n",
       "        7.4655, 6.9324, 7.5937, 6.9544, 7.4460, 7.1179, 7.0397, 7.2257, 6.9913,\n",
       "        7.2716, 7.4679, 7.6336, 7.2738, 6.9536, 7.2841, 7.0433, 7.3976, 7.6335,\n",
       "        7.2979, 6.8528, 7.0941, 7.5488, 7.7630, 7.2037, 7.3379, 6.7545, 6.9814,\n",
       "        7.1571, 7.2898, 7.2017, 7.1691, 7.8751, 7.2257], device='cuda:0',\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([12, 11,  5,  2,  8,  1,  2, 13,  4,  6,  5, 10,  4,  7,  5,  6,  7,  8,\n",
       "         1,  2,  0, 11,  4, 10,  5,  7,  9, 13, 13,  8, 11,  8,  7,  6, 12,  5,\n",
       "        13,  1, 13, 12,  9,  5], device='cuda:0'))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(Testoutput,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 11,  5,  2,  8,  1,  2, 13,  4,  6,  5, 10,  4,  7,  5,  6,  7,  8,\n",
       "         1,  2,  0, 11,  4, 10,  5,  7,  9, 13, 13,  8, 11,  8,  7,  6, 12,  5,\n",
       "        13,  1, 13, 12,  9,  5], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52utnK5XGf5M"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-63f5db24bbdbe3e8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-63f5db24bbdbe3e8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!rm -rf /logs/ # clear logs\n",
    "# if 'google.colab' in str(get_ipython()): # tensor board\n",
    "%load_ext tensorboard  \n",
    "# %tensorboard --logdir logs\n",
    "%tensorboard --logdir=./models/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJxb95imHxNM",
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#       # super(CNN, self)._init_()\n",
    "#       super(CNN, self).__init__()\n",
    "#       self.n_classes = 14\n",
    "#       n_classes =14\n",
    "#       self.conv1 = nn.Sequential(\n",
    "#           nn.Conv2d(\n",
    "#               in_channels=1,\n",
    "#               out_channels=32,\n",
    "#               kernel_size=3,\n",
    "#               stride=1,\n",
    "#               padding=1,\n",
    "#           ),\n",
    "#           nn.ReLU(),\n",
    "#           nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "#       )\n",
    "#       self.conv2 = nn.Sequential(\n",
    "#           nn.Conv2d(32,64,3,1,1),\n",
    "#           nn.ReLU(),\n",
    "#           nn.MaxPool2d (2,2),\n",
    "#       )\n",
    "#       # self.fc = nn.Linear(64*7*7,128)\n",
    "#       self.fc = nn.Linear(139264, 100)\n",
    "#       self.out = nn.Linear(100,n_classes)\n",
    "#       self.softmax = nn.Softmax()\n",
    "\n",
    "#     def forward(self,x):\n",
    "#       x=self.conv1(x)\n",
    "#       x=self.conv2(x)\n",
    "#       # x=x.view(x.size(0),-1)\n",
    "#       x = torch.flatten(x, 1) # flatten all dimensioxns except batch\n",
    "#       x=self.fc(x)\n",
    "#       x=self.out(x)      \n",
    "#       # output=self.out(x)\n",
    "#       # return output, x\n",
    "#       # x=self.softmax(x)\n",
    "#       return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
