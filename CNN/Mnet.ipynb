{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ysm6Y6V764k",
    "tags": []
   },
   "source": [
    "## env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-1242830d-0dff-bf8c-ac33-61a08f8c447a)\n",
      "GPU 1: NVIDIA A100-SXM4-40GB (UUID: GPU-7d81048e-a999-9a44-bacb-6cfa0327cc98)\n",
      "GPU 2: NVIDIA A100-SXM4-40GB (UUID: GPU-77e505f3-e454-9468-bf4c-a21cf5e7dac6)\n",
      "GPU 3: NVIDIA A100-SXM4-40GB (UUID: GPU-b3a0be24-9fd6-96c5-f6df-a78b70e2096a)\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "Fri Jul 29 18:18:45 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   58C    P0   223W / 400W |   1104MiB / 40960MiB |     92%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   63C    P0   231W / 400W |   1104MiB / 40960MiB |     92%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:B1:00.0 Off |                    0 |\n",
      "| N/A   60C    P0   288W / 400W |   2664MiB / 40960MiB |     75%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  On   | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3560197      C   pmemd.cuda                       1101MiB |\n",
      "|    1   N/A  N/A   3560151      C   pmemd.cuda                       1101MiB |\n",
      "|    2   N/A  N/A   3591265      C   python                           2661MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L\n",
    "!nvcc -V\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1656346799559,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "3NRIzhpaAPr8",
    "outputId": "3b03ab24-b730-404d-aecd-b7fae439a0fe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 18:18:48.673729: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"TF_ENABLE_ONEDNN_OPTS\"]=0\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import sys\n",
    "sys.path.append('Code/code/')\n",
    "from load_data import load_MEG_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda import amp\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "# %env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "from scipy.integrate import simps\n",
    "from mne.time_frequency import psd_array_welch\n",
    "from band_power import (\n",
    "    bandpower_multi_bands,\n",
    "    standard_scaling_sklearn,\n",
    ")\n",
    "import sklearn\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1656346662628,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "kVg_nRUnT75F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if  torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5CpfcYgAEeh",
    "tags": []
   },
   "source": [
    "# Mnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwmxzEwZAPr8",
    "tags": []
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5030,
     "status": "ok",
     "timestamp": 1656346804863,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "E4yCXxNLAPr9",
    "outputId": "20f25db3-ea31-4fd6-f746-e9fe956e23fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Split = 0.90\n",
    "X_train, y_train = load_MEG_dataset([str(i).zfill(3) for i in range(1,2)], mode = 'concatenate', output_format='numpy',shuffle = False, training=True, train_test_split=Split, batch_size=500)#, pca_n_components =30)\n",
    "X_test, y_test = load_MEG_dataset([str(i).zfill(3) for i in range(1,2)], mode = 'concatenate', output_format='numpy',shuffle = False, training=False, train_test_split=Split, batch_size=500)#, pca_n_components =30)\n",
    "\n",
    "X_train, X_test = (X_train-X_train.mean())/X_train.std(), (X_test-X_test.mean())/X_test.std()\n",
    "\n",
    "X_train = X_train[:, None, ...]\n",
    "X_test = X_test[:, None, ...]\n",
    "\n",
    "# X_train=np.repeat(X_train,3,axis=1)\n",
    "# X_test=np.repeat(X_test,3,axis=1)\n",
    "\n",
    "\n",
    "# X_train = scipy.interpolate.SmoothSphereBivariateSpline()\n",
    "\n",
    "y_train = (y_train / 2) - 1\n",
    "y_test = (y_test / 2) - 1\n",
    "\n",
    "X_train_tensors = torch.Tensor(X_train)\n",
    "X_test_tensors = torch.Tensor(X_test)\n",
    "y_train_tensors = torch.from_numpy(y_train) \n",
    "y_test_tensors = torch.from_numpy(y_test)\n",
    "\n",
    "# y_train_tensors = F.one_hot(y_train_tensors)\n",
    "# y_test_tensors = F.one_hot(y_test_tensors)\n",
    "# X_train_tensors = F.interpolate(X_train_tensors, size=(272, 272), mode ='bicubic')\n",
    "# X_test_tensors = F.interpolate(X_test_tensors, size=(272, 272), mode ='bicubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([810, 1, 272, 1040])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1656346806649,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "2U7T9VHSAPr-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors=X_train_tensors.cuda()\n",
    "X_test_tensors=X_test_tensors.cuda()\n",
    "y_train_tensors=y_train_tensors.cuda()\n",
    "y_test_tensors=y_test_tensors.cuda()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## band power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1656346806948,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "Qh0tZZq62f4C",
    "outputId": "175f769b-8003-40ab-8e81-a60c910fae31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 2.560 (s)\n",
      "processing bands (low, high) : (0.5,4)\n",
      "Absolute power: 0.3645 uV^2\n",
      "Relative power: 0.4691\n",
      "processing bands (low, high) : (4,8)\n",
      "Absolute power: 0.0309 uV^2\n",
      "Relative power: 0.0398\n",
      "processing bands (low, high) : (8,10)\n",
      "Absolute power: 0.0160 uV^2\n",
      "Relative power: 0.0206\n",
      "processing bands (low, high) : (10,12)\n",
      "Absolute power: 0.0105 uV^2\n",
      "Relative power: 0.0135\n",
      "processing bands (low, high) : (12,30)\n",
      "Absolute power: 0.2656 uV^2\n",
      "Relative power: 0.3418\n",
      "processing bands (low, high) : (30,70)\n",
      "Absolute power: 0.0491 uV^2\n",
      "Relative power: 0.0632\n"
     ]
    }
   ],
   "source": [
    "X_train_numpy = X_train_tensors.cpu().numpy()\n",
    "X_test_numpy = X_test_tensors.cpu().numpy()\n",
    "\n",
    "X = np.swapaxes(X_train_numpy, 2, -1).squeeze()\n",
    "data = X[X.shape[0]-1, 70, :]\n",
    "psd_mne, freqs_mne = psd_array_welch(data, 100, 1., 70., n_per_seg=None,\n",
    "                          n_overlap=0, n_jobs=1)\n",
    "for low, high in [(0.5, 4), (4, 8), (8, 10), (10, 12), (12, 30),\n",
    "                  (30, 70)]:\n",
    "    print(\"processing bands (low, high) : ({},{})\".format(low, high))\n",
    "    # Find intersecting values in frequency vector\n",
    "    idx_delta = np.logical_and(freqs_mne >= low, freqs_mne <= high)\n",
    "      # Frequency resolution\n",
    "    freq_res = freqs_mne[1] - freqs_mne[0]  # = 1 / 4 = 0.25\n",
    "\n",
    "    # Compute the absolute power by approximating the area under the curve\n",
    "    power = simps(psd_mne[idx_delta], dx=freq_res)\n",
    "    print('Absolute power: {:.4f} uV^2'.format(power))\n",
    "    \n",
    "    total_power = simps(psd_mne, dx=freq_res)\n",
    "    rel_power = power / total_power\n",
    "    \n",
    "    print('Relative power: {:.4f}'.format(rel_power))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n",
      "Effective window size : 2.560 (s)\n"
     ]
    }
   ],
   "source": [
    "X_train_bp = np.squeeze(X_train_numpy, axis=1)\n",
    "# X_train_bp = X_train_bp[: :, :, :]\n",
    "X_train_bp = standard_scaling_sklearn(X_train_bp)\n",
    "X_test_bp = np.squeeze(X_test_numpy, axis=1)\n",
    "# X_train_bp = X_train_bp[: :, :, :]\n",
    "X_test_bp = standard_scaling_sklearn(X_test_bp)\n",
    "bands = [(1, 4), (4, 8), (8, 10), (10, 13), (13, 30), (30, 70)]\n",
    "bp_train = bandpower_multi_bands(X_train_bp, fs=100.0, bands=bands, relative=True)\n",
    "bp_test = bandpower_multi_bands(X_test_bp, fs=100.0, bands=bands, relative=True)\n",
    "bp_train_tensor = torch.Tensor(bp_train).cuda()\n",
    "bp_test_tensor = torch.Tensor(bp_test).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([810, 272, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_train_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmxXd-1LGqPy",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Xcp2pMSWGqPy"
   },
   "outputs": [],
   "source": [
    "CRED    = '\\33[31m'\n",
    "CGREEN  = '\\33[32m'\n",
    "CYELLOW = '\\33[33m'\n",
    "CBLUE   = '\\33[34m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8xFKHSqAR4I",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uUckNRIGCy4I"
   },
   "outputs": [],
   "source": [
    "class ChannelPool(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat((torch.max(x, 1)[0].unsqueeze(1),\n",
    "                          torch.mean(x, 1).unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int):\n",
    "                How long to wait after last time validation loss improved.\n",
    "                Default: 7\n",
    "            verbose (bool):\n",
    "                If True, prints a message for each validation loss improvement.\n",
    "                Default: False\n",
    "            delta (float):\n",
    "                Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                Default: 0\n",
    "            path (str):\n",
    "                Path for the checkpoint to be saved to.\n",
    "                Default: 'checkpoint.pt'\n",
    "            trace_func (function):\n",
    "                trace print function.\n",
    "                Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.4f} --> {val_loss:.4f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rx7HUXktBbbL"
   },
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatialAttention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, 7, 7, padding=3), #padding = (7-1)/2\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('x',x.shape)\n",
    "        x = self.compress(x)\n",
    "        x = self.spatialAttention(x)\n",
    "        # scale = F.sigmoid(x)\n",
    "        scale = torch.sigmoid(x)\n",
    "        # print('scale',scale.shape)\n",
    "        \n",
    "        return x * scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iQVFvyxdEoRm"
   },
   "outputs": [],
   "source": [
    "class Flatten_MEG(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SgXMsUwoFPZT"
   },
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "            Implementation of a channel attention module.\n",
    "        \"\"\"\n",
    "    class Showsize(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ChannelAttention.Showsize, self).__init__()\n",
    "        def forward(self, x):\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "\n",
    "    def __init__(self, shape, reduction_factor=16):\n",
    "\n",
    "        super(ChannelAttention, self).__init__()\n",
    "\n",
    "        _, in_channel, h, w = shape\n",
    "        self.mlp = nn.Sequential(\n",
    "            # self.Showsize(),\n",
    "            Flatten_MEG(),\n",
    "            # self.Showsize(),\n",
    "            nn.Linear(in_channel, in_channel // reduction_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_channel // reduction_factor, in_channel),\n",
    "        )\n",
    "        # self.avg = nn.AvgPool2d(kernel_size=(h, w), stride=(h, w))\n",
    "        # self.max = nn.MaxPool2d(kernel_size=(h, w), stride=(h, w))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('x', x.shape)\n",
    "        # avg = self.avg(x)\n",
    "        # max = self.max(x)\n",
    "        avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "        max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "        sum = self.mlp(avg_pool) + self.mlp(max_pool)\n",
    "        # print(sum.shape)\n",
    "        scale = (\n",
    "            torch.sigmoid(sum)\n",
    "            .unsqueeze(2)\n",
    "            .unsqueeze(3)\n",
    "            .expand_as(x)\n",
    "        )\n",
    "\n",
    "        return x * scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concatenate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Concatenate, self).__init__()\n",
    "\n",
    "    def forward(self, x, bp):\n",
    "\n",
    "        # min_ = x.min(1, keepdim=True)[0]\n",
    "        # if min_[0] < 0:\n",
    "        #     x = x + min_\n",
    "        # else:\n",
    "        #     x = x - min_\n",
    "        # x = x / x.max()\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        bp = bp.view(bp.shape[0], -1)\n",
    "        x = torch.cat([x, bp], -1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GETcorrectnumber(loader, printcolor):\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        n_class_correct = [0 for i in range(num_classes)]\n",
    "        n_class_samples = [0 for i in range(num_classes)]\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = rpsmnet(inputs)\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            # n_correct += (predicted == labels).sum().item()\n",
    "            for k in range(predicted.shape[0]):\n",
    "                if predicted[k]==labels[k]:\n",
    "                    n_correct +=1\n",
    "            # for i in range(num_classes): # accuracy for each class\n",
    "            #     label = labels[i]\n",
    "            #     pred = predicted[i]\n",
    "            #     if (label == pred):\n",
    "            #         n_class_correct[i] += 1\n",
    "            #     n_class_samples[i] += 1\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(printcolor+f'[{epoch + 1}] t accuracy {acc}%'+printcolor)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1(torch.nn.Module):\n",
    "    def __init__(self, module, weight_decay):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Backward hook is registered on the specified module\n",
    "        self.hook = self.module.register_full_backward_hook(self._weight_decay_hook)\n",
    "\n",
    "    # Not dependent on backprop incoming values, placeholder\n",
    "    def _weight_decay_hook(self, *_):\n",
    "        for param in self.module.parameters():\n",
    "            # If there is no gradient or it was zeroed out\n",
    "            # Zeroed out using optimizer.zero_grad() usually\n",
    "            # Turn on if needed with grad accumulation/more safer way\n",
    "            # if param.grad is None or torch.all(param.grad == 0.0):\n",
    "\n",
    "            # Apply regularization on it\n",
    "            param.grad = self.regularize(param)\n",
    "\n",
    "    def regularize(self, parameter):\n",
    "        # L1 regularization formula\n",
    "        return self.weight_decay * torch.sign(parameter.data)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Simply forward and args and kwargs to module\n",
    "        return self.module(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJxb95imHxNM",
    "tags": []
   },
   "source": [
    "## RPS_Mnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RPS_MNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Model inspired by [Aoe at al., 10.1038/s41598-019-41500-x] integrated with bandpower.\n",
    "    \"\"\"\n",
    "    class Showsize(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(RPS_MNet.Showsize, self).__init__()\n",
    "        def forward(self, x):\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "\n",
    "    def __init__(self, n_times):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_times (int):\n",
    "                n_times dimension of the input data.\n",
    "        \"\"\"\n",
    "        super(RPS_MNet, self).__init__()\n",
    "        # if n_times == 501:  # TODO automatic n_times\n",
    "        #     self.n_times = 12\n",
    "        # elif n_times == 601:\n",
    "        #     self.n_times = 18\n",
    "        # elif n_times == 701:\n",
    "        #     self.n_times = 24\n",
    "        # else:\n",
    "        #     raise ValueError(\n",
    "        #         \"Network can work only with n_times = 501, 601, 701 \"\n",
    "        #         \"(epoch duration of 1., 1.2, 1.4 sec),\"\n",
    "        #         \" got instead {}\".format(n_times)\n",
    "        #     )\n",
    "        self.n_times = n_times\n",
    "        self.spatial = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(272,64), bias=True), #kernel size 204, 64\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, stride=(1, 1), kernel_size=(1, 16), bias=True), # kernel size 1,16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            # nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(8, 8), bias=True),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, stride=(1, 1), kernel_size=(8, 8), bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(5, 3), stride=(5, 3)),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, stride=(1, 1), kernel_size=(1, 4), bias=True),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(1, 4), bias=True), #conv6\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            nn.Conv2d(64, 128, stride=(1, 1), kernel_size=(1, 2), bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, stride=(1, 1), kernel_size=(1, 2), bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            nn.Conv2d(128, 256, stride=(1, 1), kernel_size=(1, 2), bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, stride=(1, 1), kernel_size=(1, 2), bias=True), #conv10\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "        )\n",
    "\n",
    "        self.attention1 = nn.Sequential(\n",
    "            ChannelAttention([None, 256, 26, self.n_times]),\n",
    "        )\n",
    "\n",
    "        self.attention2 = nn.Sequential(\n",
    "            SpatialAttention(),\n",
    "        )\n",
    "\n",
    "        self.concatenate = Concatenate()\n",
    "\n",
    "        self.flatten = Flatten_MEG()\n",
    "\n",
    "        self.ff1 = nn.Sequential(\n",
    "            # nn.Linear(256 * 26 * self.n_times + 272 * 6, 1024),\n",
    "            nn.Linear(4 , 1024),\n",
    "            # nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(),\n",
    "            self.Showsize(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            self.Showsize(),\n",
    "        )\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2656 , 14),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim =1)\n",
    "\n",
    "    def forward(self, x, pb = pb):\n",
    "        x = self.spatial(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.temporal(x)\n",
    "        x = self.attention1(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.ff1(self.flatten(x))\n",
    "        x = self.concatenate(x, pb)\n",
    "        x = self.ff2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM1(nn.Module):\n",
    "#     def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "#         super(LSTM1, self).__init__()\n",
    "#         self.num_classes = num_classes #number of classes\n",
    "#         self.num_layers = num_layers #number of layers\n",
    "#         self.input_size = input_size #input size\n",
    "#         self.hidden_size = hidden_size #hidden state\n",
    "#         self.seq_length = seq_length #sequence length\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "#                           num_layers=num_layers, batch_first=True) #lstm\n",
    "#         self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "#         self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "#         c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state\n",
    "#         # Propagate input through LSTM\n",
    "#         output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "#         hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "#         out = self.relu(hn)\n",
    "#         out = self.fc_1(out) #first Dense\n",
    "#         out = self.relu(out) #relu\n",
    "#         out = self.fc(out) #Final Output\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpsmnet = LSTM1(num_classes=14, input_size=280, hidden_size=10, num_layers=1, seq_length= 280).to(device) #our lstm class \n",
    "# print(rpsmnet)\n",
    "pb = torch.rand(64,272,6).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpsmnet = AlexNet().cuda()\n",
    "# print(rpsmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPS_MNet(\n",
      "  (spatial): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(272, 64), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(1, 16), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (temporal): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=(5, 3), stride=(5, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(32, 64, kernel_size=(1, 4), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(64, 64, kernel_size=(1, 4), stride=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(64, 128, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Dropout2d(p=0.3, inplace=False)\n",
      "    (13): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (14): ReLU()\n",
      "    (15): Dropout2d(p=0.3, inplace=False)\n",
      "    (16): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(128, 256, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (18): ReLU()\n",
      "    (19): Dropout2d(p=0.3, inplace=False)\n",
      "    (20): Conv2d(256, 256, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (21): ReLU()\n",
      "    (22): Dropout2d(p=0.3, inplace=False)\n",
      "    (23): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (attention1): Sequential(\n",
      "    (0): ChannelAttention(\n",
      "      (mlp): Sequential(\n",
      "        (0): Flatten_MEG()\n",
      "        (1): Linear(in_features=256, out_features=16, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=16, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attention2): Sequential(\n",
      "    (0): SpatialAttention(\n",
      "      (compress): ChannelPool()\n",
      "      (spatialAttention): Sequential(\n",
      "        (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(7, 7), padding=(3, 3))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (concatenate): Concatenate()\n",
      "  (flatten): Flatten_MEG()\n",
      "  (ff1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Showsize()\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Showsize()\n",
      "  )\n",
      "  (ff2): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=2656, out_features=14, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rpsmnet = RPS_MNet(30).cuda()\n",
    "print(rpsmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------+\n",
      "|                Modules                 | Parameters |\n",
      "+----------------------------------------+------------+\n",
      "|            spatial.0.weight            |   557056   |\n",
      "|             spatial.0.bias             |     32     |\n",
      "|            spatial.2.weight            |   32768    |\n",
      "|             spatial.2.bias             |     64     |\n",
      "|           temporal.0.weight            |    2048    |\n",
      "|            temporal.0.bias             |     32     |\n",
      "|           temporal.2.weight            |   65536    |\n",
      "|            temporal.2.bias             |     32     |\n",
      "|           temporal.5.weight            |    8192    |\n",
      "|            temporal.5.bias             |     64     |\n",
      "|           temporal.7.weight            |   16384    |\n",
      "|            temporal.7.bias             |     64     |\n",
      "|           temporal.10.weight           |   16384    |\n",
      "|            temporal.10.bias            |    128     |\n",
      "|           temporal.13.weight           |   32768    |\n",
      "|            temporal.13.bias            |    128     |\n",
      "|           temporal.17.weight           |   65536    |\n",
      "|            temporal.17.bias            |    256     |\n",
      "|           temporal.20.weight           |   131072   |\n",
      "|            temporal.20.bias            |    256     |\n",
      "|       attention1.0.mlp.1.weight        |    4096    |\n",
      "|        attention1.0.mlp.1.bias         |     16     |\n",
      "|       attention1.0.mlp.3.weight        |    4096    |\n",
      "|        attention1.0.mlp.3.bias         |    256     |\n",
      "| attention2.0.spatialAttention.0.weight |     98     |\n",
      "|  attention2.0.spatialAttention.0.bias  |     1      |\n",
      "|              ff1.0.weight              |    4096    |\n",
      "|               ff1.0.bias               |    1024    |\n",
      "|              ff1.3.weight              |  1048576   |\n",
      "|               ff1.3.bias               |    1024    |\n",
      "|              ff1.5.weight              |    1024    |\n",
      "|               ff1.5.bias               |    1024    |\n",
      "|              ff2.1.weight              |   37184    |\n",
      "|               ff2.1.bias               |     14     |\n",
      "+----------------------------------------+------------+\n",
      "Total Trainable Params: 2031329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2031329"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(rpsmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: ONNX Preprocess - Removing mutation from node aten::add_ on block input: '35'. This changes graph semantics.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_jit_pass_onnx_unpack_quantized_weights(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch::jit::Graph, arg1: Dict[str, IValue], arg2: bool) -> Dict[str, IValue]\n\nInvoked with: graph(%input.1 : Float(64, 1, 272, 800, strides=[217600, 217600, 800, 1], requires_grad=0, device=cuda:0),\n      %1 : Float(32, 1, 272, 64, strides=[17408, 17408, 64, 1], requires_grad=1, device=cuda:0),\n      %2 : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n      %3 : Float(64, 32, 1, 16, strides=[512, 16, 16, 1], requires_grad=1, device=cuda:0),\n      %4 : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n      %5 : Float(32, 1, 8, 8, strides=[64, 64, 8, 1], requires_grad=1, device=cuda:0),\n      %6 : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n      %7 : Float(32, 32, 8, 8, strides=[2048, 64, 8, 1], requires_grad=1, device=cuda:0),\n      %8 : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n      %9 : Float(64, 32, 1, 4, strides=[128, 4, 4, 1], requires_grad=1, device=cuda:0),\n      %10 : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n      %11 : Float(64, 64, 1, 4, strides=[256, 4, 4, 1], requires_grad=1, device=cuda:0),\n      %12 : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n      %13 : Float(128, 64, 1, 2, strides=[128, 2, 2, 1], requires_grad=1, device=cuda:0),\n      %14 : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %15 : Float(128, 128, 1, 2, strides=[256, 2, 2, 1], requires_grad=1, device=cuda:0),\n      %16 : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %17 : Float(256, 128, 1, 2, strides=[256, 2, 2, 1], requires_grad=1, device=cuda:0),\n      %18 : Float(256, strides=[1], requires_grad=1, device=cuda:0),\n      %19 : Float(256, 256, 1, 2, strides=[512, 2, 2, 1], requires_grad=1, device=cuda:0),\n      %20 : Float(256, strides=[1], requires_grad=1, device=cuda:0),\n      %21 : Float(16, 256, strides=[256, 1], requires_grad=1, device=cuda:0),\n      %22 : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n      %23 : Float(256, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n      %24 : Float(256, strides=[1], requires_grad=1, device=cuda:0),\n      %25 : Float(1, 2, 7, 7, strides=[98, 49, 7, 1], requires_grad=1, device=cuda:0),\n      %26 : Float(1, strides=[1], requires_grad=1, device=cuda:0),\n      %27 : Float(1024, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n      %28 : Float(1024, strides=[1], requires_grad=1, device=cuda:0),\n      %29 : Float(1024, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0),\n      %30 : Float(1024, strides=[1], requires_grad=1, device=cuda:0),\n      %31 : Float(1024, strides=[1], requires_grad=1, device=cuda:0),\n      %32 : Float(1024, strides=[1], requires_grad=1, device=cuda:0),\n      %33 : Float(1024, strides=[1], requires_grad=0, device=cuda:0),\n      %34 : Float(1024, strides=[1], requires_grad=0, device=cuda:0),\n      %35 : Long(requires_grad=0, device=cuda:0),\n      %36 : Float(14, 2656, strides=[2656, 1], requires_grad=1, device=cuda:0),\n      %37 : Float(14, strides=[1], requires_grad=1, device=cuda:0)):\n  %582 : int[] = prim::Constant[value=[1, 1]]()\n  %583 : int[] = prim::Constant[value=[0, 0]]()\n  %584 : int[] = prim::Constant[value=[1, 1]]()\n  %123 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %585 : int[] = prim::Constant[value=[0, 0]]()\n  %127 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %128 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %129 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %130 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %131 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.3 : Float(64, 32, 1, 737, strides=[23584, 737, 737, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.1, %1, %2, %582, %583, %584, %123, %585, %127, %128, %129, %130, %131) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.5 : Float(64, 32, 1, 737, strides=[23584, 737, 737, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.3) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %586 : int[] = prim::Constant[value=[1, 1]]()\n  %587 : int[] = prim::Constant[value=[0, 0]]()\n  %588 : int[] = prim::Constant[value=[1, 1]]()\n  %143 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %589 : int[] = prim::Constant[value=[0, 0]]()\n  %147 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %148 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %149 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %150 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %151 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.7 : Float(64, 64, 1, 722, strides=[46208, 722, 722, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.5, %3, %4, %586, %587, %588, %143, %589, %147, %148, %149, %150, %151) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %153 : Float(64, 64, 1, 722, strides=[46208, 722, 722, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.7) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %590 : int[] = prim::Constant[value=[1, 2]]()\n  %591 : int[] = prim::Constant[value=[1, 2]]()\n  %592 : int[] = prim::Constant[value=[0, 0]]()\n  %593 : int[] = prim::Constant[value=[1, 1]]()\n  %166 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %167 : Float(64, 64, 1, 361, strides=[23104, 361, 361, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%153, %590, %591, %592, %593, %166) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %168 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/2282272713.py:110:0\n  %169 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/2282272713.py:110:0\n  %input.9 : Float(64, 1, 64, 361, strides=[23104, 361, 361, 1], requires_grad=1, device=cuda:0) = aten::transpose(%167, %168, %169) # /tmp/ipykernel_3594456/2282272713.py:110:0\n  %594 : int[] = prim::Constant[value=[1, 1]]()\n  %595 : int[] = prim::Constant[value=[0, 0]]()\n  %596 : int[] = prim::Constant[value=[1, 1]]()\n  %180 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %597 : int[] = prim::Constant[value=[0, 0]]()\n  %184 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %185 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %186 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %187 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %188 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.11 : Float(64, 32, 57, 354, strides=[645696, 20178, 354, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.9, %5, %6, %594, %595, %596, %180, %597, %184, %185, %186, %187, %188) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.13 : Float(64, 32, 57, 354, strides=[645696, 20178, 354, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.11) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %598 : int[] = prim::Constant[value=[1, 1]]()\n  %599 : int[] = prim::Constant[value=[0, 0]]()\n  %600 : int[] = prim::Constant[value=[1, 1]]()\n  %200 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %601 : int[] = prim::Constant[value=[0, 0]]()\n  %204 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %205 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %206 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %207 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %208 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.15 : Float(64, 32, 50, 347, strides=[555200, 17350, 347, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.13, %7, %8, %598, %599, %600, %200, %601, %204, %205, %206, %207, %208) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %210 : Float(64, 32, 50, 347, strides=[555200, 17350, 347, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.15) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %602 : int[] = prim::Constant[value=[5, 3]]()\n  %603 : int[] = prim::Constant[value=[5, 3]]()\n  %604 : int[] = prim::Constant[value=[0, 0]]()\n  %605 : int[] = prim::Constant[value=[1, 1]]()\n  %223 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %input.17 : Float(64, 32, 10, 115, strides=[36800, 1150, 115, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%210, %602, %603, %604, %605, %223) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %606 : int[] = prim::Constant[value=[1, 1]]()\n  %607 : int[] = prim::Constant[value=[0, 0]]()\n  %608 : int[] = prim::Constant[value=[1, 1]]()\n  %234 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %609 : int[] = prim::Constant[value=[0, 0]]()\n  %238 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %239 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %240 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %241 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %242 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.19 : Float(64, 64, 10, 112, strides=[71680, 1120, 112, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.17, %9, %10, %606, %607, %608, %234, %609, %238, %239, %240, %241, %242) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.21 : Float(64, 64, 10, 112, strides=[71680, 1120, 112, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.19) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %610 : int[] = prim::Constant[value=[1, 1]]()\n  %611 : int[] = prim::Constant[value=[0, 0]]()\n  %612 : int[] = prim::Constant[value=[1, 1]]()\n  %254 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %613 : int[] = prim::Constant[value=[0, 0]]()\n  %258 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %259 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %260 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %261 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %262 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.23 : Float(64, 64, 10, 109, strides=[69760, 1090, 109, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.21, %11, %12, %610, %611, %612, %254, %613, %258, %259, %260, %261, %262) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %264 : Float(64, 64, 10, 109, strides=[69760, 1090, 109, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.23) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %614 : int[] = prim::Constant[value=[1, 2]]()\n  %615 : int[] = prim::Constant[value=[1, 2]]()\n  %616 : int[] = prim::Constant[value=[0, 0]]()\n  %617 : int[] = prim::Constant[value=[1, 1]]()\n  %277 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %input.25 : Float(64, 64, 10, 54, strides=[34560, 540, 54, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%264, %614, %615, %616, %617, %277) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %618 : int[] = prim::Constant[value=[1, 1]]()\n  %619 : int[] = prim::Constant[value=[0, 0]]()\n  %620 : int[] = prim::Constant[value=[1, 1]]()\n  %288 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %621 : int[] = prim::Constant[value=[0, 0]]()\n  %292 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %293 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %294 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %295 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %296 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.27 : Float(64, 128, 10, 53, strides=[67840, 530, 53, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.25, %13, %14, %618, %619, %620, %288, %621, %292, %293, %294, %295, %296) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.29 : Float(64, 128, 10, 53, strides=[67840, 530, 53, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.27) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %299 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %300 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %input.31 : Float(64, 128, 10, 53, strides=[67840, 530, 53, 1], requires_grad=1, device=cuda:0) = aten::feature_dropout(%input.29, %299, %300) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %622 : int[] = prim::Constant[value=[1, 1]]()\n  %623 : int[] = prim::Constant[value=[0, 0]]()\n  %624 : int[] = prim::Constant[value=[1, 1]]()\n  %311 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %625 : int[] = prim::Constant[value=[0, 0]]()\n  %315 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %316 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %317 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %318 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %319 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.33 : Float(64, 128, 10, 52, strides=[66560, 520, 52, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.31, %15, %16, %622, %623, %624, %311, %625, %315, %316, %317, %318, %319) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.35 : Float(64, 128, 10, 52, strides=[66560, 520, 52, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.33) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %322 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %323 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %324 : Float(64, 128, 10, 52, strides=[66560, 520, 52, 1], requires_grad=1, device=cuda:0) = aten::feature_dropout(%input.35, %322, %323) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %626 : int[] = prim::Constant[value=[1, 2]]()\n  %627 : int[] = prim::Constant[value=[1, 2]]()\n  %628 : int[] = prim::Constant[value=[0, 0]]()\n  %629 : int[] = prim::Constant[value=[1, 1]]()\n  %337 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %input.37 : Float(64, 128, 10, 26, strides=[33280, 260, 26, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%324, %626, %627, %628, %629, %337) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %630 : int[] = prim::Constant[value=[1, 1]]()\n  %631 : int[] = prim::Constant[value=[0, 0]]()\n  %632 : int[] = prim::Constant[value=[1, 1]]()\n  %348 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %633 : int[] = prim::Constant[value=[0, 0]]()\n  %352 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %353 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %354 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %355 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %356 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.39 : Float(64, 256, 10, 25, strides=[64000, 250, 25, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.37, %17, %18, %630, %631, %632, %348, %633, %352, %353, %354, %355, %356) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.41 : Float(64, 256, 10, 25, strides=[64000, 250, 25, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.39) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %359 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %360 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %input.43 : Float(64, 256, 10, 25, strides=[64000, 250, 25, 1], requires_grad=1, device=cuda:0) = aten::feature_dropout(%input.41, %359, %360) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %634 : int[] = prim::Constant[value=[1, 1]]()\n  %635 : int[] = prim::Constant[value=[0, 0]]()\n  %636 : int[] = prim::Constant[value=[1, 1]]()\n  %371 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %637 : int[] = prim::Constant[value=[0, 0]]()\n  %375 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %376 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %377 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %378 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %379 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.45 : Float(64, 256, 10, 24, strides=[61440, 240, 24, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.43, %19, %20, %634, %635, %636, %371, %637, %375, %376, %377, %378, %379) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.47 : Float(64, 256, 10, 24, strides=[61440, 240, 24, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.45) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %382 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %383 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %384 : Float(64, 256, 10, 24, strides=[61440, 240, 24, 1], requires_grad=1, device=cuda:0) = aten::feature_dropout(%input.47, %382, %383) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %638 : int[] = prim::Constant[value=[1, 2]]()\n  %639 : int[] = prim::Constant[value=[1, 2]]()\n  %640 : int[] = prim::Constant[value=[0, 0]]()\n  %641 : int[] = prim::Constant[value=[1, 1]]()\n  %397 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %398 : Float(64, 256, 10, 12, strides=[30720, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%384, %638, %639, %640, %641, %397) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %399 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %400 : int = aten::size(%398, %399) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %402 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %403 : int = aten::size(%398, %402) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %405 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %406 : int = aten::size(%398, %405) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %408 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %409 : int = aten::size(%398, %408) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %415 : int[] = prim::ListConstruct(%400, %403)\n  %416 : int[] = prim::ListConstruct(%406, %409)\n  %642 : int[] = prim::Constant[value=[0, 0]]()\n  %420 : bool = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %421 : bool = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %422 : NoneType = prim::Constant()\n  %423 : Float(64, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::avg_pool2d(%398, %415, %416, %642, %420, %421, %422) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %424 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %425 : int = aten::size(%398, %424) # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %427 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %428 : int = aten::size(%398, %427) # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %430 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %431 : int = aten::size(%398, %430) # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %433 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %434 : int = aten::size(%398, %433) # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %440 : int[] = prim::ListConstruct(%425, %428)\n  %441 : int[] = prim::ListConstruct(%431, %434)\n  %643 : int[] = prim::Constant[value=[0, 0]]()\n  %644 : int[] = prim::Constant[value=[1, 1]]()\n  %448 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %449 : Float(64, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%398, %440, %441, %643, %644, %448) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %450 : int = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %451 : int = aten::size(%423, %450) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %454 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %455 : int[] = prim::ListConstruct(%451, %454)\n  %456 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::view(%423, %455) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %input.49 : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = aten::linear(%456, %21, %22) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %458 : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.49) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %459 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::linear(%458, %23, %24) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %460 : int = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %461 : int = aten::size(%449, %460) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %464 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %465 : int[] = prim::ListConstruct(%461, %464)\n  %466 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::view(%449, %465) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %input.51 : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = aten::linear(%466, %21, %22) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %468 : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.51) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %469 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::linear(%468, %23, %24) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %470 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1386226351.py:34:0\n  %471 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::add(%459, %469, %470) # /tmp/ipykernel_3594456/1386226351.py:34:0\n  %472 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::sigmoid(%471) # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %473 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %474 : Float(64, 256, 1, strides=[256, 1, 1], requires_grad=1, device=cuda:0) = aten::unsqueeze(%472, %473) # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %475 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %476 : Float(64, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::unsqueeze(%474, %475) # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %477 : Float(64, 256, 10, 12, strides=[256, 1, 0, 0], requires_grad=1, device=cuda:0) = aten::expand_as(%476, %398) # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %478 : Float(64, 256, 10, 12, strides=[30720, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::mul(%398, %477) # /tmp/ipykernel_3594456/1386226351.py:43:0\n  %479 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %480 : bool = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %481 : Float(64, 10, 12, strides=[120, 12, 1], requires_grad=1, device=cuda:0), %482 : Long(64, 10, 12, strides=[120, 12, 1], requires_grad=0, device=cuda:0) = aten::max(%478, %479, %480) # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %483 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %484 : Float(64, 1, 10, 12, strides=[120, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::unsqueeze(%481, %483) # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %645 : int[] = prim::Constant[value=[1]]()\n  %487 : bool = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1795744367.py:5:0\n  %488 : NoneType = prim::Constant()\n  %489 : Float(64, 10, 12, strides=[120, 12, 1], requires_grad=1, device=cuda:0) = aten::mean(%478, %645, %487, %488) # /tmp/ipykernel_3594456/1795744367.py:5:0\n  %490 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1795744367.py:5:0\n  %491 : Float(64, 1, 10, 12, strides=[120, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::unsqueeze(%489, %490) # /tmp/ipykernel_3594456/1795744367.py:5:0\n  %492 : Tensor[] = prim::ListConstruct(%484, %491)\n  %493 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %input.53 : Float(64, 2, 10, 12, strides=[240, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::cat(%492, %493) # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %646 : int[] = prim::Constant[value=[7, 7]]()\n  %647 : int[] = prim::Constant[value=[3, 3]]()\n  %648 : int[] = prim::Constant[value=[1, 1]]()\n  %504 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %649 : int[] = prim::Constant[value=[0, 0]]()\n  %508 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %509 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %510 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %511 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %512 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %513 : Float(64, 1, 2, 2, strides=[4, 4, 2, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.53, %25, %26, %646, %647, %648, %504, %649, %508, %509, %510, %511, %512) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %514 : Float(64, 1, 2, 2, strides=[4, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::sigmoid(%513) # /tmp/ipykernel_3594456/1940439731.py:14:0\n  %515 : Float(64, 1, 2, 2, strides=[4, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::mul(%513, %514) # /tmp/ipykernel_3594456/1940439731.py:17:0\n  %516 : int = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %517 : int = aten::size(%515, %516) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %520 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %521 : int[] = prim::ListConstruct(%517, %520)\n  %522 : Float(64, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = aten::view(%515, %521) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %input.55 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::linear(%522, %27, %28) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %524 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.55) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %input.57 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::linear(%524, %29, %30) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %input.59 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.57) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %538 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %539 : float = prim::Constant[value=0.10000000000000001]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %540 : float = prim::Constant[value=1.0000000000000001e-05]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %541 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %input.61 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.59, %31, %32, %33, %34, %538, %539, %540, %541) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %543 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %544 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %545 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::dropout(%input.61, %543, %544) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %546 : int = prim::Constant[value=0]() # /tmp/ipykernel_3594456/4168394826.py:13:0\n  %547 : int = aten::size(%545, %546) # /tmp/ipykernel_3594456/4168394826.py:13:0\n  %553 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/4168394826.py:13:0\n  %554 : int[] = prim::ListConstruct(%547, %553)\n  %555 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::view(%545, %554) # /tmp/ipykernel_3594456/4168394826.py:13:0\n  %654 : Float(64, 1632, strides=[1632, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()\n  %570 : Tensor[] = prim::ListConstruct(%555, %654)\n  %571 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/4168394826.py:15:0\n  %input.63 : Float(64, 2656, strides=[2656, 1], requires_grad=1, device=cuda:0) = aten::cat(%570, %571) # /tmp/ipykernel_3594456/4168394826.py:15:0\n  %573 : float = prim::Constant[value=0.5]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %574 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %575 : Float(64, 2656, strides=[2656, 1], requires_grad=1, device=cuda:0) = aten::dropout(%input.63, %573, %574) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %input : Float(64, 14, strides=[14, 1], requires_grad=1, device=cuda:0) = aten::linear(%575, %36, %37) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %577 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1834:0\n  %578 : NoneType = prim::Constant()\n  %579 : Float(64, 14, strides=[14, 1], requires_grad=1, device=cuda:0) = aten::softmax(%input, %577, %578) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1834:0\n  %580 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/2282272713.py:119:0\n  %581 : Float(64, 14, strides=[14, 1], requires_grad=1, device=cuda:0) = aten::squeeze(%579, %580) # /tmp/ipykernel_3594456/2282272713.py:119:0\n  return (%581)\n, None, False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhiddenlayer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhl\u001b[39;00m\n\u001b[1;32m      5\u001b[0m transforms \u001b[38;5;241m=\u001b[39m [ hl\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mPrune(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConstant\u001b[39m\u001b[38;5;124m'\u001b[39m) ] \u001b[38;5;66;03m# Removes Constant nodes from graph.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mhl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpsmnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m272\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m graph\u001b[38;5;241m.\u001b[39mtheme \u001b[38;5;241m=\u001b[39m hl\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mTHEMES[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      9\u001b[0m graph\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn_hiddenlayer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/hiddenlayer/graph.py:143\u001b[0m, in \u001b[0;36mbuild_graph\u001b[0;34m(model, args, input_names, transforms, framework_transforms)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_graph, FRAMEWORK_TRANSFORMS\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument args must be provided for Pytorch models.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 143\u001b[0m     \u001b[43mimport_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_graph, FRAMEWORK_TRANSFORMS\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/hiddenlayer/pytorch_builder.py:71\u001b[0m, in \u001b[0;36mimport_graph\u001b[0;34m(hl_graph, model, args, input_names, verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimport_graph\u001b[39m(hl_graph, model, args, input_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# TODO: add input names to graph\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Run the Pytorch graph to get a trace and generate a graph from it\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     trace, out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_get_trace_graph(model, args)\n\u001b[0;32m---> 71\u001b[0m     torch_graph \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOperatorExportTypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mONNX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Dump list of nodes (DEBUG only)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/onnx/__init__.py:394\u001b[0m, in \u001b[0;36m_optimize_trace\u001b[0;34m(graph, operator_export_type)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_optimize_trace\u001b[39m(graph, operator_export_type):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/onnx/utils.py:277\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    275\u001b[0m symbolic_helper\u001b[38;5;241m.\u001b[39m_quantized_ops\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Unpack quantized weights for conv and linear ops and insert into graph.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx_unpack_quantized_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbolic_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_caffe2_aten_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m symbolic_helper\u001b[38;5;241m.\u001b[39mis_caffe2_aten_fallback():\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# Insert permutes before and after each conv op to ensure correct order.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_quantization_insert_permutes(graph, params_dict)\n",
      "\u001b[0;31mTypeError\u001b[0m: _jit_pass_onnx_unpack_quantized_weights(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch::jit::Graph, arg1: Dict[str, IValue], arg2: bool) -> Dict[str, IValue]\n\nInvoked with: graph(%input.1 : Float(64, 1, 272, 800, strides=[217600, 217600, 800, 1], requires_grad=0, device=cuda:0),\n      %1 : Float(32, 1, 272, 64, strides=[17408, 17408, 64, 1], requires_grad=1, device=cuda:0),\n      %2 : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n      %3 : Float(64, 32, 1, 16, strides=[512, 16, 16, 1], requires_grad=1, device=cuda:0),\n      %4 : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n      %5 : Float(32, 1, 8, 8, strides=[64, 64, 8, 1], requires_grad=1, device=cuda:0),\n      %6 : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n      %7 : Float(32, 32, 8, 8, strides=[2048, 64, 8, 1], requires_grad=1, device=cuda:0),\n      %8 : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n      %9 : Float(64, 32, 1, 4, strides=[128, 4, 4, 1], requires_grad=1, device=cuda:0),\n      %10 : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n      %11 : Float(64, 64, 1, 4, strides=[256, 4, 4, 1], requires_grad=1, device=cuda:0),\n      %12 : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n      %13 : Float(128, 64, 1, 2, strides=[128, 2, 2, 1], requires_grad=1, device=cuda:0),\n      %14 : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %15 : Float(128, 128, 1, 2, strides=[256, 2, 2, 1], requires_grad=1, device=cuda:0),\n      %16 : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %17 : Float(256, 128, 1, 2, strides=[256, 2, 2, 1], requires_grad=1, device=cuda:0),\n      %18 : Float(256, strides=[1], requires_grad=1, device=cuda:0),\n      %19 : Float(256, 256, 1, 2, strides=[512, 2, 2, 1], requires_grad=1, device=cuda:0),\n      %20 : Float(256, strides=[1], requires_grad=1, device=cuda:0),\n      %21 : Float(16, 256, strides=[256, 1], requires_grad=1, device=cuda:0),\n      %22 : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n      %23 : Float(256, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n      %24 : Float(256, strides=[1], requires_grad=1, device=cuda:0),\n      %25 : Float(1, 2, 7, 7, strides=[98, 49, 7, 1], requires_grad=1, device=cuda:0),\n      %26 : Float(1, strides=[1], requires_grad=1, device=cuda:0),\n      %27 : Float(1024, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n      %28 : Float(1024, strides=[1], requires_grad=1, device=cuda:0),\n      %29 : Float(1024, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0),\n      %30 : Float(1024, strides=[1], requires_grad=1, device=cuda:0),\n      %31 : Float(1024, strides=[1], requires_grad=1, device=cuda:0),\n      %32 : Float(1024, strides=[1], requires_grad=1, device=cuda:0),\n      %33 : Float(1024, strides=[1], requires_grad=0, device=cuda:0),\n      %34 : Float(1024, strides=[1], requires_grad=0, device=cuda:0),\n      %35 : Long(requires_grad=0, device=cuda:0),\n      %36 : Float(14, 2656, strides=[2656, 1], requires_grad=1, device=cuda:0),\n      %37 : Float(14, strides=[1], requires_grad=1, device=cuda:0)):\n  %582 : int[] = prim::Constant[value=[1, 1]]()\n  %583 : int[] = prim::Constant[value=[0, 0]]()\n  %584 : int[] = prim::Constant[value=[1, 1]]()\n  %123 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %585 : int[] = prim::Constant[value=[0, 0]]()\n  %127 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %128 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %129 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %130 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %131 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.3 : Float(64, 32, 1, 737, strides=[23584, 737, 737, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.1, %1, %2, %582, %583, %584, %123, %585, %127, %128, %129, %130, %131) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.5 : Float(64, 32, 1, 737, strides=[23584, 737, 737, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.3) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %586 : int[] = prim::Constant[value=[1, 1]]()\n  %587 : int[] = prim::Constant[value=[0, 0]]()\n  %588 : int[] = prim::Constant[value=[1, 1]]()\n  %143 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %589 : int[] = prim::Constant[value=[0, 0]]()\n  %147 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %148 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %149 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %150 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %151 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.7 : Float(64, 64, 1, 722, strides=[46208, 722, 722, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.5, %3, %4, %586, %587, %588, %143, %589, %147, %148, %149, %150, %151) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %153 : Float(64, 64, 1, 722, strides=[46208, 722, 722, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.7) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %590 : int[] = prim::Constant[value=[1, 2]]()\n  %591 : int[] = prim::Constant[value=[1, 2]]()\n  %592 : int[] = prim::Constant[value=[0, 0]]()\n  %593 : int[] = prim::Constant[value=[1, 1]]()\n  %166 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %167 : Float(64, 64, 1, 361, strides=[23104, 361, 361, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%153, %590, %591, %592, %593, %166) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %168 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/2282272713.py:110:0\n  %169 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/2282272713.py:110:0\n  %input.9 : Float(64, 1, 64, 361, strides=[23104, 361, 361, 1], requires_grad=1, device=cuda:0) = aten::transpose(%167, %168, %169) # /tmp/ipykernel_3594456/2282272713.py:110:0\n  %594 : int[] = prim::Constant[value=[1, 1]]()\n  %595 : int[] = prim::Constant[value=[0, 0]]()\n  %596 : int[] = prim::Constant[value=[1, 1]]()\n  %180 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %597 : int[] = prim::Constant[value=[0, 0]]()\n  %184 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %185 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %186 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %187 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %188 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.11 : Float(64, 32, 57, 354, strides=[645696, 20178, 354, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.9, %5, %6, %594, %595, %596, %180, %597, %184, %185, %186, %187, %188) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.13 : Float(64, 32, 57, 354, strides=[645696, 20178, 354, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.11) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %598 : int[] = prim::Constant[value=[1, 1]]()\n  %599 : int[] = prim::Constant[value=[0, 0]]()\n  %600 : int[] = prim::Constant[value=[1, 1]]()\n  %200 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %601 : int[] = prim::Constant[value=[0, 0]]()\n  %204 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %205 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %206 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %207 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %208 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.15 : Float(64, 32, 50, 347, strides=[555200, 17350, 347, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.13, %7, %8, %598, %599, %600, %200, %601, %204, %205, %206, %207, %208) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %210 : Float(64, 32, 50, 347, strides=[555200, 17350, 347, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.15) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %602 : int[] = prim::Constant[value=[5, 3]]()\n  %603 : int[] = prim::Constant[value=[5, 3]]()\n  %604 : int[] = prim::Constant[value=[0, 0]]()\n  %605 : int[] = prim::Constant[value=[1, 1]]()\n  %223 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %input.17 : Float(64, 32, 10, 115, strides=[36800, 1150, 115, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%210, %602, %603, %604, %605, %223) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %606 : int[] = prim::Constant[value=[1, 1]]()\n  %607 : int[] = prim::Constant[value=[0, 0]]()\n  %608 : int[] = prim::Constant[value=[1, 1]]()\n  %234 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %609 : int[] = prim::Constant[value=[0, 0]]()\n  %238 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %239 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %240 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %241 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %242 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.19 : Float(64, 64, 10, 112, strides=[71680, 1120, 112, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.17, %9, %10, %606, %607, %608, %234, %609, %238, %239, %240, %241, %242) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.21 : Float(64, 64, 10, 112, strides=[71680, 1120, 112, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.19) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %610 : int[] = prim::Constant[value=[1, 1]]()\n  %611 : int[] = prim::Constant[value=[0, 0]]()\n  %612 : int[] = prim::Constant[value=[1, 1]]()\n  %254 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %613 : int[] = prim::Constant[value=[0, 0]]()\n  %258 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %259 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %260 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %261 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %262 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.23 : Float(64, 64, 10, 109, strides=[69760, 1090, 109, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.21, %11, %12, %610, %611, %612, %254, %613, %258, %259, %260, %261, %262) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %264 : Float(64, 64, 10, 109, strides=[69760, 1090, 109, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.23) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %614 : int[] = prim::Constant[value=[1, 2]]()\n  %615 : int[] = prim::Constant[value=[1, 2]]()\n  %616 : int[] = prim::Constant[value=[0, 0]]()\n  %617 : int[] = prim::Constant[value=[1, 1]]()\n  %277 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %input.25 : Float(64, 64, 10, 54, strides=[34560, 540, 54, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%264, %614, %615, %616, %617, %277) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %618 : int[] = prim::Constant[value=[1, 1]]()\n  %619 : int[] = prim::Constant[value=[0, 0]]()\n  %620 : int[] = prim::Constant[value=[1, 1]]()\n  %288 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %621 : int[] = prim::Constant[value=[0, 0]]()\n  %292 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %293 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %294 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %295 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %296 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.27 : Float(64, 128, 10, 53, strides=[67840, 530, 53, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.25, %13, %14, %618, %619, %620, %288, %621, %292, %293, %294, %295, %296) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.29 : Float(64, 128, 10, 53, strides=[67840, 530, 53, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.27) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %299 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %300 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %input.31 : Float(64, 128, 10, 53, strides=[67840, 530, 53, 1], requires_grad=1, device=cuda:0) = aten::feature_dropout(%input.29, %299, %300) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %622 : int[] = prim::Constant[value=[1, 1]]()\n  %623 : int[] = prim::Constant[value=[0, 0]]()\n  %624 : int[] = prim::Constant[value=[1, 1]]()\n  %311 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %625 : int[] = prim::Constant[value=[0, 0]]()\n  %315 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %316 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %317 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %318 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %319 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.33 : Float(64, 128, 10, 52, strides=[66560, 520, 52, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.31, %15, %16, %622, %623, %624, %311, %625, %315, %316, %317, %318, %319) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.35 : Float(64, 128, 10, 52, strides=[66560, 520, 52, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.33) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %322 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %323 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %324 : Float(64, 128, 10, 52, strides=[66560, 520, 52, 1], requires_grad=1, device=cuda:0) = aten::feature_dropout(%input.35, %322, %323) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %626 : int[] = prim::Constant[value=[1, 2]]()\n  %627 : int[] = prim::Constant[value=[1, 2]]()\n  %628 : int[] = prim::Constant[value=[0, 0]]()\n  %629 : int[] = prim::Constant[value=[1, 1]]()\n  %337 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %input.37 : Float(64, 128, 10, 26, strides=[33280, 260, 26, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%324, %626, %627, %628, %629, %337) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %630 : int[] = prim::Constant[value=[1, 1]]()\n  %631 : int[] = prim::Constant[value=[0, 0]]()\n  %632 : int[] = prim::Constant[value=[1, 1]]()\n  %348 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %633 : int[] = prim::Constant[value=[0, 0]]()\n  %352 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %353 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %354 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %355 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %356 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.39 : Float(64, 256, 10, 25, strides=[64000, 250, 25, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.37, %17, %18, %630, %631, %632, %348, %633, %352, %353, %354, %355, %356) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.41 : Float(64, 256, 10, 25, strides=[64000, 250, 25, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.39) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %359 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %360 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %input.43 : Float(64, 256, 10, 25, strides=[64000, 250, 25, 1], requires_grad=1, device=cuda:0) = aten::feature_dropout(%input.41, %359, %360) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %634 : int[] = prim::Constant[value=[1, 1]]()\n  %635 : int[] = prim::Constant[value=[0, 0]]()\n  %636 : int[] = prim::Constant[value=[1, 1]]()\n  %371 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %637 : int[] = prim::Constant[value=[0, 0]]()\n  %375 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %376 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %377 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %378 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %379 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.45 : Float(64, 256, 10, 24, strides=[61440, 240, 24, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.43, %19, %20, %634, %635, %636, %371, %637, %375, %376, %377, %378, %379) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %input.47 : Float(64, 256, 10, 24, strides=[61440, 240, 24, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.45) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %382 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %383 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %384 : Float(64, 256, 10, 24, strides=[61440, 240, 24, 1], requires_grad=1, device=cuda:0) = aten::feature_dropout(%input.47, %382, %383) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1344:0\n  %638 : int[] = prim::Constant[value=[1, 2]]()\n  %639 : int[] = prim::Constant[value=[1, 2]]()\n  %640 : int[] = prim::Constant[value=[0, 0]]()\n  %641 : int[] = prim::Constant[value=[1, 1]]()\n  %397 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %398 : Float(64, 256, 10, 12, strides=[30720, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%384, %638, %639, %640, %641, %397) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %399 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %400 : int = aten::size(%398, %399) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %402 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %403 : int = aten::size(%398, %402) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %405 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %406 : int = aten::size(%398, %405) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %408 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %409 : int = aten::size(%398, %408) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %415 : int[] = prim::ListConstruct(%400, %403)\n  %416 : int[] = prim::ListConstruct(%406, %409)\n  %642 : int[] = prim::Constant[value=[0, 0]]()\n  %420 : bool = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %421 : bool = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %422 : NoneType = prim::Constant()\n  %423 : Float(64, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::avg_pool2d(%398, %415, %416, %642, %420, %421, %422) # /tmp/ipykernel_3594456/1386226351.py:32:0\n  %424 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %425 : int = aten::size(%398, %424) # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %427 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %428 : int = aten::size(%398, %427) # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %430 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %431 : int = aten::size(%398, %430) # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %433 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %434 : int = aten::size(%398, %433) # /tmp/ipykernel_3594456/1386226351.py:33:0\n  %440 : int[] = prim::ListConstruct(%425, %428)\n  %441 : int[] = prim::ListConstruct(%431, %434)\n  %643 : int[] = prim::Constant[value=[0, 0]]()\n  %644 : int[] = prim::Constant[value=[1, 1]]()\n  %448 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %449 : Float(64, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%398, %440, %441, %643, %644, %448) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %450 : int = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %451 : int = aten::size(%423, %450) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %454 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %455 : int[] = prim::ListConstruct(%451, %454)\n  %456 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::view(%423, %455) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %input.49 : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = aten::linear(%456, %21, %22) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %458 : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.49) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %459 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::linear(%458, %23, %24) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %460 : int = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %461 : int = aten::size(%449, %460) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %464 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %465 : int[] = prim::ListConstruct(%461, %464)\n  %466 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::view(%449, %465) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %input.51 : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = aten::linear(%466, %21, %22) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %468 : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.51) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %469 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::linear(%468, %23, %24) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %470 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1386226351.py:34:0\n  %471 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::add(%459, %469, %470) # /tmp/ipykernel_3594456/1386226351.py:34:0\n  %472 : Float(64, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = aten::sigmoid(%471) # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %473 : int = prim::Constant[value=2]() # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %474 : Float(64, 256, 1, strides=[256, 1, 1], requires_grad=1, device=cuda:0) = aten::unsqueeze(%472, %473) # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %475 : int = prim::Constant[value=3]() # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %476 : Float(64, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::unsqueeze(%474, %475) # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %477 : Float(64, 256, 10, 12, strides=[256, 1, 0, 0], requires_grad=1, device=cuda:0) = aten::expand_as(%476, %398) # /tmp/ipykernel_3594456/1386226351.py:37:0\n  %478 : Float(64, 256, 10, 12, strides=[30720, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::mul(%398, %477) # /tmp/ipykernel_3594456/1386226351.py:43:0\n  %479 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %480 : bool = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %481 : Float(64, 10, 12, strides=[120, 12, 1], requires_grad=1, device=cuda:0), %482 : Long(64, 10, 12, strides=[120, 12, 1], requires_grad=0, device=cuda:0) = aten::max(%478, %479, %480) # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %483 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %484 : Float(64, 1, 10, 12, strides=[120, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::unsqueeze(%481, %483) # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %645 : int[] = prim::Constant[value=[1]]()\n  %487 : bool = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1795744367.py:5:0\n  %488 : NoneType = prim::Constant()\n  %489 : Float(64, 10, 12, strides=[120, 12, 1], requires_grad=1, device=cuda:0) = aten::mean(%478, %645, %487, %488) # /tmp/ipykernel_3594456/1795744367.py:5:0\n  %490 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1795744367.py:5:0\n  %491 : Float(64, 1, 10, 12, strides=[120, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::unsqueeze(%489, %490) # /tmp/ipykernel_3594456/1795744367.py:5:0\n  %492 : Tensor[] = prim::ListConstruct(%484, %491)\n  %493 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %input.53 : Float(64, 2, 10, 12, strides=[240, 120, 12, 1], requires_grad=1, device=cuda:0) = aten::cat(%492, %493) # /tmp/ipykernel_3594456/1795744367.py:4:0\n  %646 : int[] = prim::Constant[value=[7, 7]]()\n  %647 : int[] = prim::Constant[value=[3, 3]]()\n  %648 : int[] = prim::Constant[value=[1, 1]]()\n  %504 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %649 : int[] = prim::Constant[value=[0, 0]]()\n  %508 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %509 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %510 : bool = prim::Constant[value=0]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %511 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %512 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %513 : Float(64, 1, 2, 2, strides=[4, 4, 2, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.53, %25, %26, %646, %647, %648, %504, %649, %508, %509, %510, %511, %512) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n  %514 : Float(64, 1, 2, 2, strides=[4, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::sigmoid(%513) # /tmp/ipykernel_3594456/1940439731.py:14:0\n  %515 : Float(64, 1, 2, 2, strides=[4, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::mul(%513, %514) # /tmp/ipykernel_3594456/1940439731.py:17:0\n  %516 : int = prim::Constant[value=0]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %517 : int = aten::size(%515, %516) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %520 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %521 : int[] = prim::ListConstruct(%517, %520)\n  %522 : Float(64, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = aten::view(%515, %521) # /tmp/ipykernel_3594456/1594679781.py:3:0\n  %input.55 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::linear(%522, %27, %28) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %524 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.55) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %input.57 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::linear(%524, %29, %30) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %input.59 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.57) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %538 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %539 : float = prim::Constant[value=0.10000000000000001]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %540 : float = prim::Constant[value=1.0000000000000001e-05]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %541 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %input.61 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.59, %31, %32, %33, %34, %538, %539, %540, %541) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:2438:0\n  %543 : float = prim::Constant[value=0.29999999999999999]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %544 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %545 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::dropout(%input.61, %543, %544) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %546 : int = prim::Constant[value=0]() # /tmp/ipykernel_3594456/4168394826.py:13:0\n  %547 : int = aten::size(%545, %546) # /tmp/ipykernel_3594456/4168394826.py:13:0\n  %553 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/4168394826.py:13:0\n  %554 : int[] = prim::ListConstruct(%547, %553)\n  %555 : Float(64, 1024, strides=[1024, 1], requires_grad=1, device=cuda:0) = aten::view(%545, %554) # /tmp/ipykernel_3594456/4168394826.py:13:0\n  %654 : Float(64, 1632, strides=[1632, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()\n  %570 : Tensor[] = prim::ListConstruct(%555, %654)\n  %571 : int = prim::Constant[value=-1]() # /tmp/ipykernel_3594456/4168394826.py:15:0\n  %input.63 : Float(64, 2656, strides=[2656, 1], requires_grad=1, device=cuda:0) = aten::cat(%570, %571) # /tmp/ipykernel_3594456/4168394826.py:15:0\n  %573 : float = prim::Constant[value=0.5]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %574 : bool = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %575 : Float(64, 2656, strides=[2656, 1], requires_grad=1, device=cuda:0) = aten::dropout(%input.63, %573, %574) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1252:0\n  %input : Float(64, 14, strides=[14, 1], requires_grad=1, device=cuda:0) = aten::linear(%575, %36, %37) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %577 : int = prim::Constant[value=1]() # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1834:0\n  %578 : NoneType = prim::Constant()\n  %579 : Float(64, 14, strides=[14, 1], requires_grad=1, device=cuda:0) = aten::softmax(%input, %577, %578) # /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages/torch/nn/functional.py:1834:0\n  %580 : int = prim::Constant[value=1]() # /tmp/ipykernel_3594456/2282272713.py:119:0\n  %581 : Float(64, 14, strides=[14, 1], requires_grad=1, device=cuda:0) = aten::squeeze(%579, %580) # /tmp/ipykernel_3594456/2282272713.py:119:0\n  return (%581)\n, None, False"
     ]
    }
   ],
   "source": [
    "# # !pip uninstall torchviz\n",
    "# sudo apt-get install graphviz\n",
    "import hiddenlayer as hl\n",
    "\n",
    "transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n",
    "\n",
    "graph = hl.build_graph(rpsmnet, torch.zeros(64,1,272,800).cuda())\n",
    "graph.theme = hl.graph.THEMES['blue'].copy()\n",
    "graph.save('rnn_hiddenlayer', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'graphviz.backend' has no attribute 'ENCODING'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m rpsmnet \u001b[38;5;241m=\u001b[39m RPS_MNet(\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m272\u001b[39m, \u001b[38;5;241m800\u001b[39m)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m rpsmnet(x)    \u001b[38;5;66;03m# \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/torchviz/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot, make_dot_from_trace\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/torchviz/dot.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/graphviz/__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# graphviz - create dot, save, render, view\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Assemble DOT source code and render it with Graphviz.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m>>> dot = Digraph(comment='The Round Table')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m}\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Graph, Digraph\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m escape, nohtml\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/graphviz/dot.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Assemble DOT source code objects.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m>>> dot = Graph(comment=u'M\\xf8nti Pyth\\xf8n ik den H\\xf8lie Grailen')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m'test-output/m00se.gv.pdf'\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lang\n\u001b[1;32m     35\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDigraph\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/graphviz/files.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBase\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m     24\u001b[0m     _engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m     _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/graphviz/files.py:28\u001b[0m, in \u001b[0;36mBase\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m _engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m _encoding \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mENCODING\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mengine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124;03m\"\"\"The layout commmand used for rendering (``'dot'``, ``'neato'``, ...).\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'graphviz.backend' has no attribute 'ENCODING'"
     ]
    }
   ],
   "source": [
    "rpsmnet = RPS_MNet(30).cuda()\n",
    "\n",
    "from torchviz import make_dot\n",
    "x = torch.randn(64, 1, 272, 800).requires_grad_(True).cuda() # \n",
    "y = rpsmnet(x)    # \n",
    "# y = y.cuda()\n",
    "MyConvNetVis = make_dot(y)#, params=dict(list(rpsmnet.named_parameters()) + [('x', x)]))\n",
    "MyConvNetVis.format = \"png\"\n",
    "# \n",
    "MyConvNetVis.directory = \"data\"\n",
    "# \n",
    "MyConvNetVis.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz==0.0.2 in /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages (from torchviz==0.0.2) (1.12.0)\n",
      "Requirement already satisfied: graphviz in /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages (from torchviz==0.0.2) (0.20.1)\n",
      "Requirement already satisfied: typing_extensions in /users/k21116947/miniconda3/envs/Mnet/lib/python3.9/site-packages (from torch->torchviz==0.0.2) (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz==0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'graphviz.backend' has no attribute 'ENCODING'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[1;32m      2\u001b[0m x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m10\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m y\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/torchviz/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot, make_dot_from_trace\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/torchviz/dot.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/graphviz/__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# graphviz - create dot, save, render, view\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Assemble DOT source code and render it with Graphviz.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m>>> dot = Digraph(comment='The Round Table')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m}\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Graph, Digraph\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m escape, nohtml\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/graphviz/dot.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Assemble DOT source code objects.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m>>> dot = Graph(comment=u'M\\xf8nti Pyth\\xf8n ik den H\\xf8lie Grailen')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m'test-output/m00se.gv.pdf'\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lang\n\u001b[1;32m     35\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDigraph\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/graphviz/files.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBase\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m     24\u001b[0m     _engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m     _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Mnet/lib/python3.9/site-packages/graphviz/files.py:28\u001b[0m, in \u001b[0;36mBase\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m _engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m _encoding \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mENCODING\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mengine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124;03m\"\"\"The layout commmand used for rendering (``'dot'``, ``'neato'``, ...).\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'graphviz.backend' has no attribute 'ENCODING'"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "x=torch.ones(10, requires_grad=True)\n",
    "\n",
    "y=x**2\n",
    "z=x**3\n",
    "r=(y+z).sum()\n",
    "make_dot(r).render(\"attached\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0PFl-B9Gf5G"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "jJcV7N9KHWAU"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "num_epochs=1\n",
    "train = Data.TensorDataset(X_train_tensors, y_train_tensors, bp_train_tensor)\n",
    "test = Data.TensorDataset(X_test_tensors, y_test_tensors, bp_test_tensor)\n",
    "train_loader = Data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = Data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(rpsmnet.parameters(), lr=learning_rate)#, weight_decay=0.001)\n",
    "# optimizer = torch.optim.SGD(rpsmnet.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params': rpsmnet.spatial.parameters()}, \n",
    "                             {'params': rpsmnet.temporal.parameters()},\n",
    "                             {'params': rpsmnet.attention1.parameters()},\n",
    "                             {'params': rpsmnet.attention2.parameters()},\n",
    "                             {'params': rpsmnet.ff1.parameters()},\n",
    "                             {'params': rpsmnet.concatenate.parameters(),'weight_decay': 0.0005},\n",
    "                             {'params': rpsmnet.ff2.parameters(), 'weight_decay': 0.0005}], lr=learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "error",
     "timestamp": 1656336840520,
     "user": {
      "displayName": "Lei Luo",
      "userId": "12117982536677976370"
     },
     "user_tz": -60
    },
    "id": "lDvOt24MGf5L",
    "outputId": "bf0e73e4-4335-4e8f-c128-dd2635df4a37",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[1, 13] trainning loss: 34.30713725090027\u001b[31m\n",
      "\u001b[32m[1, 2] valid_loss: 5.279666185379028 Acc: 0.038461538461538464\u001b[32m\n",
      "Validation loss decreased (inf --> 5.2797).  Saving model ...\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "summary_writer = SummaryWriter(f'./models/test')\n",
    "# valid_loss = []\n",
    "running_loss = 0\n",
    "door_for_test = 1\n",
    "rpsmnet.train()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    t_loss = 0\n",
    "    train_loss = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "#         get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels, bp = data #, bp\n",
    "        # print(inputs.shape)\n",
    "        inputs, labels, bp = inputs.to(device), labels.type(torch.LongTensor).to(device), bp.to(device)# , bp  , bp.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = rpsmnet(inputs,bp)\n",
    "        # print(outputs)\n",
    "        with torch.autocast('cuda'):\n",
    "        # loss = criterion(outputs, torch.tensor(labels).cuda())\n",
    "            loss = criterion(outputs, labels)\n",
    "#         l2_lambda = 0.001\n",
    "#         l2_reg = torch.tensor(0.).cuda()\n",
    "#         for param in rpsmnet.parameters():\n",
    "#             l2_reg += torch.norm(param)\n",
    "#         loss += l2_lambda * l2_reg\n",
    "        \n",
    "        # l1_lambda = 0.001\n",
    "        # l1_norm = sum(torch.linalg.norm(p, 1) for p in rpsmnet.parameters())\n",
    "        # # l2_norm = sum(torch.linalg.norm(p, 2) for p in model.parameters())\n",
    "        # loss = loss + l1_lambda * l1_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "        # print(t_loss)\n",
    "        train_loss.append(np.mean(t_loss))\n",
    "    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        # if i % (math.ceil(900*Split/BATCH_SIZE)-1) == 0 and i !=0:\n",
    "        print(CRED+ f'[{epoch + 1}, {i + 1}] trainning loss: {train_loss[-1]}'+ CRED)\n",
    "    summary_writer.add_scalar('train_loss', train_loss[-1], epoch)\n",
    "    \n",
    "    if door_for_test == 1:\n",
    "        if epoch % 1 == 0:\n",
    "            # rpsmnet.eval()\n",
    "            valid_loss = []\n",
    "            acc = []\n",
    "            va_loss = 0\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                i_list = []\n",
    "                i_list.append(i)\n",
    "                val_x, val_y, bp = data #, bp \n",
    "                val_x, val_y, bp = val_x.to(device), val_y.type(torch.LongTensor).to(device), bp.to(device)#, bp.to(device) #, bp\n",
    "                Testoutput = rpsmnet(val_x, bp)# , bp\n",
    "                # v_loss = criterion(Testoutput, val_y, torch.Tensor(Testoutput.size(0)).cuda().fill_(1.0))\n",
    "                v_loss = criterion(Testoutput, val_y) #loss\n",
    "                va_loss += v_loss.item()\n",
    "                valid_loss.append(np.mean(va_loss))\n",
    "                # if i = i_list[-1]:    # print every first\n",
    "                _, predicted = torch.max(Testoutput,1)\n",
    "                labels = val_y.cpu()\n",
    "                predicted = predicted.cpu()\n",
    "                mean_accuracy = accuracy_score(labels[labels != 99], predicted[predicted != 99])\n",
    "                acc.append(np.mean(mean_accuracy))\n",
    "            print(CGREEN+f'[{epoch + 1}, {i + 1}] valid_loss: {valid_loss[-1]} Acc: {acc[-1]}'+CGREEN)\n",
    "            # rpsmnet.train()\n",
    "        summary_writer.add_scalar('valid_loss', valid_loss[-1], epoch)\n",
    "#     scheduler.step(acc[-1])\n",
    "\n",
    "  # if epoch % 1 == 0:\n",
    "  #   GETcorrectnumber(train_loader,CYELLOW)      \n",
    "  #   GETcorrectnumber(test_loader,CBLUE)\n",
    "  # aoemnet.train()\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "    early_stopping(valid_loss[-1], rpsmnet)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(outputs, params=dict(list(rpsmnet.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.rand(64, 256, 26, 30)\n",
    "test2 = torch.rand(64, 256, 26, 1).expand_as(test1)\n",
    "\n",
    "test3 = test1*test2\n",
    "test3 = test3.squeeze(1)\n",
    "print(test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inputs.squeeze(1).shape\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WubBEV-ctmdF"
   },
   "source": [
    "Confusion matrix and mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h6xSCd_riOV"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "confusion_matrix_test_loader = Data.DataLoader(test, batch_size = 90, shuffle = False)\n",
    "with torch.no_grad():\n",
    "    list_mean_accuracy = []\n",
    "    for i, data in enumerate(confusion_matrix_test_loader, 0):\n",
    "        inputs, labels, bp = data\n",
    "        inputs, labels, bp = inputs.to(device), labels.type(torch.LongTensor).to(device), bp.to(device)\n",
    "        # inputs, labels = data\n",
    "        # inputs, labels = inputs.to(device), labels.type(torch.LongTensor).to(device)\n",
    "        outputs = rpsmnet(inputs, bp)\n",
    "        # outputs = rpsmnet(inputs)\n",
    "#         optimizer.step()          \n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        # predicted = torch.max(outputs)\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "        labels = labels.numpy()\n",
    "        predicted = predicted.numpy()\n",
    "        # mean_conf_mat = confusion_matrix(labels, predicted)\n",
    "        # mean_accuracy = accuracy_score(labels[labels != 99], predicted[predicted != 99])\n",
    "        # mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)  # normalise\n",
    "        mean_conf_mat = confusion_matrix(labels, predicted)\n",
    "        mean_accuracy = accuracy_score(labels[labels != 99], predicted[predicted != 99])\n",
    "        mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1) \n",
    "        list_mean_accuracy.append(mean_accuracy)\n",
    "        print(\"Mean accuracy = {0}\".format(mean_accuracy))\n",
    "        ConfusionMatrixDisplay.from_predictions(labels, predicted)\n",
    "        # plt.savefig('/content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/notebooks/templates/save_folder/fig-{}.png'.format(session_id), dpi=600)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(Testoutput,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52utnK5XGf5M"
   },
   "outputs": [],
   "source": [
    "!rm -rf /logs/ # clear logs\n",
    "# if 'google.colab' in str(get_ipython()): # tensor board\n",
    "%load_ext tensorboard  \n",
    "# %tensorboard --logdir logs\n",
    "%tensorboard --logdir=./models/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJxb95imHxNM",
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#       # super(CNN, self)._init_()\n",
    "#       super(CNN, self).__init__()\n",
    "#       self.n_classes = 14\n",
    "#       n_classes =14\n",
    "#       self.conv1 = nn.Sequential(\n",
    "#           nn.Conv2d(\n",
    "#               in_channels=1,\n",
    "#               out_channels=32,\n",
    "#               kernel_size=3,\n",
    "#               stride=1,\n",
    "#               padding=1,\n",
    "#           ),\n",
    "#           nn.ReLU(),\n",
    "#           nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "#       )\n",
    "#       self.conv2 = nn.Sequential(\n",
    "#           nn.Conv2d(32,64,3,1,1),\n",
    "#           nn.ReLU(),\n",
    "#           nn.MaxPool2d (2,2),\n",
    "#       )\n",
    "#       # self.fc = nn.Linear(64*7*7,128)\n",
    "#       self.fc = nn.Linear(139264, 100)\n",
    "#       self.out = nn.Linear(100,n_classes)\n",
    "#       self.softmax = nn.Softmax()\n",
    "\n",
    "#     def forward(self,x):\n",
    "#       x=self.conv1(x)\n",
    "#       x=self.conv2(x)\n",
    "#       # x=x.view(x.size(0),-1)\n",
    "#       x = torch.flatten(x, 1) # flatten all dimensioxns except batch\n",
    "#       x=self.fc(x)\n",
    "#       x=self.out(x)      \n",
    "#       # output=self.out(x)\n",
    "#       # return output, x\n",
    "#       # x=self.softmax(x)\n",
    "#       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
