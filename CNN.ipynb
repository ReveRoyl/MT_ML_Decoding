{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "s2D5XWs_jZsA"
      ],
      "mount_file_id": "1ENFKFkJp4WThRmWwmoMTPB6Ak-30jcPT",
      "authorship_tag": "ABX9TyOlHIW190FznrFneFzeKFB/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReveRoyl/MT_ML_Decoding/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "52Y8d1Vai6l4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package installation and import"
      ],
      "metadata": {
        "id": "jPnYrBnWjDIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ61uPB4j4Oo",
        "outputId": "3f10371f-f93b-4780-cd2a-18ece604cf2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne\n",
        "!pip install sklearn\n",
        "!pip install tensorflow\n",
        "!pip install -U dm-haiku\n",
        "!pip install optax"
      ],
      "metadata": {
        "id": "NsGsc_lKi6S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('drive/MyDrive/load/code')\n",
        "from load_data import load_MEG_dataset\n",
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "from jax import numpy as jnp\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "8rUYw68cjY2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load data"
      ],
      "metadata": {
        "id": "s2D5XWs_jZsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X, y = load_MEG_dataset([str(i).zfill(3) for i in range(1,5)])\n",
        "X_train, y_train = load_MEG_dataset([str(i).zfill(3) for i in range(1,5)], mode = 'concatenate', output_format='numpy')\n",
        "X_test, y_test = load_MEG_dataset([str(i).zfill(3) for i in range(1,5)], mode = 'concatenate', output_format='numpy')\n",
        "X_train, X_test, y_train, y_test = jnp.array(X_train, dtype=jnp.float32),\\\n",
        "                                   jnp.array(X_test, dtype=jnp.float32),\\\n",
        "                                   jnp.array(y_train, dtype=jnp.float32),\\\n",
        "                                   jnp.array(y_test, dtype=jnp.float32)\n",
        "print('loading done')"
      ],
      "metadata": {
        "id": "hwbLYH_Fi5i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(X_test).any()"
      ],
      "metadata": {
        "id": "0tCioYV2uwDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN process"
      ],
      "metadata": {
        "id": "UNitrn7EwORK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes =  jnp.unique(y_train)\n",
        "class CNN(hk.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"CNN\")\n",
        "        self.conv1 = hk.Conv2D(output_channels=32, kernel_shape=(3,3), padding=\"SAME\")\n",
        "        self.conv2 = hk.Conv2D(output_channels=16, kernel_shape=(3,3), padding=\"SAME\")\n",
        "        self.flatten = hk.Flatten()\n",
        "        self.linear = hk.Linear(len(classes))\n",
        "\n",
        "    def __call__(self, x_batch):\n",
        "        x = self.conv1(x_batch)\n",
        "        x = hk.MaxPool(window_shape=(2, 2), strides=(2, 2), padding='SAME')(x)\n",
        "        x = jax.nn.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = jax.nn.relu(x)\n",
        "        x = hk.MaxPool(window_shape=(2, 2), strides=(2, 2), padding='SAME')(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = jax.nn.softmax(x)\n",
        "        return x\n",
        "\n",
        "def ConvNet(x):\n",
        "    cnn = CNN()\n",
        "    return cnn(x)\n",
        "\n",
        "conv_net = hk.transform(ConvNet)        \n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "params = conv_net.init(rng, X_train[:5])\n",
        "\n",
        "print(\"Weights Type : {}\\n\".format(type(params)))\n",
        "\n",
        "for layer_name, weights in params.items():\n",
        "    print(layer_name)\n",
        "    print(\"Weights : {}, Biases : {}\\n\".format(params[layer_name][\"w\"].shape,params[layer_name][\"b\"].shape))\n",
        "\n",
        "\n",
        "preds = conv_net.apply(params, rng, X_train[:5])\n",
        "\n",
        "preds[:5]"
      ],
      "metadata": {
        "id": "ZL6Wq_NIwNzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function and Weights update function"
      ],
      "metadata": {
        "id": "cxtPkiAqxw9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossEntropyLoss(weights, input_data, actual):\n",
        "    preds = conv_net.apply(weights, rng, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)\n",
        "def UpdateWeights(weights,gradients):\n",
        "    return weights - learning_rate * gradients"
      ],
      "metadata": {
        "id": "IhyP1X4MxyrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "PwLX3hzDx_8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "rng = jax.random.PRNGKey(42) ## Reproducibility ## Initializes model with same weights each time.\n",
        "\n",
        "conv_net = hk.transform(ConvNet)\n",
        "params = conv_net.init(rng, X_train[:5])\n",
        "epochs = 25\n",
        "batch_size = 256\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "\n",
        "\n",
        "optimizer = optax.adam(learning_rate=learning_rate) ## Initialize SGD Optimizer\n",
        "optimizer_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "for i in range(1, epochs+1):\n",
        "    batches = jnp.arange((X_train.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    losses = [] ## Record loss of each batch\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch, Y_batch = X_train[start:end], y_train[start:end] ## Single batch of data\n",
        "\n",
        "        loss, param_grads = value_and_grad(CrossEntropyLoss)(params, X_batch, Y_batch) ## Forward pass, loss and grads calculation\n",
        "        #print(param_grads)\n",
        "        updates, optimizer_state = optimizer.update(param_grads, optimizer_state) ## Calculate parameter updates\n",
        "        params = optax.apply_updates(params, updates) ## Update model weights\n",
        "        #params = jax.tree_map(UpdateWeights, params, param_grads) ## Update Params\n",
        "        losses.append(loss) ## Record Loss\n",
        "\n",
        "    print(\"CrossEntropy Loss : {:.2f}\".format(jnp.array(losses).mean()))"
      ],
      "metadata": {
        "id": "-fVd_isKyEBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make prediction"
      ],
      "metadata": {
        "id": "uTGOkzGXySM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MakePredictions(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        preds.append(conv_net.apply(weights, rng, X_batch))\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "DoKxjxnqyRXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = MakePredictions(params, X_train, 256)\n",
        "train_preds = jnp.concatenate(train_preds).squeeze()\n",
        "train_preds = train_preds.argmax(axis=1)\n",
        "\n",
        "test_preds = MakePredictions(params, X_test, 256)\n",
        "test_preds = jnp.concatenate(test_preds).squeeze()\n",
        "test_preds = test_preds.argmax(axis=1)"
      ],
      "metadata": {
        "id": "d4qKLXtIyWoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "Ugu11JzyyZ0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(y_test, test_preds)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL8a1F4oybo3",
        "outputId": "4864038d-d2e8-459b-af9e-c729f14be571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.125\n",
            "Test  Accuracy : 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Classification Report \")\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "id": "DueB-3XVyndX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}