{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay in Aversive Environments - Sequenceness analysis\n",
    "\n",
    "#### _This is a template that will be parameterised and run via [Papermill](http://papermill.readthedocs.io/) for each subject_\n",
    "\n",
    "This notebook uses the classifer trained on the localiser data to detect spontaneous state reactivation during the planning and rest phases of the task.\n",
    "\n",
    "Analysis steps:\n",
    "\n",
    "1. Loading task data and classifier\n",
    "2. Applying the classifer to the task data to generate time X state reactivation probabilities matrices\n",
    "3. Running the GLM-based sequenceness estimation procedure using a sliding window approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'code')\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "from state_prediction import *\n",
    "from sequenceness import *\n",
    "from utils import *\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# DEFAULT PARAMETERS - OVERRRIDEN BY PAPERMILL EXECUTION\n",
    "session_id = '001'  # ID of the scanning session\n",
    "output_dir = 'data/derivatives'  # Where the output data should go\n",
    "window_width = 40  # Width of the sliding window used for sequenceness analysis\n",
    "classifier_window = [5, 6] # Window used for classification\n",
    "classifier_center_idx = 37  # The center index of the classification window, post stimulus onset\n",
    "max_lag = 20  # Maximum time-lag to look at sequenceness for\n",
    "correct_alpha = True  # Correct for alpha oscillations (only if using GLM)\n",
    "glm_constant = False  # Use constant (only if using GLM)\n",
    "method = 'cc'  # Method for assessing sequenceness, 'cc' for cross-correlation (e.g. Kurth-Nelson, Eldar), 'glm' for GLM (e.g. Liu)\n",
    "scale_data = False  # Scale state reactivation probabilities prior to sequenceness analysis\n",
    "n_stim = 14  # Number of stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "#import sklearn\n",
    "#print (sklearn.__version__) # this causes the problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State detection\n",
    "\n",
    "### Load the classifier\n",
    "\n",
    "First we load the classifier that we previously trained on the localiser data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.decomposition.pca'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8b1699269262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classifier_idx_{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_center_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sub-{0}_classifier_idx_{1}.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_center_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m/usr/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mload_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.decomposition.pca'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "clf = joblib.load(os.path.join(output_dir, 'classifier', 'classifier_idx_{0}'.format(classifier_center_idx + 50), 'sub-{0}_classifier_idx_{1}.pkl').format(session_id, classifier_center_idx + 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the task data\n",
    "\n",
    "We're interested in the planning and rest phases so we'll select these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_epochs = mne.read_epochs(os.path.join(output_dir, 'preprocessing/task', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-planning_proc_ICA-epo.fif.gz').format(session_id))\n",
    "outcome_epochs = mne.read_epochs(os.path.join(output_dir, 'preprocessing/task', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-task_outcome_proc_ICA-epo.fif.gz').format(session_id))\n",
    "\n",
    "# Get the data as a numpy array, excluding non-MEG channels\n",
    "picks_meg = mne.pick_types(planning_epochs.info, meg=True, ref_meg=False)\n",
    "planning_X = planning_epochs.get_data()[:, picks_meg, :] # MEG signals: n_epochs, n_channels, n_times\n",
    "outcome_X = outcome_epochs.get_data()[:, picks_meg, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isnan(planning_X).any() == False, \"Nans present in planning data\"\n",
    "assert np.isnan(outcome_X).any() == False, \"Nans present in outcome data\"\n",
    "assert np.isinf(planning_X).any() == False, \"Infs present in planning data\"\n",
    "assert np.isinf(outcome_X).any() == False, \"Infs present in outcome data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State detection\n",
    "\n",
    "Here we iterate over trials, reshape the data for each trial into the format `[n_trials, n_sensors, n_timepoints]`, where the first dimension is 1 and the final dimension is the timepoint of interest plus additional adjacent timepoints used as extra features, and finally and use the `predict_proba` method of the fitted classifier to get predicted state reactivation probabilities for every timepoint within the trial.\n",
    "\n",
    "\n",
    "This involves a lot of for loops and could probably be made far more efficient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_state_reactivation = predict_states(planning_X, clf, shifts=classifier_window, n_stim=n_stim)\n",
    "assert np.isnan(planning_state_reactivation).any() == False, \"Nans present in planning state reactivation array\"\n",
    "assert np.isinf(planning_state_reactivation).any() == False, \"Infs present in planning state reactivation array\"\n",
    "\n",
    "outcome_state_reactivation = predict_states(outcome_X, clf, shifts=classifier_window, n_stim=n_stim)\n",
    "assert np.isnan(outcome_state_reactivation).any() == False, \"Nans present in outcome state reactivation array\"\n",
    "assert np.isinf(outcome_state_reactivation).any() == False, \"Infs present in outcome state reactivation array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(output_dir, 'state_reactivation_arrays', 'planning', 'classifier_idx_{0}'.format(classifier_center_idx))):\n",
    "    os.makedirs(os.path.join(output_dir, 'state_reactivation_arrays', 'planning', 'classifier_idx_{0}'.format(classifier_center_idx)))\n",
    "np.save(os.path.join(output_dir, 'state_reactivation_arrays', 'planning', 'classifier_idx_{0}'.format(classifier_center_idx), 'sub-{0}_planning_state_reactivation_idx_{1}.pkl'.format(session_id, classifier_center_idx)), planning_state_reactivation)\n",
    "\n",
    "if not os.path.exists(os.path.join(output_dir, 'state_reactivation_arrays', 'outcome', 'classifier_idx_{0}'.format(classifier_center_idx))):\n",
    "    os.makedirs(os.path.join(output_dir, 'state_reactivation_arrays', 'outcome', 'classifier_idx_{0}'.format(classifier_center_idx)))\n",
    "np.save(os.path.join(output_dir, 'state_reactivation_arrays', 'outcome', 'classifier_idx_{0}'.format(classifier_center_idx), 'sub-{0}_outcome_state_reactivation_idx_{1}.pkl'.format(session_id, classifier_center_idx)), outcome_state_reactivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to StateReactivation class\n",
    "outcome_seq = StateReactivation(outcome_state_reactivation)\n",
    "planning_seq = StateReactivation(planning_state_reactivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequenceness analysis\n",
    "\n",
    "After determining the state reactivation probabilities for each trial, we can submit this data to the sequenceness analysis. We use a GLM approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transition matrix\n",
    "\n",
    "Here we load the transition matrix of the task, which is necessary for sequenceness analysis. We then subset this matrix to get the four arms of the task tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = np.loadtxt(r'task/Task_information/transition_matrix.txt')\n",
    "\n",
    "matrices = []\n",
    "\n",
    "# Select individual arms\n",
    "for start in [0, 1, 2, 3]:\n",
    "    if start in [0,1]:\n",
    "        m = select_path(transition_matrix, start, 12)\n",
    "    else:\n",
    "        m = select_path(transition_matrix, start, 13)\n",
    "    matrices.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sequenceness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_windowed_sequenceness = outcome_seq.get_windowed_sequenceness(max_lag, matrices, alpha=correct_alpha, \n",
    "                                                                      width=window_width, remove_first=False, constant=glm_constant, set_zero=False, scale=scale_data, method=method)\n",
    "planning_windowed_sequenceness = planning_seq.get_windowed_sequenceness(max_lag, matrices, alpha=correct_alpha, \n",
    "                                                                        width=window_width, remove_first=False, constant=glm_constant, set_zero=False, scale=scale_data, method=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the sequenceness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(output_dir, 'sw_sequenceness', 'planning', 'classifier_idx_{0}'.format(classifier_center_idx))):\n",
    "    os.makedirs(os.path.join(output_dir, 'sw_sequenceness', 'planning', 'classifier_idx_{0}'.format(classifier_center_idx)))\n",
    "joblib.dump(planning_windowed_sequenceness, os.path.join(output_dir, 'sw_sequenceness', 'planning', 'classifier_idx_{0}'.format(classifier_center_idx), 'sub-{0}_planning_sequenceness_idx_{1}__{2}.pkl'.format(session_id, classifier_center_idx, method)))\n",
    "\n",
    "if not os.path.exists(os.path.join(output_dir, 'sw_sequenceness', 'outcome', 'classifier_idx_{0}'.format(classifier_center_idx))):\n",
    "    os.makedirs(os.path.join(output_dir, 'sw_sequenceness', 'outcome', 'classifier_idx_{0}'.format(classifier_center_idx)))\n",
    "joblib.dump(outcome_windowed_sequenceness, os.path.join(output_dir, 'sw_sequenceness', 'outcome', 'classifier_idx_{0}'.format(classifier_center_idx), 'sub-{0}_outcome_sequenceness_idx_{1}__{2}.pkl'.format(session_id, classifier_center_idx, method)))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "psychopy3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nteract": {
   "version": "0.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
