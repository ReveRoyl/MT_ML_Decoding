{"cells":[{"cell_type":"markdown","metadata":{"id":"vSkpg4anh7RQ"},"source":["# Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23087,"status":"ok","timestamp":1652448693834,"user":{"displayName":"Lei Luo","userId":"12117982536677976370"},"user_tz":-60},"id":"AShsm69WH9Ey","outputId":"f8497749-1cc4-4f65-e173-3272add3a1b0"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\") #connect to google drive if run via browser"]},{"cell_type":"markdown","metadata":{"id":"Qu3fJfZeh7RT"},"source":["## Intsall reuirements and import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":40788,"status":"ok","timestamp":1652448734611,"user":{"displayName":"Lei Luo","userId":"12117982536677976370"},"user_tz":-60},"id":"vlVemiQbJICX","outputId":"0deb213a-b1ee-4a93-c7f8-8c0f89ad9a93"},"outputs":[],"source":["%pip install -r /content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5789,"status":"ok","timestamp":1652448803131,"user":{"displayName":"Lei Luo","userId":"12117982536677976370"},"user_tz":-60},"id":"3aLZyBMFGsyI"},"outputs":[],"source":["import os\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/notebooks/templates')\n","import mne\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import json\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.model_selection import RandomizedSearchCV, cross_val_predict\n","from sklearn.model_selection import GridSearchCV #test\n","#from sklearn.externals import joblib\n","import joblib\n","from scipy.stats import halfcauchy\n","from mne.decoding import UnsupervisedSpatialFilter\n","import plotly.graph_objs as go\n","from plotly.offline import init_notebook_mode, iplot\n","from utils import add_features\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from state_prediction import *\n","from sliding_window_classifiers import *\n","import gc\n","np.random.seed(100)\n","\n","\n","\n","# DEFAULT PARAMETERS - OVERRRIDEN BY PAPERMILL EXECUTION\n","session_id = '001'  # ID of the scanning session\n","output_dir = '/content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/notebooks/templates/data/derivatives'  # Where the output data should go\n","n_stim = 14  # Number of stimuli\n","classifier_window = [-5, 6]  # Additional timepoints to use as features\n","classifier_center_idx_setting = 20  # The center index of the classification window, post stimulus onset\n","n_pca_components = [30, 60]  # Range of PCA components to try when optimising the classifier\n","param_optimisation_cv = 5  # Folds of CV to use in optimisation\n","classifier_regularisation = 'l1'  # Type of regularisation to use, l1 or l2\n","classifier_multiclass = 'ovr'  # Type of multi-class approach to use, ovr for one-vs-the-rest or multiclass\n","confusion_matrix_cv = 5  # CV to use for making the confusion matrix\n","n_iter_search = 100  # Number of iterations of the random search parameter optimisation procedure\n","cores = 1  # Number of cores to use for parallel processing\n","os.environ['OMP_NUM_THREADS'] = str(cores)\n","list_mean_accuracy = []\n","list_X = []"]},{"cell_type":"markdown","metadata":{"id":"p5_VI9gDh7RV"},"source":["## Apply sequenceness classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":127468,"status":"error","timestamp":1652276365989,"user":{"displayName":"Lei Luo","userId":"12117982536677976370"},"user_tz":-60},"id":"ljBwuJE5GN2Z","outputId":"660a8bef-7018-4cf4-aaf5-70506da4e2ce"},"outputs":[],"source":["for session_id_int in range(1, 5):\n","    gc.collect()\n","    session_id = '{:03d}'.format(session_id_int)\n","    print(session_id)\n","\n","    # Get data\n","    localiser_epochs = mne.read_epochs(os.path.join(output_dir, 'preprocessing', 'sub-{}', 'localiser', 'sub-{}_ses-01_task-AversiveLearningReplay_run-localiser_proc_ICA-epo.fif.gz').format(session_id,session_id))  \n","\n","    # Get epoch data\n","    X_raw = localiser_epochs.get_data()  # MEG signals: n_epochs, n_channels, n_times (exclude non MEG channels)\n","    y_raw = localiser_epochs.events[:, 2]  # Get event types\n","\n","    # select events and time period of interest\n","    picks_meg = mne.pick_types(localiser_epochs.info, meg=True, ref_meg=False)\n","    event_selector = (y_raw < n_stim * 2 + 1)\n","    X_raw = X_raw[event_selector, ...]\n","    y_raw = y_raw[event_selector]\n","    X_raw = X_raw[:, picks_meg, :]\n","\n","    assert len(np.unique(y_raw)) == n_stim, \"Found {0} stimuli, expected {1}\".format(len(np.unique(y_raw)), n_stim)\n","\n","    print(\"Number of unique events = {0}\\n\\nEvent types = {1}\".format(len(np.unique(y_raw)),\n","                                                                    np.unique(y_raw)))\n","\n","    times = localiser_epochs.times\n","\n","    prestim_samples = int(np.abs(localiser_epochs.tmin * localiser_epochs.info['sfreq']))\n","    classifier_center_idx = prestim_samples + classifier_center_idx_setting\n","\n","\n","    # Get data\n","    X, y = (X_raw.copy(), y_raw.copy())\n","    X = X[..., classifier_center_idx + classifier_window[0]:classifier_center_idx + classifier_window[1]] \n","\n","    #Create null data\n","    # X_null = np.zeros((X.shape[0], 272, np.sum(np.abs(classifier_window))))\n","    # for n, i in enumerate(np.random.randint(np.sum(np.abs(classifier_window)), prestim_samples, X.shape[0])):\n","    #     X_null[n, :, :] = X_raw[n, :, i:np.sum(np.abs(classifier_window)) + i]\n","    # y_null = np.ones(X_null.shape[0]) * 99\n","    # X = np.vstack([X, X_null])\n","    # y = np.hstack([y, y_null])\n","    \n","    # Create a pipiline that combines PCA, feature augmentation, scaling, and the logistic regression classifier\n","    clf = make_pipeline(UnsupervisedSpatialFilter(PCA(50), average=False), \n","                        FunctionTransformer(add_features, validate=False), StandardScaler(), \n","                        LogisticRegression(multi_class=classifier_multiclass, C=0.1, penalty=classifier_regularisation, solver='saga', max_iter=100000, tol=0.2, class_weight=\"balanced\"))\n","    # # Parameter distributions passed to the random search procedure\n","    param_dist = {\"unsupervisedspatialfilter__estimator__n_components\": range(*n_pca_components),\n","              \"logisticregression__C\": halfcauchy(scale=5)}\n","\n","    # # run randomized search\n","    random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n","                                   n_iter=n_iter_search, cv=param_optimisation_cv, n_jobs=8, scoring='accuracy', verbose=True)\n","    random_search.fit(X, y)\n","\n","    # # Produce a dataframe of the search results\n","    results = pd.DataFrame(random_search.cv_results_)\n","\n","    #print(\"Parameter optimisation done\")\n","\n","\n","    init_notebook_mode(connected=True)\n","\n","    trace = go.Mesh3d(x=results.param_logisticregression__C,\n","                    y=results.param_unsupervisedspatialfilter__estimator__n_components,\n","                    z=results.mean_test_score, \n","                    color='#275fb5', opacity=0.20)\n","\n","    layout = go.Layout(\n","        title='Hyperparameter optimisation results',\n","        autosize=True,\n","        width=700,\n","        height=700,\n","        scene = dict(\n","        xaxis = dict(\n","            title='Logistic regression C'),\n","        yaxis = dict(\n","            title='PCA N components'),\n","        zaxis = dict(\n","            title='Mean accuracy'),)\n","    )\n","\n","    fig = go.Figure(data=[trace], layout=layout)\n","    #iplot(fig)\n","\n","\n","    clf.set_params(**random_search.best_params_)\n","    \n","    # Get predictions with 5 fold CV\n","    y_pred = cross_val_predict(clf, X, y, cv=confusion_matrix_cv)\n","    mean_conf_mat = confusion_matrix(y, y_pred)\n","    mean_accuracy = accuracy_score(y[y != 99], y_pred[y != 99])\n","    mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)  # normalise\n","    list_mean_accuracy.append(mean_accuracy)\n","    print(\"Mean accuracy = {0}\".format(mean_accuracy))\n","        \n","    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n","    plt.savefig('/content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/notebooks/templates/save_folder/fig-{}.png'.format(session_id), dpi=600)\n","    plt.show()\n","\n","\n","    #save\n","    if not os.path.exists(os.path.join(output_dir, 'classifier', 'classifier_idx_{0}'.format(classifier_center_idx))):\n","        os.makedirs(os.path.join(output_dir, 'classifier', 'classifier_idx_{0}'.format(classifier_center_idx)))\n","    joblib.dump(random_search.best_estimator_ , os.path.join(output_dir, 'classifier', \n","                                                            'classifier_idx_{0}'.format(classifier_center_idx), 'sub-{0}_classifier_idx_{1}.pkl').format(session_id, classifier_center_idx))\n","\n","    accuracy_data = {\n","        'mean_accuracy': mean_accuracy,\n","        'best_C': random_search.best_params_['logisticregression__C'],\n","        'best_n_components': random_search.best_params_['unsupervisedspatialfilter__estimator__n_components']\n","    }\n","\n","    with open(os.path.join(output_dir, 'classifier', 'sub-{0}_classifier_info.json'), 'w') as f:\n","        json.dump(accuracy_data, f)\n","\n","    if not os.path.exists(os.path.join(output_dir, 'localiser_classifier_performance', 'confusion_matrix', 'classifier_idx_{0}'.format(classifier_center_idx))):\n","        os.makedirs(os.path.join(output_dir, 'localiser_classifier_performance', 'confusion_matrix', 'classifier_idx_{0}'.format(classifier_center_idx)))\n","    np.save(os.path.join(output_dir, 'localiser_classifier_performance', 'confusion_matrix', 'sub-{0}_confusion_matrix_idx_{1}.pkl').format(session_id, classifier_center_idx), mean_conf_mat)\n","\n","    print(os.path.join(output_dir, 'classifier', 'classifier_idx_{0}'.format(classifier_center_idx), 'sub-{0}_classifier_idx_{1}.pkl').format(session_id, classifier_center_idx))\n","    print(os.path.join(output_dir, 'localiser_classifier_performance', 'confusion_matrix', 'classifier_idx_{0}'.format(classifier_center_idx)))\n","    print(os.path.join(output_dir, 'localiser_classifier_performance', 'confusion_matrix', 'sub-{0}_confusion_matrix_idx_{1}.pkl').format(session_id, classifier_center_idx), mean_conf_mat)\n"]},{"cell_type":"markdown","metadata":{"id":"ZgkFsvzxn6i5"},"source":["## Loop to transform X data for each subject without running classification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78808,"status":"ok","timestamp":1651940452232,"user":{"displayName":"Lei Luo","userId":"12117982536677976370"},"user_tz":-60},"id":"3P6lkC3ykdSF","outputId":"0651b339-6d8e-4339-93de-15dee00597e3"},"outputs":[],"source":["for session_id_int in range(1, 2):\n","    gc.collect()\n","    session_id = '{:03d}'.format(session_id_int)\n","    print(session_id)\n","\n","    # Get data\n","    localiser_epochs = mne.read_epochs(os.path.join(output_dir, 'preprocessing', 'sub-{}', 'localiser', 'sub-{}_ses-01_task-AversiveLearningReplay_run-localiser_proc_ICA-epo.fif.gz').format(session_id,session_id))  \n","\n","    # Get epoch data\n","    X_raw = localiser_epochs.get_data()  # MEG signals: n_epochs, n_channels, n_times (exclude non MEG channels)\n","    y_raw = localiser_epochs.events[:, 2]  # Get event types\n","\n","    # select events and time period of interest\n","    picks_meg = mne.pick_types(localiser_epochs.info, meg=True, ref_meg=False)\n","    event_selector = (y_raw < n_stim * 2 + 1)\n","    X_raw = X_raw[event_selector, ...]\n","    y_raw = y_raw[event_selector]\n","    X_raw = X_raw[:, picks_meg, :]\n","\n","    assert len(np.unique(y_raw)) == n_stim, \"Found {0} stimuli, expected {1}\".format(len(np.unique(y_raw)), n_stim)\n","\n","    print(\"Number of unique events = {0}\\n\\nEvent types = {1}\".format(len(np.unique(y_raw)),\n","                                                                    np.unique(y_raw)))\n","\n","    times = localiser_epochs.times\n","\n","    prestim_samples = int(np.abs(localiser_epochs.tmin * localiser_epochs.info['sfreq']))\n","    classifier_center_idx = prestim_samples + classifier_center_idx_setting\n","\n","\n","    # Get data\n","    X, y = (X_raw.copy(), y_raw.copy())\n","    X = X[..., classifier_center_idx + classifier_window[0]:classifier_center_idx + classifier_window[1]] \n","\n","    # Preprocess using _clf\n","    _clf = make_pipeline(UnsupervisedSpatialFilter(PCA(50), average=False), \n","                        FunctionTransformer(add_features, validate=False), StandardScaler())\n","    \n","    # Add the new X to a list or something\n","    _X = _clf.fit_transform(X)\n","    list_X.append(_X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wo9gE2LXrlFM"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"slUZR4P7A9jl"},"source":["# Followings are tests"]},{"cell_type":"markdown","metadata":{"id":"efXAt_NEh7RZ"},"source":["## Concatenate the input data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJsD4FGAA9jm"},"outputs":[],"source":["X_append = []\n","for session_id_int in range(1, 29):\n","    session_id = '{:03d}'.format(session_id_int)\n","    print(session_id)\n","    # Get data\n","    # if session_id_int == 1:\n","        # localiser_epochs_concatenate = mne.read_epochs(os.path.join(output_dir, 'preprocessing', 'sub-001', 'localiser', 'sub-001_ses-01_task-AversiveLearningReplay_run-localiser_proc_ICA-epo.fif.gz')) \n","    # else:\n","    localiser_epochs = mne.read_epochs(os.path.join(output_dir, 'preprocessing', 'sub-{}', 'localiser', 'sub-{}_ses-01_task-AversiveLearningReplay_run-localiser_proc_ICA-epo.fif.gz').format(session_id,session_id))  \n","    # localiser_epochs.info = localiser_epochs_concatenate.info #Q:Info varies?\n","    # localiser_epochs_concatenate = mne.concatenate_epochs([localiser_epochs_concatenate, localiser_epochs]) \n","    X_raw = localiser_epochs.get_data()\n","    y_raw = localiser_epochs.events[:, 2]\n","\n","    picks_meg = mne.pick_types(localiser_epochs.info, meg=True, ref_meg=False)\n","    event_selector = (y_raw < n_stim * 2 + 1)\n","    X_raw = X_raw[event_selector, ...]\n","    X_raw = X_raw[:, picks_meg, :]\n","\n","    assert len(np.unique(y_raw)) == n_stim, \"Found {0} stimuli, expected {1}\".format(len(np.unique(y_raw)), n_stim)\n","\n","    print(\"Number of unique events = {0}\\n\\nEvent types = {1}\".format(len(np.unique(y_raw)),\n","                                                                    np.unique(y_raw)))\n","\n","    times = localiser_epochs.times\n","    prestim_samples = int(np.abs(localiser_epochs.tmin * localiser_epochs.info['sfreq']))\n","    classifier_center_idx = prestim_samples + classifier_center_idx_setting\n","\n","    X = X_raw.copy()\n","    classifier_center_idx = classifier_center_idx_setting\n","    X = X[..., classifier_center_idx + classifier_window[0]:classifier_center_idx + classifier_window[1]] \n","\n","    X_append.append(X)\n","# for epoch in localiser_epochs_concatenate:\n","#     print(epoch.shape)\n","# localiser_epochs = mne.concatenate_epochs(localiser_epochs_concatenate)\n","\n","X_append_correct = [i for i in X_append if i.shape[0] == 900]\n","\n","X_4D = np.stack(X_append_correct)\n"]},{"cell_type":"markdown","metadata":{"id":"4bSrTjeGh7Ra"},"source":["## Test for sequenceness classifier with concatenated data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"elapsed":80396,"status":"ok","timestamp":1652287849463,"user":{"displayName":"Lei Luo","userId":"12117982536677976370"},"user_tz":-60},"id":"_5EMU9NdA9jo","outputId":"d0195158-c5f6-4145-f85e-bc8c76a9c8f2"},"outputs":[],"source":["# Get epoch data\n","X_raw = localiser_epochs.get_data()  # MEG signals: n_epochs, n_channels, n_times (exclude non MEG channels)\n","y_raw = localiser_epochs.events[:, 2]  # Get event types\n","\n","# select events and time period of interest\n","picks_meg = mne.pick_types(localiser_epochs.info, meg=True, ref_meg=False)\n","event_selector = (y_raw < n_stim * 2 + 1)\n","X_raw = X_raw[event_selector, ...]\n","y_raw = y_raw[event_selector]\n","X_raw = X_raw[:, picks_meg, :]\n","\n","assert len(np.unique(y_raw)) == n_stim, \"Found {0} stimuli, expected {1}\".format(len(np.unique(y_raw)), n_stim)\n","\n","print(\"Number of unique events = {0}\\n\\nEvent types = {1}\".format(len(np.unique(y_raw)),\n","                                                                np.unique(y_raw)))\n","\n","times = localiser_epochs.times\n","\n","prestim_samples = int(np.abs(localiser_epochs.tmin * localiser_epochs.info['sfreq']))\n","classifier_center_idx = prestim_samples + classifier_center_idx_setting\n","\n","\n","# Get data\n","X, y = (X_raw.copy(), y_raw.copy())\n","X = X[..., classifier_center_idx + classifier_window[0]:classifier_center_idx + classifier_window[1]] \n","\n","#Create null data\n","# X_null = np.zeros((X.shape[0], 272, np.sum(np.abs(classifier_window))))\n","# for n, i in enumerate(np.random.randint(np.sum(np.abs(classifier_window)), prestim_samples, X.shape[0])):\n","#     X_null[n, :, :] = X_raw[n, :, i:np.sum(np.abs(classifier_window)) + i]\n","# y_null = np.ones(X_null.shape[0]) * 99\n","# X = np.vstack([X, X_null])\n","# y = np.hstack([y, y_null])\n","\n","# Create a pipiline that combines PCA, feature augmentation, scaling, and the logistic regression classifier\n","clf = make_pipeline(UnsupervisedSpatialFilter(PCA(50), average=False), \n","                    FunctionTransformer(add_features, validate=False), StandardScaler(), \n","                    LogisticRegression(multi_class=classifier_multiclass, C=0.1, penalty=classifier_regularisation, solver='saga', max_iter=100000, tol=0.2, class_weight=\"balanced\"))\n","# # Parameter distributions passed to the random search procedure\n","param_dist = {\"unsupervisedspatialfilter__estimator__n_components\": range(*n_pca_components),\n","            \"logisticregression__C\": halfcauchy(scale=5)}\n","\n","# # run randomized search\n","random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n","                                n_iter=n_iter_search, cv=param_optimisation_cv, n_jobs=8, scoring='accuracy', verbose=True)\n","random_search.fit(X, y)\n","\n","# # Produce a dataframe of the search results\n","results = pd.DataFrame(random_search.cv_results_)\n","\n","#print(\"Parameter optimisation done\")\n","\n","\n","init_notebook_mode(connected=True)\n","\n","trace = go.Mesh3d(x=results.param_logisticregression__C,\n","                y=results.param_unsupervisedspatialfilter__estimator__n_components,\n","                z=results.mean_test_score, \n","                color='#275fb5', opacity=0.20)\n","\n","layout = go.Layout(\n","    title='Hyperparameter optimisation results',\n","    autosize=True,\n","    width=700,\n","    height=700,\n","    scene = dict(\n","    xaxis = dict(\n","        title='Logistic regression C'),\n","    yaxis = dict(\n","        title='PCA N components'),\n","    zaxis = dict(\n","        title='Mean accuracy'),)\n",")\n","\n","fig = go.Figure(data=[trace], layout=layout)\n","#iplot(fig)\n","\n","\n","clf.set_params(**random_search.best_params_)\n","\n","# Get predictions with 5 fold CV\n","y_pred = cross_val_predict(clf, X, y, cv=confusion_matrix_cv)\n","mean_conf_mat = confusion_matrix(y, y_pred)\n","mean_accuracy = accuracy_score(y[y != 99], y_pred[y != 99])\n","mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)  # normalise\n","list_mean_accuracy.append(mean_accuracy)\n","print(\"Mean accuracy = {0}\".format(mean_accuracy))\n","    \n","ConfusionMatrixDisplay.from_predictions(y, y_pred)\n","#plt.savefig('/content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/notebooks/templates/save_folder/fig-{}.png'.format(session_id), dpi=600)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"LeGphP46h7Rb"},"source":["## Enlarge dimensions of X"]},{"cell_type":"markdown","metadata":{"id":"c7BEJPaqA9jq"},"source":["## GridSearchCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Y4nTKjWA9jr","outputId":"6944735b-5a39-4f9a-8e01-165472be1723"},"outputs":[],"source":["# Get epoch data\n","X_raw = localiser_epochs.get_data()  # MEG signals: n_epochs, n_channels, n_times (exclude non MEG channels)\n","y_raw = localiser_epochs.events[:, 2]  # Get event types\n","\n","# select events and time period of interest\n","picks_meg = mne.pick_types(localiser_epochs.info, meg=True, ref_meg=False)\n","event_selector = (y_raw < n_stim * 2 + 1)\n","X_raw = X_raw[event_selector, ...]\n","y_raw = y_raw[event_selector]\n","X_raw = X_raw[:, picks_meg, :]\n","\n","assert len(np.unique(y_raw)) == n_stim, \"Found {0} stimuli, expected {1}\".format(len(np.unique(y_raw)), n_stim)\n","\n","print(\"Number of unique events = {0}\\n\\nEvent types = {1}\".format(len(np.unique(y_raw)),\n","                                                                np.unique(y_raw)))\n","\n","times = localiser_epochs.times\n","\n","prestim_samples = int(np.abs(localiser_epochs.tmin * localiser_epochs.info['sfreq']))\n","classifier_center_idx = prestim_samples + classifier_center_idx_setting\n","\n","\n","# Get data\n","X, y = (X_raw.copy(), y_raw.copy())\n","X = X[..., classifier_center_idx + classifier_window[0]:classifier_center_idx + classifier_window[1]] \n","\n","#Create null data\n","# X_null = np.zeros((X.shape[0], 272, np.sum(np.abs(classifier_window))))\n","# for n, i in enumerate(np.random.randint(np.sum(np.abs(classifier_window)), prestim_samples, X.shape[0])):\n","#     X_null[n, :, :] = X_raw[n, :, i:np.sum(np.abs(classifier_window)) + i]\n","# y_null = np.ones(X_null.shape[0]) * 99\n","# X = np.vstack([X, X_null])\n","# y = np.hstack([y, y_null])\n","\n","# Create a pipiline that combines PCA, feature augmentation, scaling, and the logistic regression classifier\n","clf = make_pipeline(UnsupervisedSpatialFilter(PCA(50), average=False), \n","                    FunctionTransformer(add_features, validate=False), StandardScaler(), \n","                    LogisticRegression(multi_class=classifier_multiclass, C=0.1, penalty=classifier_regularisation, solver='saga', max_iter=100000, tol=0.2, class_weight=\"balanced\"))\n","# # Parameter distributions passed to the random search procedure\n","param_grid_ = {\"unsupervisedspatialfilter__estimator__n_components\": range(*n_pca_components),\n","                \"logisticregression__C\": range(*n_pca_components)}\n","\n","# # run grid search\n","random_search = GridSearchCV(clf, param_grid=param_grid_,\n","                                cv=param_optimisation_cv, n_jobs=8, scoring='accuracy', verbose=True)\n","random_search.fit(X, y)\n","\n","# # Produce a dataframe of the search results\n","results = pd.DataFrame(random_search.cv_results_)\n","\n","#print(\"Parameter optimisation done\")\n","\n","\n","init_notebook_mode(connected=True)\n","\n","trace = go.Mesh3d(x=results.param_logisticregression__C,\n","                y=results.param_unsupervisedspatialfilter__estimator__n_components,\n","                z=results.mean_test_score, \n","                color='#275fb5', opacity=0.20)\n","\n","layout = go.Layout(\n","    title='Hyperparameter optimisation results',\n","    autosize=True,\n","    width=700,\n","    height=700,\n","    scene = dict(\n","    xaxis = dict(\n","        title='Logistic regression C'),\n","    yaxis = dict(\n","        title='PCA N components'),\n","    zaxis = dict(\n","        title='Mean accuracy'),)\n",")\n","\n","fig = go.Figure(data=[trace], layout=layout)\n","#iplot(fig)\n","\n","\n","clf.set_params(**random_search.best_params_)\n","\n","# Get predictions with 5 fold CV\n","y_pred = cross_val_predict(clf, X, y, cv=confusion_matrix_cv)\n","mean_conf_mat = confusion_matrix(y, y_pred)\n","mean_accuracy = accuracy_score(y[y != 99], y_pred[y != 99])\n","mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)  # normalise\n","list_mean_accuracy.append(mean_accuracy)\n","print(\"Mean accuracy = {0}\".format(mean_accuracy))\n","    \n","ConfusionMatrixDisplay.from_predictions(y, y_pred)\n","#plt.savefig('/content/drive/MyDrive/MT_ML_Decoding/Aversive_state_reactivation/notebooks/templates/save_folder/fig-{}.png'.format(session_id), dpi=600)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1651940452232,"user":{"displayName":"Lei Luo","userId":"12117982536677976370"},"user_tz":-60},"id":"dQU_drIpANzL","outputId":"823db575-71a7-4a64-b63b-e99d8d052e53"},"outputs":[],"source":["print('mean accuracy= ', list_mean_accuracy)\n","print('X= ', list_X)"]}],"metadata":{"colab":{"background_execution":"on","collapsed_sections":["Qu3fJfZeh7RT","p5_VI9gDh7RV","ZgkFsvzxn6i5","c7BEJPaqA9jq"],"machine_shape":"hm","name":"Classifier.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}
